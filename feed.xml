<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2022-01-02T13:00:11-05:00</updated><id>/feed.xml</id><title type="html">Xavier Sumba</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">Neural Information Retrieval</title><link href="/blog/2020/neural_information_retrieval/" rel="alternate" type="text/html" title="Neural Information Retrieval" /><published>2020-08-21T09:30:19-04:00</published><updated>2020-08-21T09:30:19-04:00</updated><id>/blog/2020/neural_information_retrieval</id><content type="html" xml:base="/blog/2020/neural_information_retrieval/">&lt;p&gt;&lt;strong&gt;Retrieval of information:&lt;/strong&gt; a user express their need in the form of a query and retrieval consists in ranking existing pieces of content or incorporating new responses from the obtained information.&lt;/p&gt;

&lt;p&gt;Neural IR is the application of shallow or deep NN for IR tasks. Other NLP capabilities such as &lt;em&gt;machine translation&lt;/em&gt; and &lt;em&gt;named entity linking&lt;/em&gt; are &lt;strong&gt;not&lt;/strong&gt;  neural IR but could be used in an IR system.&lt;/p&gt;

&lt;p&gt;IR systems must deal with short queries that may contain previously &lt;strong&gt;&lt;em&gt;unseen vocabulary&lt;/em&gt;&lt;/strong&gt; to match against documents that &lt;strong&gt;&lt;em&gt;vary in length&lt;/em&gt;&lt;/strong&gt; or find relevant documents that may contain large sections of &lt;strong&gt;&lt;em&gt;irrelevant text&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Goal:&lt;/strong&gt; Learn fundamentals of neural IR and traditional IR research&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/neuralinformation_retrieval/Untitled.png&quot; alt=&quot;Neural%20Information%20Retrieval%20a3cc768e80eb456591167fa7887a2705/Untitled.png&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;fundamentals-of-text-retrieval&quot;&gt;Fundamentals of text retrieval&lt;/h1&gt;

&lt;h3 id=&quot;tasks&quot;&gt;&lt;strong&gt;Tasks&lt;/strong&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;ad hoc retrieval systems: ranking document retrieval&lt;/li&gt;
  &lt;li&gt;question answering systems: ranking answers containing passages in response to a question&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;what-is-needed&quot;&gt;&lt;strong&gt;What is needed?&lt;/strong&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Semantic understanding&lt;/strong&gt;: careful → &lt;em&gt;hot dog ≠ warm puppy&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Robustness to rare inputs&lt;/strong&gt;: support for infrequently searched-for documents and perform well in rare terms&lt;/p&gt;

    &lt;p&gt;query: &lt;em&gt;“&lt;strong&gt;pekarovic&lt;/strong&gt; land company” →&lt;/em&gt; exact match works for rare terms&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/img/neuralinformation_retrieval/Untitled00.png&quot; alt=&quot;query pekaravic&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;“Learning latent representations of text important for dealing with &lt;strong&gt;vocabulary mismatch&lt;/strong&gt;, but exact matching is also important to deal with &lt;strong&gt;rare terms&lt;/strong&gt; and intents.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Robustness to corpus variance:&lt;/strong&gt; IR models perform on corpora whose distributions are different from the data that the model was trained on.&lt;/p&gt;

    &lt;p&gt;BM25: words “out of the box”&lt;/p&gt;

    &lt;p&gt;DNN: sensitive to distributional differences&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Robustness to variable length inputs:&lt;/strong&gt; relevant documents may contain irrelevant sections and relevant content may either be localized or spread over multiple sections (in IR it is commonly used document length normalization)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Robustness to errors in input:&lt;/strong&gt; No IR system should assume error-free inputs, maybe use some error correction.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Sensitivity to context:&lt;/strong&gt; leverage implicit and explicit context information → query: weather (Seattle or London)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Efficiency:&lt;/strong&gt; It might be necessary to perform search with billions of documents
    &lt;ul&gt;
      &lt;li&gt;Use &lt;strong&gt;telescoping&lt;/strong&gt;: multi-tier architecture that different IR models prune a set of candidates (re-rank)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;metrics&quot;&gt;Metrics&lt;/h3&gt;

&lt;p&gt;IR metrics focus on rank-based evaluation of retrieved results vs ground truth information (manual judgments of implicit feedback from user behavior data)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Precision and recall&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/img/neuralinformation_retrieval/Untitled%201.png&quot; alt=&quot;assets/img/neuralinformation_retrieval/Untitled%201.png&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Mean reciprocal rank (MRR):&lt;/strong&gt; averaged first relevant document over all queries&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/neuralinformation_retrieval/Untitled%202.png&quot; alt=&quot;assets/img/neuralinformation_retrieval/Untitled%202.png&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Mean average precision (MAP):&lt;/strong&gt; average precision for a ranked list of documents &lt;em&gt;R&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/neuralinformation_retrieval/Untitled%203.png&quot; alt=&quot;assets/img/neuralinformation_retrieval/Untitled%203.png&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Normalized discounted cumulative gain (NDCG):&lt;/strong&gt; used when graded relevance judgments are available for a query q, e.g. five-point scale.&lt;/p&gt;

    &lt;p&gt;&lt;strong&gt;Discounted cumulative gain:&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/img/neuralinformation_retrieval/Untitled%204.png&quot; alt=&quot;assets/img/neuralinformation_retrieval/Untitled%204.png&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;NDCG: assume the ideal DCG by getting the ideal rank order for the documents, up to rank k&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/img/neuralinformation_retrieval/Untitled%205.png&quot; alt=&quot;assets/img/neuralinformation_retrieval/Untitled%205.png&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;traditional-ir-models&quot;&gt;Traditional IR models&lt;/h3&gt;

&lt;p&gt;Important baselines for comparison&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;BM25:&lt;/strong&gt; consider the number of occurrence of each query term in the document (TF-IDF)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/neuralinformation_retrieval/Untitled%206.png&quot; alt=&quot;assets/img/neuralinformation_retrieval/Untitled%206.png&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Language modeling (LM):&lt;/strong&gt; rank documents by the posterior probability&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/neuralinformation_retrieval/Untitled%207.png&quot; alt=&quot;assets/img/neuralinformation_retrieval/Untitled%207.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Using MLE:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/neuralinformation_retrieval/Untitled%208.png&quot; alt=&quot;assets/img/neuralinformation_retrieval/Untitled%208.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Adding smoothing:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/neuralinformation_retrieval/Untitled%209.png&quot; alt=&quot;assets/img/neuralinformation_retrieval/Untitled%209.png&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Translation models:&lt;/strong&gt; alternative to LM, &lt;em&gt;assume query q is being generated via a “translation” process from the document d&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/neuralinformation_retrieval/Untitled%2010.png&quot; alt=&quot;assets/img/neuralinformation_retrieval/Untitled%2010.png&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Dependence model:&lt;/strong&gt; consider proximity between query terms&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ow and uw are the set of all contiguous n-grams or phrases&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/neuralinformation_retrieval/Untitled%2011.png&quot; alt=&quot;assets/img/neuralinformation_retrieval/Untitled%2011.png&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Pseudo relevance feedback (PRF):&lt;/strong&gt; a relevance model; additional round for retrieval
    &lt;ol&gt;
      &lt;li&gt;R1: first round for retrieval&lt;/li&gt;
      &lt;li&gt;Use R1 to expand terms and augment the query to retrieve R2 to the user&lt;/li&gt;
    &lt;/ol&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/img/neuralinformation_retrieval/Untitled%2012.png&quot; alt=&quot;assets/img/neuralinformation_retrieval/Untitled%2012.png&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Neural approaches to IR:&lt;/strong&gt; 1) query representation; 2) document representation; 3) estimating relevance&lt;/p&gt;

    &lt;p&gt;Traditionally shallow NN rely on hand-crafted features&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/neuralinformation_retrieval/Untitled%2013.png&quot; alt=&quot;assets/img/neuralinformation_retrieval/Untitled%2013.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Traditionally DNN rely on embeddings&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/neuralinformation_retrieval/Untitled%2014.png&quot; alt=&quot;assets/img/neuralinformation_retrieval/Untitled%2014.png&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;unsupervised-learning-of-term-representations&quot;&gt;Unsupervised learning of term representations&lt;/h1&gt;

&lt;p&gt;Properties of &lt;strong&gt;compositionality&lt;/strong&gt; help to select a term representation.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Local representation:&lt;/strong&gt; one-hot encoding, terms are different, vocabulary size&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Distributed representation:&lt;/strong&gt; there’s a concept of similarity and embedding and latent space&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/neuralinformation_retrieval/Untitled000.png&quot; alt=&quot;/assets/img/neuralinformation_retrieval/Untitled.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In distributed representation the choice of the feature is important. Makes a choice of which items are more similar.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Distributional hypothesis:&lt;/strong&gt; terms that are used in similar context tend to be semantically similar, Harris 1954&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Distributional semantics:&lt;/strong&gt; a word is characterized by the company it keeps, Firth 1957&lt;/p&gt;

&lt;h3 id=&quot;notions-of-similarity&quot;&gt;Notions of similarity&lt;/h3&gt;

&lt;p&gt;Is it a notion of relatedness between terms?  Depends on the type of relationship we are interested in.&lt;/p&gt;

&lt;p&gt;Is “Seattle” closer to “Sydney” or to “Seahawks”?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Terms of similar type (&lt;strong&gt;typical&lt;/strong&gt;): &lt;em&gt;Seattle vs Sydney&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;Terms co-occur in the same document (&lt;strong&gt;topical&lt;/strong&gt;): &lt;em&gt;Seattle vs Seahawks&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/neuralinformation_retrieval/Untitled%201020.png&quot; alt=&quot;/assets/img/neuralinformation_retrieval/Untitled%201.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Many ways to categorize distributional features (neighboring terms, in-documents, etc) and different weighting schemes (e.g. TF-IDF)&lt;/p&gt;

&lt;p&gt;We can get analogies such as:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/neuralinformation_retrieval/Untitled%20220.png&quot; alt=&quot;/assets/img/neuralinformation_retrieval/Untitled%202.png&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;latent-feature-spaces&quot;&gt;Latent feature spaces&lt;/h3&gt;

&lt;p&gt;Here some neural and non-neural latent space models&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Latent Semantic Analysis (LSA)&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;Perform SVD on term-document matrix X to obtain its low-rank approximation. We select the &lt;em&gt;k&lt;/em&gt; largest singular values to obtain a rank k approximation of X. For instance:&lt;/p&gt;

\[X_k=U_k\Sigma_kV_k^T\]

    &lt;p&gt;&lt;img src=&quot;/assets/img/neuralinformation_retrieval/Untitled%20320.png&quot; alt=&quot;/assets/img/neuralinformation_retrieval/Untitled%203.png&quot; /&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Probabilistic Latent Semantic Analysis (PLSA)&lt;/strong&gt;&lt;/p&gt;

        &lt;p&gt;Learns low-dimensional representations by modeling their co-occurrence &lt;em&gt;p( t, d ), where C are the latent topics.&lt;/em&gt;&lt;/p&gt;

\[p(t,d)=p(d)\sum_{c\in C}p(c|d)p(t|c)\]

        &lt;p&gt;LDA: adds a Dirichlet prior&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Neural term embedding&lt;/strong&gt;&lt;/p&gt;
        &lt;ul&gt;
          &lt;li&gt;predict term from its features&lt;/li&gt;
          &lt;li&gt;learns dense low-dimensional representations by minimizing the prediction error based on the &lt;em&gt;information bottleneck method&lt;/em&gt;&lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;&lt;strong&gt;Word2vec (skip-gram / CBOW)&lt;/strong&gt;&lt;/p&gt;

            &lt;p&gt;&lt;em&gt;c: number of neighbours&lt;/em&gt;&lt;/p&gt;

            &lt;p&gt;&lt;em&gt;S: set of all windows&lt;/em&gt;&lt;/p&gt;

            &lt;p&gt;Denominator is prohibitively costly&lt;/p&gt;

            &lt;ul&gt;
              &lt;li&gt;Hierarchical softmax&lt;/li&gt;
              &lt;li&gt;Negative sampling&lt;/li&gt;
            &lt;/ul&gt;

            &lt;p&gt;W_{in} matrix embedding used and  W_{out} is usually discarded after training. However, for IR applications, we can use both&lt;/p&gt;

            &lt;p&gt;&lt;img src=&quot;/assets/img/neuralinformation_retrieval/Untitled%20420.png&quot; alt=&quot;/assets/img/neuralinformation_retrieval/Untitled%204.png&quot; /&gt;&lt;/p&gt;

            &lt;p&gt;&lt;img src=&quot;/assets/img/neuralinformation_retrieval/Untitled%20520.png&quot; alt=&quot;/assets/img/neuralinformation_retrieval/Untitled%205.png&quot; /&gt;&lt;/p&gt;

            &lt;p&gt;Skip-gram creates 2xc samples by individually pairing each neighbouring term with the middle term → slower to train&lt;/p&gt;

            &lt;p&gt;&lt;img src=&quot;/assets/img/neuralinformation_retrieval/Untitled%20620.png&quot; alt=&quot;/assets/img/neuralinformation_retrieval/Untitled%206.png&quot; /&gt;&lt;/p&gt;

            &lt;p&gt;&lt;img src=&quot;/assets/img/neuralinformation_retrieval/Untitled%20720.png&quot; alt=&quot;/assets/img/neuralinformation_retrieval/Untitled%207.png&quot; /&gt;&lt;/p&gt;

            &lt;p&gt;Encourage sparseness in the learnt representation, add the constraint (useful for term analogies):&lt;/p&gt;

            &lt;p&gt;&lt;img src=&quot;/assets/img/neuralinformation_retrieval/Untitled%20820.png&quot; alt=&quot;/assets/img/neuralinformation_retrieval/Untitled%208.png&quot; /&gt;&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;&lt;strong&gt;Glove&lt;/strong&gt;&lt;/p&gt;

            &lt;p&gt;Shows that we can compute the cross-entropy error between the actual co-occurrence probability and the on predicted.&lt;/p&gt;

            &lt;p&gt;$L_{skip-gram}=\sum_{i=1}^{\mid{T}\mid}x_iH(\hat p(t_j\mid t_i), p(t_j\mid t_i))$&lt;/p&gt;

            &lt;p&gt;Two main changes compared with skip-gram model:&lt;/p&gt;

            &lt;ol&gt;
              &lt;li&gt;replace cross-entropy error with squared error&lt;/li&gt;
              &lt;li&gt;apply a saturation function&lt;/li&gt;
            &lt;/ol&gt;

            &lt;p&gt;&lt;img src=&quot;/assets/img/neuralinformation_retrieval/Untitled%20920.png&quot; alt=&quot;/assets/img/neuralinformation_retrieval/Untitled%209.png&quot; /&gt;&lt;/p&gt;

            &lt;p&gt;&lt;img src=&quot;/assets/img/neuralinformation_retrieval/Untitled%201020.png&quot; alt=&quot;/assets/img/neuralinformation_retrieval/Untitled%2010.png&quot; /&gt;&lt;/p&gt;

            &lt;p&gt;Final embeddings are the sum of IN and OUT vectors.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;&lt;strong&gt;Paragraph2vec&lt;/strong&gt;
            &lt;ul&gt;
              &lt;li&gt;Predict term given the id of a document&lt;/li&gt;
              &lt;li&gt;Train term-document pairs to learn an embedding that is more aligned with a topical notion of the term-term similarity (appropriate for IR tasks)&lt;/li&gt;
              &lt;li&gt;However, term-document relationship tens to be more sparse&lt;/li&gt;
            &lt;/ul&gt;

            &lt;p&gt;&lt;img src=&quot;/assets/img/neuralinformation_retrieval/Untitled%201120.png&quot; alt=&quot;/assets/img/neuralinformation_retrieval/Untitled%2011.png&quot; /&gt;&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;term-embeddings-for-ir&quot;&gt;Term embeddings for IR&lt;/h1&gt;

&lt;p&gt;Term embeddings can be useful for inexact matching or for selecting additional terms for query expansion&lt;/p&gt;

&lt;h3 id=&quot;query-document-matching&quot;&gt;Query-document matching&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Derive a dense vector representation for the query and document from the embeddings. Term embeddings can be aggregated most commonly used is AWE (&lt;em&gt;average word embeddings&lt;/em&gt;)&lt;/li&gt;
  &lt;li&gt;Query and document embeddings can be compared using a similarity metric&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/neuralinformation_retrieval/Untitled30.png&quot; alt=&quot;/assets/img/neuralinformation_retrieval/Untitled.png&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Understand the notion of inter-term similarity&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/neuralinformation_retrieval/Untitled%20130.png&quot; alt=&quot;/assets/img/neuralinformation_retrieval/Untitled%201.png&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Dual Embedding Space Model (DESM)&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;Estimates the query-document relevance as follows&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/img/neuralinformation_retrieval/Untitled%20230.png&quot; alt=&quot;/assets/img/neuralinformation_retrieval/Untitled%202.png&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;Instead of aggregating embeddings, we can incorporate term representations into existing IR models&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Neural Translation Language Model (NTLM)&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;Uses the similarity between term embeddings as a measure for term-term translation probability&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/img/neuralinformation_retrieval/Untitled%20330.png&quot; alt=&quot;/assets/img/neuralinformation_retrieval/Untitled%203.png&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Generalized Language Model (GLM)&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/img/neuralinformation_retrieval/Untitled%20430.png&quot; alt=&quot;/assets/img/neuralinformation_retrieval/Untitled%204.png&quot; /&gt;&lt;/p&gt;

\[N_t:\text{set of nearest neighbours of term t}\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Other metrics for similarity&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Word Mover’s Distance (WMD):&lt;/strong&gt; estimate similarity between pairs of documents by computing the minimum distance in the embedding space → “first document &lt;strong&gt;needs to travel&lt;/strong&gt; to reach the terms in the second document”&lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Non-linear Word Transportation (NWT):&lt;/strong&gt; incorporates a similar notion of distance.&lt;/p&gt;

        &lt;p&gt;solve optimization problem:&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/img/neuralinformation_retrieval/Untitled%20530.png&quot; alt=&quot;/assets/img/neuralinformation_retrieval/Untitled%205.png&quot; /&gt;&lt;/p&gt;

        &lt;p&gt;u(d): is the set of all unique terms in document d&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Saliency weighted semantic network (SWSN):&lt;/strong&gt; computing short-text similarity&lt;/p&gt;

        &lt;p&gt;motivation by BM25&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;/assets/img/neuralinformation_retrieval/Untitled%20630.png&quot; alt=&quot;/assets/img/neuralinformation_retrieval/Untitled%206.png&quot; /&gt;&lt;/p&gt;

\[S_s\text{: shorter sentence}\]

\[S_l\text{: longer sentence}\]
      &lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;Errors by embedding based models and exact match models may be different and a &lt;strong&gt;combination&lt;/strong&gt;  is often preferred.&lt;/p&gt;

    &lt;p&gt;a) exact match for “Cambridge”&lt;/p&gt;

    &lt;p&gt;b) fails to detect “Oxford”&lt;/p&gt;

    &lt;p&gt;c) artificially injected the term “Cambridge”&lt;/p&gt;

    &lt;p&gt;&lt;strong&gt;explanation:&lt;/strong&gt; exact match works for a and c (shouldn’t identify), embeddings work for a and c (no similarity found) but fails (embeddings) for b and exact match works best.&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/img/neuralinformation_retrieval/Untitled%20730.png&quot; alt=&quot;/assets/img/neuralinformation_retrieval/Untitled%207.png&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;query-expansion&quot;&gt;Query expansion&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Find good expansion candidates from a global vocabulary&lt;/li&gt;
  &lt;li&gt;Retrieve documents using the expanded query&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Compare every candidate term and then aggregate to get a relevance score&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/neuralinformation_retrieval/Untitled%20830.png&quot; alt=&quot;/assets/img/neuralinformation_retrieval/Untitled%208.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Term embeddings solely performs worse than &lt;em&gt;pseudo-relevance feedback (PRF),&lt;/em&gt; but performs better when combined with PRF.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Local embeddings&lt;/strong&gt; (embeddings trained in a query-specific corpus) are more useful for generating expansion terms&lt;/p&gt;

&lt;h1 id=&quot;supervised-learning-to-rank&quot;&gt;Supervised learning to rank&lt;/h1&gt;

&lt;p&gt;Learning to rank (LTR) uses training data to train towards an IR objective.&lt;/p&gt;

&lt;p&gt;x → query-document pair as feature vector and learn a ranking model f&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;hand-crafted&lt;/strong&gt; features can be categorized into three categories:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Query-independent features (incoming link count and document length)&lt;/li&gt;
  &lt;li&gt;Query-dependent features (BM25)&lt;/li&gt;
  &lt;li&gt;Query-level features (query length)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;neural models:&lt;/strong&gt; learn from query and documents texts (can be combined with other features e.g. popularity)&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Feature learning&lt;/li&gt;
  &lt;li&gt;Interaction-based representations&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;LTR approaches based on their training objective:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Pointwise approach:&lt;/strong&gt; numerical value associated with every query-document pair&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Pairwise approach:&lt;/strong&gt; preference between pairs of documents with respect to individual queries $d_i &amp;gt;_q d_j$&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Listwise approach:&lt;/strong&gt; optimize for a rank-based metric such as NDCG&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;mapping function &lt;strong&gt;&lt;em&gt;f&lt;/em&gt;&lt;/strong&gt; can be many models:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;SVMs&lt;/li&gt;
  &lt;li&gt;Boosted Decision Trees&lt;/li&gt;
  &lt;li&gt;Loss functions (see book for details)
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;regression loss:&lt;/strong&gt; estimate relevance label&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;classification loss:&lt;/strong&gt; multiclass classification&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;contrastive loss:&lt;/strong&gt; minimize distance between relevant pairs and increase distance between dissimilar items&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;cross-entropy loss over documents:&lt;/strong&gt; probability of ranking right document above over all other documents in a collection&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;rank-net loss:&lt;/strong&gt; rank document i higher than document j&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;LambdaRank loss:&lt;/strong&gt; weight rank-net with NDCG&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;ListNet and ListMLE loss&lt;/strong&gt;: probability of observing a particular order (distribution over all possible permutations)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Challenges&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;extreme classification:&lt;/em&gt; extremely large number of classes&lt;/li&gt;
  &lt;li&gt;new loss functions suited for retrieval (research)&lt;/li&gt;
  &lt;li&gt;require large amount of training data&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;deep-neural-networks-for-ir&quot;&gt;Deep neural networks for IR&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Autoencoders:&lt;/strong&gt; optimized for reducing reconstruction errors&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/neuralinformation_retrieval/Untitled40.png&quot; alt=&quot;/assets/img/neuralinformation_retrieval/Untitled.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Siamese networks:&lt;/strong&gt; optimized to better discriminate similar pairs from dissimilar ones&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/neuralinformation_retrieval/Untitled%20140.png&quot; alt=&quot;/assets/img/neuralinformation_retrieval/Untitled%201.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Data consists mainly of:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Corpus of search queries&lt;/li&gt;
  &lt;li&gt;Corpus of candidate documents&lt;/li&gt;
  &lt;li&gt;Ground truth: implicit (clicks) or explicit (human relevance judgments)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For application consider the &lt;strong&gt;level of supervision&lt;/strong&gt; based on the proportion of (un)labelled data:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;unsupervised&lt;/li&gt;
  &lt;li&gt;semi-supervised&lt;/li&gt;
  &lt;li&gt;fully-supervised&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Ranking based on document length&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;long documents:&lt;/em&gt; mixture of many topics and query matches may be spread&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;short documents:&lt;/em&gt; vocabulary mismatches when performing similarity between pairs of texts&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;document-autoencoders&quot;&gt;Document autoencoders&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Semantic Hashing&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Each document is a bag-of-terms and uses one hot representation for the terms.&lt;/li&gt;
      &lt;li&gt;After training, the output is thresholded to generate a binary vector encoding for documents&lt;/li&gt;
      &lt;li&gt;Given a search query → generate corresponding hash → retrieve relevant candidate documents&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Variational autoencoders&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Been explored but it might not be practical for IR tasks since it minimizes the document reconstruction error.&lt;/li&gt;
      &lt;li&gt;Siamese architecture models seem to be more practical&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;siamese-networks&quot;&gt;&lt;strong&gt;Siamese networks&lt;/strong&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Deep Semantic Similarity Model (DSSM)&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;Two deep models that use cosine distance that minimizes cross-entropy loss&lt;/p&gt;

    &lt;p&gt;Used for short texts&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/img/neuralinformation_retrieval/Untitled%20240.png&quot; alt=&quot;/assets/img/neuralinformation_retrieval/Untitled%202.png&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;DSSM can use&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;fully-connected layers&lt;/li&gt;
      &lt;li&gt;Convolutional layers (CDSSM)&lt;/li&gt;
      &lt;li&gt;RNNs&lt;/li&gt;
      &lt;li&gt;Tree-structured networks&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;&lt;strong&gt;Notions of similarity for CDSSM&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;Similarity depends on the choice of paired data&lt;/p&gt;

    &lt;p&gt;query and document title pairs → &lt;strong&gt;topical&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;query prefix and suffix pairs → &lt;strong&gt;typical&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/img/neuralinformation_retrieval/Untitled%20340.png&quot; alt=&quot;/assets/img/neuralinformation_retrieval/Untitled%203.png&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;Similarly other combinations tend to be better for  &lt;em&gt;query auto-completion or session-based personalization&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;interaction-based-networks&quot;&gt;Interaction-based networks&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Instead of representing query and document embeddings as single vectors (Siamese networks), we can &lt;strong&gt;individually compare different parts of the query with different parts of the document.&lt;/strong&gt; Than aggregate these partial evidences of relevance.&lt;/li&gt;
  &lt;li&gt;Useful for long documents&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Interaction:&lt;/strong&gt; use an sliding window for both query and document text. Each instance of the &lt;strong&gt;query window&lt;/strong&gt; is compare with each instance of the &lt;strong&gt;document window&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Typically implemented with a CNN&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/neuralinformation_retrieval/Untitled%20440.png&quot; alt=&quot;/assets/img/neuralinformation_retrieval/Untitled%204.png&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Other works explore interaction matrix approach for &lt;em&gt;short text matching&lt;/em&gt; and &lt;em&gt;ranking long documents.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;lexical-and-semantic-matching&quot;&gt;Lexical and semantic matching&lt;/h3&gt;

&lt;p&gt;Neural IR models perform poorly when rare terms are encountered&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Query (exact match):&lt;/strong&gt; “pekarovic land company”&lt;/p&gt;

&lt;p&gt;“pekarovic” → rare term&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;it is easier to estimate relevance based on exact matches.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;for a neural model it is harder to match or have a good representation&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Query (semantic match):&lt;/strong&gt; “what channel are the seahawks on today?”&lt;/p&gt;

&lt;p&gt;maybe document doesn’t contain the word channel, but contains ESPN or SkySports&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Use &lt;strong&gt;histogram-based features&lt;/strong&gt; in the DNN to capture lexical notion of relevance&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Duet:&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;leverage search data&lt;/li&gt;
      &lt;li&gt;trains jointly lexical and semantic matches&lt;/li&gt;
      &lt;li&gt;Neural models focusing in lexical matching have fewer parameters and can be trained under small data regimes&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/img/neuralinformation_retrieval/Untitled%20540.png&quot; alt=&quot;/assets/img/neuralinformation_retrieval/Untitled%205.png&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;matching-with-multiple-document-fields&quot;&gt;Matching with multiple document fields&lt;/h3&gt;

&lt;p&gt;Different documents fields may contain information relevant to different aspects of the query intent&lt;/p&gt;

&lt;p&gt;Specially in search engines that index documents with different metadata such as hyperlinks, anchor texts, etc.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/neuralinformation_retrieval/Untitled%20640.png&quot; alt=&quot;/assets/img/neuralinformation_retrieval/Untitled%206.png&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;future-research-problems-for-neural-ir&quot;&gt;Future research problems for Neural IR&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;Should the ideal IR model behave like a &lt;strong&gt;library&lt;/strong&gt; that knows about everything in the Universe, or like a &lt;strong&gt;librarian&lt;/strong&gt; who can effectively retrieve without memorizing the corpus?&lt;/li&gt;
  &lt;li&gt;Future IR explorations using related areas
    &lt;ul&gt;
      &lt;li&gt;reinforcement learning&lt;/li&gt;
      &lt;li&gt;adversarial learning&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Some interesting applications
    &lt;ul&gt;
      &lt;li&gt;query auto-completion&lt;/li&gt;
      &lt;li&gt;query recommendation&lt;/li&gt;
      &lt;li&gt;session modelling&lt;/li&gt;
      &lt;li&gt;modeling diversity&lt;/li&gt;
      &lt;li&gt;modeling user click behaviours&lt;/li&gt;
      &lt;li&gt;proactive recommendations&lt;/li&gt;
      &lt;li&gt;entity ranking&lt;/li&gt;
      &lt;li&gt;multi-modal retrieval&lt;/li&gt;
      &lt;li&gt;knowledge-based IR&lt;/li&gt;
      &lt;li&gt;optimizing for for multiple IR tasks&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Some emerging scenarios&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;conversation IR&lt;/li&gt;
          &lt;li&gt;multi-modal retrieval&lt;/li&gt;
          &lt;li&gt;one-shot learning&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Metrics for evaluation of document ranking systems&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Some interesting links for embeddings&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/facebookresearch/StarSpace&quot;&gt;StarSpace&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/facebookresearch/faiss&quot;&gt;FAISS&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Model implementations&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/bmitra-msft/NDRM/blob/master/notebooks/Duet.ipynb&quot;&gt;DUET&lt;/a&gt;&lt;/p&gt;</content><author><name></name></author><category term="nlp," /><category term="ir," /><category term="neural-networks" /><summary type="html">Retrieval of information: a user express their need in the form of a query and retrieval consists in ranking existing pieces of content or incorporating new responses from the obtained information.</summary></entry><entry><title type="html">Topic Model Applications</title><link href="/blog/2020/topic_model_review/" rel="alternate" type="text/html" title="Topic Model Applications" /><published>2020-08-21T09:30:19-04:00</published><updated>2020-08-21T09:30:19-04:00</updated><id>/blog/2020/topic_model_review</id><content type="html" xml:base="/blog/2020/topic_model_review/">&lt;h1 id=&quot;topic-model&quot;&gt;Topic Model&lt;/h1&gt;

&lt;p&gt;First attempts to find topics from data is &lt;strong&gt;Latent Semantic Analysis&lt;/strong&gt; (LSA): find the best low-rank approximation of a document-term matrix. Approximation, based on SVD.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/topic_model/Untitled.png&quot; alt=&quot;Topic%20Model%204b99c1a88603442cbf99baa8af4412c4/Untitled.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;latent-dirichlet-allocation&quot;&gt;Latent Dirichlet Allocation&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Latent&lt;/strong&gt; because we use probabilistic inference to infer missing probabilistic pieces of the generative story.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Dirichlet&lt;/strong&gt; because of the Dirichlet parameters encoding sparsity. Allocation because the Dirichlet distribution encodes the prior for each document’s allocation over topics.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/topic_model/Untitled%201.png&quot; alt=&quot;Topic%20Model%204b99c1a88603442cbf99baa8af4412c4/Untitled%201.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Story&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Generate Topics&lt;/li&gt;
  &lt;li&gt;Document Allocations&lt;/li&gt;
  &lt;li&gt;Words in Context&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Inference (rvs.)&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Topic Assignments&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Document Allocation&lt;/p&gt;

\[\theta_{d,i}\approx\frac{N_{d,i}+\alpha_i}{\sum_kN_{d,k}+\alpha_k}\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Topics&lt;/p&gt;

\[\phi_{i,v}\approx\frac{V_{i,v}+\beta_v}{\sum_wV_{i,w}+\beta_w}\]
  &lt;/li&gt;
  &lt;li&gt;Assign word to a particular topic&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/topic_model/Untitled%202.png&quot; alt=&quot;Topic%20Model%204b99c1a88603442cbf99baa8af4412c4/Untitled%202.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/topic_model/Untitled%203.png&quot; alt=&quot;Topic%20Model%204b99c1a88603442cbf99baa8af4412c4/Untitled%203.png&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;information-retrieval&quot;&gt;Information Retrieval&lt;/h1&gt;

&lt;h2 id=&quot;ad-hoc-ir&quot;&gt;Ad-hoc IR&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;Where IR systems might look for the “needle in the haystack”, topic models will tell you about the overall proportion of hay and needles, and perhaps inform you about the mice that you did not know were there&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p&gt;Topic models helpful when we have a &lt;em&gt;specific information need&lt;/em&gt; but no idea &lt;em&gt;how to search&lt;/em&gt; for it&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Traditionally:&lt;/strong&gt; retrieve and rank documents by measuring the word overlap between queries and documents. Limited! Words with similar meaning or different forms should also be considered as matching keywords.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Language modeling:&lt;/strong&gt; allows to capture semantic relationship&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Query expansion:&lt;/strong&gt; use background knowledge to interpret and understand queries and add missing words.&lt;/p&gt;

&lt;h3 id=&quot;document-language-modeling&quot;&gt;Document Language Modeling&lt;/h3&gt;

&lt;p&gt;Statistical language model estimates the probability of for word sequences&lt;/p&gt;

\[p(w_1,w_2,\dots,w_n)\]

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Often approximate using ngram models.&lt;/p&gt;

    &lt;p&gt;In a unigram model, words in the sequence are independent&lt;/p&gt;

\[p(w_1,w_2,\dots,w_n)=p(w_1)p(w_2)\dots p(w_n)\]

    &lt;p&gt;Trigram assumes probability in a window of two words&lt;/p&gt;

\[p(w_1,w_2,\dots,w_n)=p(w_1)p(w_2\mid w_1)p(w_3\mid w_1,w_2)\dots p(w_n\mid w_{n-2},w_{n-1})\]
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Then generate the probability of generating a given query (maximum likelihood)&lt;/p&gt;

\[p(q\mid d)=\prod_{w\in q}p(w\mid d)=\prod_{w\in q}\frac{n_{d,w}}{n_{d,.}}\]

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;For IR, rank documents based on p(q&lt;/td&gt;
      &lt;td&gt;d), but maximum-likelihood gives zero probability to unseen words  and it can throw out good matches. Solve by allocating non-zero probability to missing terms, &lt;strong&gt;smoothing.&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Smoothing directions&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;interpolation:&lt;/strong&gt; discount contribution of seen words and add contribution of unseen and seen words&lt;/p&gt;

    &lt;p&gt;&lt;em&gt;Jelinek-Mercer:&lt;/em&gt; linear interpolation with the document and corpus using a coefficient, solving the data sparsity problem. Missing words falls back to the probability of the corpus level.&lt;/p&gt;

\[p(w\mid d)=(1-\lambda)p(w\mid d) + \lambda p(w\mid\mathcal{C})\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;backoff:&lt;/strong&gt; trust ML estimation for high count words and redistribute mass for less common words&lt;/p&gt;

    &lt;p&gt;&lt;em&gt;Bayesian Smoothing using Dirichlet Priors:&lt;/em&gt; discrete distribution smoothed by applying Dirichlet priors with a concentration parameter.&lt;/p&gt;

\[p(w\mid d)=\frac{n_{d,w}+\beta p(w\mid\mathcal{C})}{\sum_{v\in V}n_{d,v} +\beta}\]
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;applying-topic-models-to-document-language-models&quot;&gt;Applying Topic Models to Document Language Models&lt;/h3&gt;

&lt;p&gt;Lead relationship between query words and documents by marginalizing all topics.&lt;/p&gt;

\[p(w\mid d) = \sum_k p(w\mid k)p(k\mid d)\]

&lt;ul&gt;
  &lt;li&gt;Combine Topic models and language models, &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/1148170.1148204&quot;&gt;see&lt;/a&gt;.
    &lt;ul&gt;
      &lt;li&gt;Linear Interpolation + Jelinek-Mercer smoothing&lt;/li&gt;
    &lt;/ul&gt;

\[p(w\mid d)=\lambda&apos;\left((1-\lambda)p_{ml}(w\mid d)+\lambda p(w\mid \mathcal{C})\right)+(1-\lambda&apos;)p_{tm}(w\mid d)\]

    &lt;ul&gt;
      &lt;li&gt;Linear Interpolation + Bayesian smoothing&lt;/li&gt;
    &lt;/ul&gt;

\[p(w\mid d)=\lambda\frac{n_{d,w}+\beta p(w\mid\mathcal{C})}{\sum_{v\in V}n_{d,v} +\beta}+(1-\lambda)p_{tm}(w\mid d)\]
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;query-expansion-in-ir&quot;&gt;Query Expansion in IR&lt;/h3&gt;

&lt;p&gt;Query-Word relationships for query expansion, two main steps:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Find relationships between queries and words and select top related words to expand query&lt;/p&gt;

    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;Query language model:&lt;/strong&gt; Combine query content with relevant documents (e.g. clicked documents). Find p(w&lt;/td&gt;
          &lt;td&gt;q).&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;

\[\hat\theta_{Q&apos;}=(1-\lambda)\hat\theta_{Q}+\lambda\hat\theta_{\mathcal{F}}\]

    &lt;p&gt;&lt;strong&gt;Relevance Model:&lt;/strong&gt; query and relevant documents are random samples from an unknown relevance model &lt;em&gt;R&lt;/em&gt;&lt;/p&gt;

\[p(w\mid R)\approx p(w\mid q)=\frac{p(w,q)}{p(q)}=\frac{\sum_{d\in\mathcal{C}}p(d)p(w\mid d)p(q\mid d)}{p(q)}\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;rank expanded queries and rank relevance scores&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;Combine &lt;em&gt;expanded query language model&lt;/em&gt; with &lt;em&gt;query language model&lt;/em&gt; (&lt;a href=&quot;https://www.notion.so/Topic-Model-Applications-425324b816d34f2ea2180d2b66fccc70&quot;&gt;see equation&lt;/a&gt;).
  Then compare topic distributions between query &lt;em&gt;q&lt;/em&gt; generated and document &lt;em&gt;d&lt;/em&gt; generated.&lt;/li&gt;
    &lt;/ul&gt;

\[p(q\mid \hat\theta_{Q&apos;}) \text{ vs } p(d\mid\hat\theta_{D})\]

    &lt;ul&gt;
      &lt;li&gt;Compute relevance score using &lt;em&gt;original query&lt;/em&gt; and &lt;em&gt;expanded query&lt;/em&gt;, the linearly combine the two scores, obtaining a final &lt;em&gt;query-document relevance score.&lt;/em&gt;&lt;/li&gt;
    &lt;/ul&gt;

\[\hat s_d(q)=\lambda s_d(e)+(1-\lambda)s_d(q)\]
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;applying-topic-models-for-query-expansion&quot;&gt;Applying topic models for query expansion&lt;/h3&gt;

&lt;p&gt;Capture semantic relation between words by learning latent topics (distribution over words). Thus, it is a way to expand or match words at the semantic level.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Smoothing query language model:&lt;/strong&gt; make query expansion and compute words relevance from topics directly.&lt;/p&gt;

\[p(w\mid q)=\sum_kp_{tm}(w\mid k)p_{tm}(k\mid q)\]

    &lt;p&gt;However only queries are normally short and topic results are limited. A solution is to train topic models using the relevant documents (e.g. clicked documents)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Improving relevance model:&lt;/strong&gt; improve the &lt;a href=&quot;https://www.notion.so/Topic-Model-Applications-425324b816d34f2ea2180d2b66fccc70&quot;&gt;relevance model&lt;/a&gt;.&lt;/p&gt;

\[p(w\mid q)=\sum_{d\in\mathcal{C}}\left(\lambda p(w\mid d)+(1-\lambda)p_{tm}(w\mid d,q)\right)p(d\mid q)\\\text{,where }p_{tm}(w\mid d,q)=\sum_kp(w\mid k)p(k\mid d)p(q\mid k)\]

    &lt;p&gt;Model captures word relationships on a semantic level: improves query-word relationships and query expansion.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Learning pair-wise word relationships:&lt;/strong&gt; use topic models for query expansion by finding relationship between words and compute document &lt;a href=&quot;https://www.notion.so/Topic-Model-Applications-425324b816d34f2ea2180d2b66fccc70&quot;&gt;rank scores&lt;/a&gt;.&lt;/p&gt;

\[p(w_x\mid w_y,\alpha)=\frac{\sum_kp(w_x\mid k,\alpha)p(w_y\mid k,\alpha)\alpha_k}{\sum_{k&apos;}p(w_y\mid k&apos;,\alpha)\alpha_{k&apos;}}\]

    &lt;p&gt;Select top related terms as the expanded terms &lt;em&gt;e&lt;/em&gt; for a given query &lt;em&gt;q.&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Interactive feedback:&lt;/strong&gt; users feedback for the retrieval process and thus improving results. Show user a initial set of retrieval results and ask for feedback.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;beyond-relevance---search-personalization&quot;&gt;Beyond relevance - search personalization&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Contextualization:&lt;/strong&gt; consider user search activity (e.g. time, location, etc)&lt;/p&gt;

    &lt;p&gt;&lt;em&gt;preference documents:&lt;/em&gt; concatenate clicked documents or top &lt;em&gt;n&lt;/em&gt; ranked documents if no click happened&lt;/p&gt;

    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;Then, rank preference documents using a &lt;a href=&quot;https://www.notion.so/Topic-Model-Applications-425324b816d34f2ea2180d2b66fccc70&quot;&gt;query language model&lt;/a&gt;. First, infer latent topics p(w&lt;/td&gt;
          &lt;td&gt;k) and then query-topics. However, queries &lt;em&gt;q&lt;/em&gt; are often too short, so we could train a language model using the preference documents for query &lt;em&gt;q&lt;/em&gt; and compare the cosine similarity against each topic &lt;em&gt;k&lt;/em&gt; to estimate the query-topic distribution.&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;

\[p(k\mid q)=\frac{p(k,q)}{\sum_kp(k,q)}\approx\frac{\text{sim}(\theta_k,\theta_q)}{\sum_k\text{sim}(\theta_k,\theta_q)}\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Individualization:&lt;/strong&gt; individual characteristics (users’ profile)&lt;/p&gt;

    &lt;p&gt;Given the &lt;em&gt;topic distribution&lt;/em&gt; of the document, there will be words chosen at random to generate the &lt;em&gt;query&lt;/em&gt; and &lt;em&gt;users&lt;/em&gt; who chose to click that document (&lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/2505515.2505642&quot;&gt;see paper&lt;/a&gt;).&lt;/p&gt;

\[p\left(k \mid w_{i}, d_{i}, u_{i}\right)=\frac{p\left(k, w_{i}, u_{i} \mid d_{i}\right)}{p\left(w_{i}, u_{i} \mid d_{i}\right)} \propto p\left(w_{i} \mid k\right) p\left(u_{i} \mid k\right) p\left(k \mid d_{i}\right)\]
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;evaluation-and-interpretation&quot;&gt;Evaluation and Interpretation&lt;/h1&gt;

&lt;p&gt;Users need to understand a model’s output to draw conclusions. Understanding can be done by model visualization, interaction, and evaluation.&lt;/p&gt;

&lt;p&gt;## Display Topics&lt;/p&gt;

&lt;p&gt;Words with highest weight in a topic best explain what the topic is about.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Word lists&lt;/strong&gt; are a good way of showing topics, use lists, sets, tables, bars for word probability.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Word clouds&lt;/strong&gt; use size of words to convey information.&lt;/p&gt;

    &lt;p&gt;Also, &lt;a href=&quot;Concurrent visualization of relationships between words and topics in topic models. In&quot;&gt;plot word clouds with word&lt;/a&gt; &lt;a href=&quot;https://www.aclweb.org/anthology/W14-3112.pdf&quot;&gt;associations&lt;/a&gt;&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/img/topic_model/Untitled_00.png&quot; alt=&quot;Evaluation%20and%20Interpretation%20485a6b35e0644dfe8d7c0120d3be35a6/Untitled.png&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;labeling-topics&quot;&gt;Labeling Topics&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;labeling focuses on showing not the original words of a topic but rather a clearer label more akin to what a human summary of the data would provide.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;Internal Information (unsupervised)
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Internal Labeling:&lt;/strong&gt; take prominent phrases from the topic and compares how consistent the phrase context is with the topic distribution. It can be extended for &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/2396761.2398646&quot;&gt;hierarchies&lt;/a&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;External Knowledge (higher quality)
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Labelling with Supervised Labels:&lt;/strong&gt; use human annotators to select a top word topic using a SVR (predict most representative word), &lt;a href=&quot;https://www.aclweb.org/anthology/C10-2069.pdf&quot;&gt;paper&lt;/a&gt;.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Labeling with Knowledge Bases:&lt;/strong&gt; 1) align topic models with an external ontology; 2) build a graph, find matching words and rank them with page rank&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Using Labeled Documents:&lt;/strong&gt;
        &lt;ol&gt;
          &lt;li&gt;&lt;a href=&quot;https://www.aclweb.org/anthology/D09-1026.pdf&quot;&gt;Labeled LDA&lt;/a&gt; provides consistent topics with the target variable.&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/2232817.2232861&quot;&gt;Labeled Pachinko Allocation&lt;/a&gt; for hierarchical label sets (e.g. labels: Ecuador → Country)&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;https://www.aclweb.org/anthology/N15-1076.pdf&quot;&gt;Sup. Anchor:&lt;/a&gt; Use labels to compute anchor words, which are words that might define a topic.&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;displaying-models&quot;&gt;Displaying models&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Find relevant documents:&lt;/strong&gt; select a particular document-topic and sort them from largest to smallest.&lt;/li&gt;
  &lt;li&gt;Plot how words are used within a topic&lt;/li&gt;
  &lt;li&gt;How topics relate to each other?!&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;evaluation-stability-and-repair&quot;&gt;Evaluation, Stability and Repair&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/1553374.1553515&quot;&gt;Evaluate&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Held-out likelihood of a model&lt;/li&gt;
  &lt;li&gt;TM is a generative model: predict next unseen word&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Find errors&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Documents with poor held-out likelihood are random noise&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;On the other hand, hundreds topics so specific that any held-out document is modeled well yields an excellent held-out likelihood.&lt;/p&gt;

  &lt;ul&gt;
    &lt;li&gt;Held-out likelihood emphasizes &lt;a href=&quot;https://papers.nips.cc/paper/3700-reading-tea-leaves-how-humans-interpret-topic-models.pdf&quot;&gt;complexity&lt;/a&gt; but interpretability&lt;/li&gt;
    &lt;li&gt;Hyper-parameters play a important &lt;a href=&quot;https://papers.nips.cc/paper/3854-rethinking-lda-why-priors-matter.pdf&quot;&gt;role&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://www.aclweb.org/anthology/E14-1056.pdf&quot;&gt;Topic quality&lt;/a&gt; is a proxy to human interpretability but it doesn’t tell the parts where the model isn’t reliable or fails.&lt;/li&gt;
    &lt;li&gt;Check if a dataset is &lt;a href=&quot;http://proceedings.mlr.press/v32/tang14.pdf&quot;&gt;good enough&lt;/a&gt; to apply topic modeling&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;a mismatch between the number of topics and documents, topics that are “too close together”, or a mismatch between the number of topics in a document and the Dirichlet parameter α.&lt;/p&gt;

&lt;/blockquote&gt;

&lt;h2 id=&quot;some-projects&quot;&gt;Some projects&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://sana-malik.github.io/topicflow/TopicFlow.html#&quot;&gt;TopicFLow&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://mimno.infosci.cornell.edu/jsLDA/jslda.html&quot;&gt;jslda&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/ajbc/tmv&quot;&gt;Topic Model Visualization Engine&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.cs.cmu.edu/~dchau/topicviz/topicviz.mp4&quot;&gt;TopicViz&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/StanfordHCI/termite&quot;&gt;Terminate&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;future-trends-and-other-models&quot;&gt;Future trends and other models&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;https://guidedlda.readthedocs.io/en/latest/&quot;&gt;Guided LDA&lt;/a&gt; to seed topics with specific words or more recently &lt;a href=&quot;https://yuzhimanhua.github.io/papers/www20.pdf&quot;&gt;CatE&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/0708.3601.pdf&quot;&gt;Dynamic Topic Model&lt;/a&gt;: each topic has a distinct distribution over words for each time period&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1206.3298.pdf&quot;&gt;cDTM&lt;/a&gt;: time is considered continuously&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://cocosci.princeton.edu/tom/papers/author_topics_kdd.pdf&quot;&gt;Author-Topic model:&lt;/a&gt;  an author has a collection of topics that they write about and each document is a combination of the topics that its set of authors care about.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.cs.sfu.ca/~jpei/publications/Topic%20Evolution%20CIKM09.pdf&quot;&gt;Inheritance topic model:&lt;/a&gt; adapt LDA to a citation network&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.cs.columbia.edu/~blei/papers/GerrishBlei2010.pdf&quot;&gt;Dynamic influence model&lt;/a&gt;: Find most influential documents&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.aaai.org/Papers/ICWSM/2008/ICWSM08-018.pdf&quot;&gt;LinkLDA&lt;/a&gt;: relationship between documents&lt;/p&gt;

&lt;h2 id=&quot;short-documents&quot;&gt;Short documents&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;it can &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/1964858.1964870&quot;&gt;confuse&lt;/a&gt; topic modeling algorithms see these studies (&lt;a href=&quot;https://ink.library.smu.edu.sg/cgi/viewcontent.cgi?article=2374&amp;amp;context=sis_research&quot;&gt;see&lt;/a&gt; for a specific application in Twitter; also, interesting on how to apply and analyze topic models)&lt;/li&gt;
  &lt;li&gt;to &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/2484028.2484166&quot;&gt;capture trends&lt;/a&gt; over time or across users, algorithms must know these connections between users or messages&lt;/li&gt;
  &lt;li&gt;Topic model for &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/2566486.2567980&quot;&gt;sparse data&lt;/a&gt; or &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/2623330.2623715&quot;&gt;short texts&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;supervised-topic-model&quot;&gt;Supervised Topic model&lt;/h2&gt;

&lt;p&gt;Results vary based on the number of topics. The &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.1145/1667053.1667056&quot;&gt;nested Dirichlet process&lt;/a&gt; provide a way of selecting the appropriate number of topics.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.aclweb.org/anthology/D09-1026/&quot;&gt;Labelled LDA&lt;/a&gt;: it can do multi-class (see &lt;a href=&quot;https://www.aaai.org/ocs/index.php/ICWSM/ICWSM10/paper/download/1528/1846&quot;&gt;application&lt;/a&gt; on Twitter)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1003.0783.pdf&quot;&gt;Supervised LDA&lt;/a&gt;:  sLDA for different response types&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.jmlr.org/papers/volume13/zhu12a/zhu12a.pdf&quot;&gt;MedLDA:&lt;/a&gt; changes objective function of sLDA that improve predictions (uses maximum margin similar to SVM to improve predictions)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;embedding-topic-model&quot;&gt;Embedding Topic Model&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://dl.acm.org/doi/fullHtml/10.1145/3366423.3380102&quot;&gt;GATON&lt;/a&gt;: leverages the graph structure and word embeddings&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1907.04907&quot;&gt;ETM&lt;/a&gt;: Learns a topic embedding and word embedding end-to-end&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1907.05545&quot;&gt;DETM&lt;/a&gt;: Similar to ETM but considers time&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;some-notes&quot;&gt;Some notes&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Interesting &lt;a href=&quot;http://www.cs.cmu.edu/~yohanj/research/papers/WSDM11.pdf&quot;&gt;application&lt;/a&gt; for reviews and sentiments&lt;/li&gt;
  &lt;li&gt;Thematic variation is a hard problem in topic models since we expect that documents won’t change while reading (e.g.a novel)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;https://mimno.infosci.cornell.edu/topics.html&quot;&gt;Bibliography&lt;/a&gt;&lt;/p&gt;</content><author><name></name></author><category term="topic-model" /><summary type="html">Topic Model</summary></entry><entry><title type="html">Graph Representation Learning notes</title><link href="/blog/2020/graph-representation-learning/" rel="alternate" type="text/html" title="Graph Representation Learning notes" /><published>2020-01-25T14:41:19-05:00</published><updated>2020-01-25T14:41:19-05:00</updated><id>/blog/2020/graph-representation-learning</id><content type="html" xml:base="/blog/2020/graph-representation-learning/">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Graphs are everywhere and most knowledge can be modeled with graphs. Graphs can be understood as a complex data structure that can be used as a universal language. Therefore, as any language, graphs communicate ideas using nodes (objects) and edges (interaction); thus any complex system can be modeled by creating relationships between points. We focus in the use machine learning on graphs, but the field of network analysis provide other approaches to analyze, understand and learn from graphs.&lt;/p&gt;

&lt;h3 id=&quot;graphs-data-structure&quot;&gt;Graphs: data structure&lt;/h3&gt;
&lt;p&gt;Graphs are composed with a set of nodes \(\mathcal{V}\) on a set de edges \(\mathcal{E}\) that define a relation \(\mathcal{G}=(\mathcal{V}, \mathcal{E})\).&lt;/p&gt;

\[u,v\in\mathcal{V}\]

&lt;p&gt;Also, notice that &lt;em&gt;graph&lt;/em&gt; and &lt;em&gt;network&lt;/em&gt; are terms used interchangeably, but graphs involves properties of real-world data while a network provides mathematical properties for graphs.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Adjacency Matrix&lt;/strong&gt;: \(A\in \mathbb{R}^{\mid V\mid\times\mid V\mid}
\begin{cases}
  A_{uv}=1&amp;amp; \text{if } (u,v)\in\mathcal{E}\\
  A_{uv}=0              &amp;amp; \text{otherwise}
\end{cases}\)
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;directed&lt;/em&gt;: no symmetric&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;undirected&lt;/em&gt;: symmetric&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;weighted&lt;/em&gt;: real values \(\{0,1\}\)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Graph Types&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;Simple Graph&lt;/em&gt;: relation \((u,v)\in\mathcal{E}\)&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;Multi-relational&lt;/em&gt;: considers type of edge with \(R\) types, i.e. \((u,\tau,v)\in\mathcal{E}, A_\tau\) and \(A\in\mathbb{R}^{\mid V\mid\times\mid V\mid\times\mid R\mid}\)&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;Attributes&lt;/em&gt;: some graphs have information associated with each node \(X\in\mathbb{R}^{\mid V\mid\times\mid m\mid}\).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;machine-learning-tasks-on-graphs&quot;&gt;Machine Learning tasks on Graphs&lt;/h3&gt;
&lt;p&gt;ML on graphs involves common ML tasks such as 1) supervised learning: predict a target output; 2) unsupervised learning: infer patterns; 3) reinforcement learning: learn to act in an environment, but there are some task on graphs that lie on these categories.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Node classification&lt;/strong&gt;: Consist in predicting a label \(y_u\) associated with a node \(u\in\mathcal{V}\).&lt;/p&gt;

    &lt;p&gt;Modeling in graphs is different since nodes are interconnected with each other and they are not i.i.d. Thus, some connection between nodes is needed. For example:&lt;/p&gt;
    &lt;ol&gt;
      &lt;li&gt;homophily: nodes share attributes.&lt;/li&gt;
      &lt;li&gt;structural equivalence: nodes with similar local neighborhood structure have similar labels.&lt;/li&gt;
      &lt;li&gt;monophily: nodes with unrelated labels.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Relation prediction&lt;/strong&gt;: aka. link prediction or graph completion seeks to infer the relationship of a node with other nodes in the graph. The problems involves that given a set of nodes \(\mathcal{V}\) and an incomplete set of edges \(\mathcal{E}_{\text{train}}\in\mathcal{E}\), infer the missing edges \(\mathcal{E}\setminus\mathcal{E}_{\text{train}}\). Complexity is highly depend on the type of graph (i.e. simple graph or multi-relational graph) and it often requires inductive biases specific for each graph domain.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Clustering and community detection&lt;/strong&gt;: An unsupervised task that finds a community structure where nodes are more likely to form edges with nodes that belong to the same community. The challenge is to infer the &lt;em&gt;community structure&lt;/em&gt; given an input graph \(\mathcal{G}=(\mathcal{V},\mathcal{E})\).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Graph classification/clustering&lt;/strong&gt;: Task over entire graphs. Graph classification involves that given multiple different graphs, learn predictions specific to each graph. And graph clustering (similarity matching) the goal is to learn an unsupervised measure of similarity between a set of i.i.d. graphs. Challenge is to define useful features that consider the relational structure of each datapoint.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;traditional-approaches&quot;&gt;Traditional approaches&lt;/h3&gt;
&lt;p&gt;Uses for node classification tasks&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;graph statistics&lt;/li&gt;
  &lt;li&gt;kernel methods
Link prediction&lt;/li&gt;
  &lt;li&gt;measure overlap between node neighborhoods
Clustering / community detection&lt;/li&gt;
  &lt;li&gt;spectral clustering (graph Laplacians)&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;graphs-statistics-and-kernel-methods&quot;&gt;Graphs statistics and kernel methods&lt;/h4&gt;
&lt;p&gt;Extract statistics or features and use them in traditional machine learning classifiers.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Node level statistics and features&lt;/strong&gt;:
    &lt;ol&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Node degree&lt;/strong&gt;: it is the number of incident edges to a node.&lt;/p&gt;

\[d_u=\sum_{v\in V}A_{u,v}\]
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Node centrality&lt;/strong&gt;: measures the importance of a node; it takes into account the importance of a node’s neighbors.&lt;/p&gt;

\[e_u=\frac{1}{\lambda}\sum_{v\in V}A_{u,v}e_v \forall_u \in V \text{   (eigenvector centrality)}\]

        &lt;p&gt;We can compute the vector of node centrality \(\mathbb{e}\) using the eigenvector equation with the adjacency matrix \(\lambda\mathbb{e}=A\mathbb{e}\).&lt;/p&gt;

        &lt;p&gt;We can use the eigenvector centrality to rank the likelihood that a node has visited infinity nodes on a random walk (due to the power of iteration) \(\mathbb{e}^{(t+1)}=A\mathbb{e^{(t)}}\).&lt;/p&gt;

        &lt;p&gt;Other measures:&lt;/p&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;em&gt;betweeness centrality&lt;/em&gt;: measures how often a node lies on the shortest path between two other nodes.&lt;/li&gt;
          &lt;li&gt;&lt;em&gt;closeness centrality&lt;/em&gt;: measures the average shortest path length between a node and all other nodes.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Clustering coefficient&lt;/strong&gt;:&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Motifs&lt;/strong&gt;:&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Graph-level stats -&amp;gt; kernel methods&lt;/p&gt;</content><author><name></name></author><category term="GRL" /><summary type="html">Introduction</summary></entry><entry><title type="html">Notes on the Bayesian Framework</title><link href="/blog/2019/bayesian-framework/" rel="alternate" type="text/html" title="Notes on the Bayesian Framework" /><published>2019-11-01T21:41:19-04:00</published><updated>2019-11-01T21:41:19-04:00</updated><id>/blog/2019/bayesian-framework</id><content type="html" xml:base="/blog/2019/bayesian-framework/">&lt;p&gt;Some tools:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Stochastic variational inference&lt;/li&gt;
  &lt;li&gt;Variance reduction&lt;/li&gt;
  &lt;li&gt;Normalizing flows&lt;/li&gt;
  &lt;li&gt;Gaussian processes&lt;/li&gt;
  &lt;li&gt;Scalable MCMC algorithms&lt;/li&gt;
  &lt;li&gt;Semi-implicit variational inference&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;bayesian-framework&quot;&gt;Bayesian framework&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Bayes theorem&lt;/strong&gt;:&lt;/li&gt;
&lt;/ul&gt;

\[conditional = \frac{joint}{marginal}\]

\[p(x\mid y)=\frac{p(x,y)}{p(y)}\]

&lt;p&gt;It defines a rule for uncertainty conversion when new information arrives&lt;/p&gt;

\[posterior = \frac{likelihood \times prior}{evidence}\]

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Product rule&lt;/strong&gt;: any joint distribution can be expressed with conditional distributions&lt;/li&gt;
&lt;/ul&gt;

\[p(x,y,z)=p(x\mid y,z)p(y\mid z)p(z)\]

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Sum rule&lt;/strong&gt;: any marginal distribution can be obtained from the joint distribution by integrating out&lt;/li&gt;
&lt;/ul&gt;

\[p(y)=\int p(x,y)dx\]

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Statistical inference&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;&lt;strong&gt;Problem&lt;/strong&gt;: given i.i.d. data \(X=\{x_i\}_{i=1}^n\) from distribution \(p(x\mid\theta)\), estimate \(\theta\)&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;&lt;strong&gt;Frequentist framework&lt;/strong&gt;: use maximum likelihood estimation (MLE)&lt;/li&gt;
    &lt;/ol&gt;

\[\theta_{ML}=\operatorname*{arg\,max}p(X\mid\theta)=\operatorname*{arg\,max}\prod_{i=1}^n p(x_i\mid\theta)=\operatorname*{arg\,max}\sum_i^n\log p(x_i\mid\theta)\]

    &lt;p&gt;Applicability: \(n\ll d\)&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;&lt;strong&gt;Bayesian framework&lt;/strong&gt;: encode uncertainty about \(\theta\) in a prior \(p(\theta)\) and apply Bayesian inference&lt;/li&gt;
    &lt;/ol&gt;

\[p(\theta\mid X)=\frac{\prod_i^n p(x_i\mid\theta)p(\theta)}{\int\prod_i^n p(x_i\mid\theta)p(\theta)d\theta}\]

    &lt;p&gt;Applicability: \(\forall_nd\)&lt;/p&gt;

    &lt;p&gt;Advantages:
    - we can encode prior knowledge/desired properties into a prior distribution
    - prior is a form of regularization
    - additionally to the point estimate of \(\theta\), posterior contains information about the uncertainty of the estimate
    - frequentist case is a limit case of Bayesian one
      \(\lim_{n/d\to\infty}p(\theta\mid x_1,\dots,x_n)=\delta(\theta-\theta_{ML})\)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;bayesian-ml-models&quot;&gt;Bayesian ML models&lt;/h1&gt;
&lt;p&gt;In ML, we have \(x\) features (observed variables) and \(y\) class labels or hidden representations (hidden or latent variables) with some model parameters \(\theta\) (e.g. weights of a linear model).&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Discriminative approach, models \(p(y,\theta\mid x)\)&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Cannot generate new objects since it needs \(x\) as an input and assumes that the prior over \(\theta\) does not depend on \(x\): \(p(y,\theta)=p(y\mid x,\theta)p(\theta)\)&lt;/li&gt;
      &lt;li&gt;Examples: 1) classification/regression (hidden space is small) 2) Machine translation (complex hidden space)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Generative approach, models \(p(x,y,\theta)=p(x,y\mid\theta)p(\theta)\)&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;It can generate objects (pairs \(p(x,y)\)), but it can be hard to train since the observed space is most often more complicated.&lt;/li&gt;
      &lt;li&gt;Example: Generation of text, speech, images, etc.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Training&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;Given data points \((X_{tr}, Y_{tr})\) and a discriminative model \(p(y,\theta\mid x)\).&lt;/p&gt;

    &lt;p&gt;&lt;em&gt;Use the Bayesian framework:&lt;/em&gt;&lt;/p&gt;

\[p(\theta\mid X_{tr}, Y_{tr})=\frac{p(Y_{tr}\mid X_{tr},\theta)p(\theta)}{\int p(Y_{tr}\mid X_{tr},\theta)p(\theta) d\theta}\]

    &lt;p&gt;This results in a ensemble of algorithms rather than a single one \(\theta_{ML}\). Ensembles usually performs better than a single model.&lt;/p&gt;

    &lt;p&gt;In addition, the posterior captures all dependencies from the training data and can be used later as a new prior.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Testing&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;We have the posterior \(p(\theta\mid X_{tr},Y_{tr})\) and a new data point \(x\). We can use the predictive distribution on its hidden value \(y\)&lt;/p&gt;

\[p(y\mid x, X_{tr},Y_{tr}) = \int p(y\mid x,\theta)p(\theta\mid X_{tr},Y_{tr})d\theta\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Full Bayesian inference&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;During training the evidence \(\int p(Y_{tr}\mid X_{tr},\theta)p(\theta) d\theta\) or in testing the predictive distribution \(\int p(y\mid x,\theta)p(\theta\mid X_{tr},Y_{tr})d\theta\) might be intractable, so it is impractical or impossible to perform full Bayesian inference. In other words, there is not closed form.&lt;/p&gt;

&lt;h1 id=&quot;conjugacy&quot;&gt;Conjugacy&lt;/h1&gt;
&lt;h2 id=&quot;conjugate-distributions&quot;&gt;Conjugate distributions&lt;/h2&gt;

&lt;p&gt;Distribution \(p(y)\) and \(p(x\mid y)\) are &lt;a href=&quot;(https://en.wikipedia.org/wiki/Conjugate_prior)&quot;&gt;conjugate&lt;/a&gt; \(\iff\) \(p(y\mid x)\) belongs to the same parametric family as \(p(y)\)&lt;/p&gt;

\[p(y)\in\mathcal{A}(\alpha), p(x\mid y)\in\mathcal{B}(y) \rightarrow p(y\mid x)\in\mathcal{A}(\alpha&apos;)\]

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;There’s not conjugacy&lt;/strong&gt; We can perform MAP to approximate the posterior with \(\theta_{MP}\) since we don’t need to calculate the normalization constant, but we cannot compute the true posterior.&lt;/li&gt;
&lt;/ul&gt;

\[\theta_{MP}=\operatorname*{arg\,max}p(\theta\mid X_{tr},Y_{tr})=\operatorname*{arg\,max}p(Y_{tr}\mid X_{tr},\theta)p(\theta)\]

&lt;p&gt;During testing:&lt;/p&gt;

\[p(y\mid x,X_{tr},Y_{tr})=\int p(y\mid x,\theta)p(\theta\mid X_{tr},Y_{tr})d\theta\approx p(y\mid x,\theta_{MP})\]

&lt;h2 id=&quot;conditional-conjugacy&quot;&gt;Conditional conjugacy&lt;/h2&gt;
&lt;p&gt;Given the model: \(p(x,\theta)=p(x\mid\theta)p(\theta)\) where \(\theta=[\theta_1,\dots,\theta_m]\)&lt;/p&gt;

&lt;p&gt;Conditional conjugacy of likelihood and prior on each \(\theta_j\) conditional on all other \(\{\theta_i\}_{i\neq j}\)&lt;/p&gt;

\[p(\theta_j\mid\theta_{i\neq j})\in\mathcal{A}(\alpha), p(x\mid\theta_j,\theta_{i\neq j})\in\mathcal{B}(\theta_j) \rightarrow p(\theta_j\mid x,\theta_{i\neq j})\in\mathcal{A}(\alpha&apos;)\]

&lt;p&gt;Check conditional conjugacy in practice:
For each \(\theta_j\)&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Fix all other \(\{\theta_i\}_{i\neq j}\) (look at them as constants)&lt;/li&gt;
  &lt;li&gt;Check whether \(p(x\mid\theta)\) and \(p(\theta)\) are conjugate w.r.t. \(\theta_j\)&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;variational-inference&quot;&gt;Variational Inference&lt;/h1&gt;
&lt;p&gt;Given the model \(p(x,\theta)=p(x\mid \theta)p(\theta)\), find a posterior approximation \(p(\theta\mid x) \approx q(\theta)\in\mathcal{Q}\), such that:&lt;/p&gt;

\[KL(q(\theta)\parallel p(\theta\mid x)) \rightarrow \min_{q(\theta)\in\mathcal{Q}}\]

&lt;p&gt;KL is a good mismatch measure between two distributions over the &lt;strong&gt;same domain&lt;/strong&gt; (see figure). And it has the following properties:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
\[KL(q\parallel p) \geq 0\]
  &lt;/li&gt;
  &lt;li&gt;
\[KL(q\parallel p)=0 \Leftrightarrow q=p\]
  &lt;/li&gt;
  &lt;li&gt;
\[KL(q\parallel p \neq KL(p\parallel q))\]
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/kl/kl_mismatch.png&quot; alt=&quot;KL&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;evidence-lower-bound-elbo-derivation&quot;&gt;Evidence Lower Bound (ELBO) derivation&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Posterior&lt;/strong&gt;: \(p(\theta\mid x)\)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Evidence&lt;/strong&gt;: \(p(x)\), shows the total probability of the observing data.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Lower bound&lt;/strong&gt;: \(\log p(x) \geq \mathcal{L}(q(\theta))\)&lt;/li&gt;
&lt;/ul&gt;

\[\begin{align*}
  \log p(x)  &amp;amp;= \int q(\theta) \log p(x)d\theta\\
            &amp;amp;= \int q(\theta) \log\frac{p(x,\theta)}{p(\theta\mid x)}d\theta\\
            &amp;amp;= \int q(\theta) \log\frac{p(x,\theta)q(\theta)}{p(\theta\mid x)q(\theta)}d\theta\\
            &amp;amp;= \int q(\theta) \log\frac{p(x,\theta)}{q(\theta)}d\theta + \int q(\theta) \log\frac{q(\theta)}{p(\theta\mid x)}d\theta\\
            &amp;amp;= \mathcal{L}(q(\theta)) + KL(q(\theta)\parallel p(\theta\mid x))
\end{align*}\]

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;\(\log p(x)\) does &lt;strong&gt;not depend&lt;/strong&gt; on \(q\)&lt;/li&gt;
  &lt;li&gt;\(\mathcal{L}\) and \(KL\) &lt;strong&gt;depend&lt;/strong&gt; on \(q\)&lt;/li&gt;
  &lt;li&gt;minimizing \(KL\) is the same as maximizing \(\mathcal{L}\).&lt;/li&gt;
&lt;/ul&gt;

\[KL(q(\theta)\parallel p(\theta\mid x)) \rightarrow \min_{q(\theta)\in\mathcal{Q}} \Leftrightarrow \mathcal{L}(q(\theta)) \rightarrow\max_{q(\theta)\in\mathcal{Q}}\]

&lt;h3 id=&quot;optimizing-elbo-mathcall&quot;&gt;Optimizing ELBO \(\mathcal{L}\)&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Goal:&lt;/strong&gt; \(\mathcal{L}(q(\theta)) \rightarrow\max_{q(\theta)\in\mathcal{Q}}\)&lt;/p&gt;

\[\begin{align*}
      \mathcal{L}(q(\theta)) &amp;amp;= \int q(\theta) \log\frac{p(x,\theta)}{q(\theta)}d\theta\\    
                             &amp;amp;= \int q(\theta) \log p(x\mid\theta)d\theta +
                                \int q(\theta) \log\frac{p(\theta)}{q(\theta)}d\theta\\
                             &amp;amp;= \mathbb{E}_{q(\theta)} \log p(x\mid\theta)
                                - KL(q(\theta)\parallel p(\theta))
  \end{align*}\]

&lt;ul&gt;
  &lt;li&gt;Data term: \(\mathbb{E}_{q(\theta)} \log p(x\mid\theta)\)&lt;/li&gt;
  &lt;li&gt;Regularizer: \(KL(q(\theta)\parallel p(\theta))\)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Necessary to perform optimization w.r.t. a distribution \(\max_{q(\theta)\in\mathcal{Q}} \mathcal{L}(q(\theta))\). Hard problem! In VI, we approximate with an approximate distribution \(q\). This approximate distribution can belong to a factorized or parametric family.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Mean field approximation&lt;/strong&gt;: Factorized family, \(q(\theta)=\prod_{j=1}^m q_j(\theta_j)\), \(\theta=[\theta_1,\dots,\theta_m]\)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Parametric approximation&lt;/strong&gt;: Parametric family, \(q(\theta)=q(\theta\mid \lambda)\)&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;mean-field-approximation&quot;&gt;Mean Field Approximation&lt;/h4&gt;
&lt;p&gt;Mean field assumes that \(\theta_1,\dots,\theta_m\) are independent.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Apply product rule to distribution \(q\): \(q(\theta)=\prod_{j=1}^m q_j(\theta_j\mid\theta_{&amp;lt;j})\)&lt;/li&gt;
  &lt;li&gt;Apply i.i.d. assumption: \(q(\theta)=\prod_{j=1}^m q_j(\theta_j)\)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The optimization problem becomes:&lt;/p&gt;

\[\max_{\prod_{j=1}^m q_j(\theta_j)\in\mathcal{Q}} \mathcal{L}(q(\theta))\]

&lt;p&gt;This can be solved with &lt;strong&gt;block coordinate assent&lt;/strong&gt; as follows: &lt;strong&gt;at each step fix all factors \(\{q_i(\theta_i)\}_{i\neq j}\) except one and optimize w.r.t. to it \(\max_{q_j(\theta_j)}\mathcal{L}(q(\theta))\)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Derivation&lt;/strong&gt;&lt;/p&gt;

\[\begin{align*}
      \mathcal{L}(q(\theta)) &amp;amp;= \mathbb{E}_{q(\theta)} \log p(x,\theta)
                              - \mathbb{E}_{q(\theta)} \log q(\theta) \\
                             &amp;amp;= \mathbb{E}_{q(\theta)} \log p(x,\theta)
                                -  \sum_{k=1}^m \mathbb{E}_{q_k(\theta_k)} \log q_k(\theta_k) \\
                             &amp;amp;= \mathbb{E}_{q(\theta)} \log p(x,\theta)
                                -  \mathbb{E}_{q_j(\theta_j)} \log q_j(\theta_j) + C \\
                                &amp;amp;= \{r_j(\theta_j)=\frac{1}{Z_j}\exp(\mathbb{E}_{q_{i\neq j}} \log p(x,\theta))\}\\
                                &amp;amp;= \mathbb{E}_{q_j(\theta_j)} \log r_j(\theta_j)
                                   -  \mathbb{E}_{q_j(\theta_j)} \log q_j(\theta_j) + C \\
                                &amp;amp;= - KL(q_j(\theta_j)\parallel r_j(\theta_j)) + C
  \end{align*}\]

&lt;p&gt;So, the optimization problem for step \(j\) is:&lt;/p&gt;

\[\max_{q_j(\theta_j)}\mathcal{L}(q(\theta)) = \max_{q_j(\theta_j)} - KL(q_j(\theta_j)\parallel r_j(\theta_j)) + C\]

&lt;p&gt;Where this happens when:&lt;/p&gt;

\[q_j(\theta_j) = r_j(\theta_j) = \frac{1}{Z_j}\exp(\mathbb{E}_{q_{i\neq j}} \log p(x,\theta))\]

\[\log q_j(\theta_j) = \mathbb{E}_{q_{i\neq j}} \log p(x,\theta) + C\]

&lt;p&gt;Block coordinate assent can be described in two steps 1) initialize; 2) iterate&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Initialize: \(q(\theta)=\prod_{j=1}^m q_j(\theta_j)\)&lt;/li&gt;
  &lt;li&gt;Iterate (repeat until ELBO converge):
    &lt;ul&gt;
      &lt;li&gt;Update each factor \(q_1,\dots,q_m\): \(q_j(\theta_j)=\frac{1}{Z_j}\exp(\mathbb{E}_{q_{i\neq j}} \log p(x,\theta))\)&lt;/li&gt;
      &lt;li&gt;Compute ELBO \(\mathcal{L}(q(\theta))\)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Mean-field can be applied when we can compute analytically \(\mathbb{E}_{q_{i\neq j}} \log p(x,\theta)\). In other words, applicable when we can compute the conditional conjugacy.&lt;/p&gt;

&lt;h4 id=&quot;parametric-approximation&quot;&gt;Parametric Approximation&lt;/h4&gt;
&lt;p&gt;Select a parametric family of variational distributions, \(q(\theta)=q(\theta\mid\lambda)\), where \(\lambda\) is a variational parameter.&lt;/p&gt;

&lt;p&gt;The restriction is that we need to select a family of some fixed form, and as a result:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;it might be too simple and insufficient to model the data&lt;/li&gt;
  &lt;li&gt;if it is complex enough then there is no guarantee we can train it well to fit the data&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The ELBO is:&lt;/p&gt;

\[\max_{\lambda}\mathcal{L}(q\theta\mid\lambda)=\int q(\theta\mid\lambda)\log\frac{p(x\mid\theta)}{q(\theta\mid\lambda)}d\theta\]

&lt;p&gt;If we’re able to calculate derivatives of ELBO w.r.t \(\theta\), then we can solve this problem using some numerical optimization solver.&lt;/p&gt;

&lt;h3 id=&quot;inference-methods&quot;&gt;Inference methods&lt;/h3&gt;
&lt;p&gt;So we have:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Full Bayesian inference: \(p(\theta\mid x)\)&lt;/li&gt;
  &lt;li&gt;MAP inference: \(p(\theta\mid x)\approx \delta (\theta-\theta_{MP})\)&lt;/li&gt;
  &lt;li&gt;Mean field variational inference: \(p(\theta\mid x)\approx q(\theta)=\prod_{j=1}^m q_j(\theta_j)\)&lt;/li&gt;
  &lt;li&gt;Parametric variational inference: \(p(\theta\mid x)\approx q(\theta)=q(\theta\mid\lambda)\)&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;latent-variable-model&quot;&gt;Latent variable model&lt;/h1&gt;
&lt;h2 id=&quot;mixture-of-gaussians&quot;&gt;Mixture of Gaussians&lt;/h2&gt;
&lt;p&gt;Establish a latent variable \(z_i\) for each data point \(x_i\) that denotes the \(ith\) gaussian where the model was generated.&lt;/p&gt;

&lt;p&gt;Model:&lt;/p&gt;

\[\begin{align*}
    p(X,Z\mid\theta) &amp;amp;= \prod_i^n p(x_i,z_i\mid\theta)\\
                     &amp;amp;= \prod_i^n p(x_i\mid z_i,\theta)p(z_i\mid\theta)\\
                     &amp;amp;= \prod_i^n \pi_{z_i}\mathcal{N}(x_i\mid\mu_{z_i},\sigma_{z_i}^2)
  \end{align*}\]

&lt;p&gt;where \(\pi_j=p(z_i=j)\) is the prior of the \(jth\) gaussian and \(\theta=\{\mu_j,\sigma^2_j,\pi_j\}_{j=1}^K\) are the parameters to estimate.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; If \(X\) and \(Z\) are known, we can use ML. For instance:&lt;/p&gt;

\[\begin{align*}
    \theta_{ML}&amp;amp;=\operatorname*{arg\,max}_{\theta} p(X,Z\mid\theta)\\
               &amp;amp;=\operatorname*{arg\,max}_{\theta} \log p(X,Z\mid\theta)
  \end{align*}\]

&lt;ul&gt;
  &lt;li&gt;Since \(Z\) is a latent variable, we need to maximize the log of incomplete likelihood w.r.t. \(\theta\).&lt;/li&gt;
  &lt;li&gt;Instead of optimizing \(\log p(X\mid \theta)\), we optimize the variational lower bound w.r.t. to both \(\theta\) and \(q(Z)\)&lt;/li&gt;
  &lt;li&gt;This can be solved by block-coordinate algorithm a.k.a. EM-algorithm.&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Variational Lower Bound:&lt;/strong&gt; \(g(\xi,\theta)\) is the variational lower bound function for \(f(x)\) iff:&lt;/p&gt;
  &lt;ol&gt;
    &lt;li&gt;For all \(\xi\) for all \(x\): \(f(x)\geq g(\xi,x)\)&lt;/li&gt;
    &lt;li&gt;For any \(x_0\) exists \(\xi(x_0)\) such that: \(f(x_0)=g(\xi(x0),x_0)\)&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;If we find such variational lower bound, instead of solving
\(f(x)\rightarrow\max_x\), we can interatively perform block coordinate updates of \(g(\xi, x)\).&lt;/p&gt;
  &lt;ol&gt;
    &lt;li&gt;
\[x_n=\operatorname*{arg\,max}_{x}g(\xi_{n-1},x)\]
    &lt;/li&gt;
    &lt;li&gt;
\[\xi_n=\xi(x_n)=\operatorname*{arg\,max}_{\xi} g(\xi,x_n)\]
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Expectation Maximization algorithm&lt;/strong&gt;
We want to solve:&lt;/p&gt;

\[\operatorname*{arg\,max}_{q,\theta}\mathcal{L}(q, \theta) = \operatorname*{arg\,max}_{q,\theta}\int q(Z)\frac{p(X,Z\mid\theta)}{q(Z)}dZ\]

&lt;p&gt;&lt;strong&gt;Algorithm&lt;/strong&gt;:
Set an initial point \(\theta_0\)&lt;/p&gt;

&lt;p&gt;Repeat iteratively 1 and 2 until convergence&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;E-step, find:
\(\begin{align*}
 q(Z)&amp;amp;=\operatorname*{arg\,max}_{q}\mathcal{L}(q,\theta_0)\\
    &amp;amp;=\operatorname*{arg\,max}_{q}{KL}(q\parallel p)\\
    &amp;amp;=p(Z\mid X,\theta_0)
  \end{align*}\)&lt;/li&gt;
  &lt;li&gt;M-step, solve:
\(\begin{align*}
 \theta_*&amp;amp;=\operatorname*{arg\,max}_{\theta}\mathcal{L}(q,\theta)\\
    &amp;amp;=\operatorname*{arg\,max}_{\theta}\mathbb{E}_Z \log p(X,Z\mid\theta)
  \end{align*}\)
    &lt;ul&gt;
      &lt;li&gt;Set \(\theta_0=\theta_*\) and go to 1&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;EM monotonically increases the lower bound and converges to a stationary point of \(\log p(X\mid\theta)\), see figure.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/em/em_optimize.gif&quot; alt=&quot;EM algorithm&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Benefits of EM&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;In some cases E-step and M-step can be solved in closed-information&lt;/li&gt;
  &lt;li&gt;Allow to build more complicated models&lt;/li&gt;
  &lt;li&gt;If true posterior \(p(Z\mid X,\theta)\) is intractable, we may search for the closest \(q(Z)\) &lt;em&gt;among tractable distributions&lt;/em&gt; by solving optimization problem&lt;/li&gt;
  &lt;li&gt;Allows to process missing data by treating them as latent variables
    &lt;ul&gt;
      &lt;li&gt;It can deal with both discrete and latent variables&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Categorical latent variables&lt;/strong&gt;
Since \(z_i\in\{1,\dots,K\}\) the marginal of a mixture of gaussians is a finite mixture of distributions:&lt;/p&gt;

\[p(x_i\mid\theta)=\sum_{k=1}^Kp(x_i\mid k,\theta)p(z_i=k\mid\theta)\]

&lt;ul&gt;
  &lt;li&gt;E-step is closed-form: \(q(z_i=k)=p(z_i=k\mid x_i,\theta)=\frac{p(x_i\mid k,\theta)p(z_i=k\mid\theta)}{\sum_{l=1}^Kp(x_i\mid l,\theta)p(z_i=l\mid\theta)}\)&lt;/li&gt;
  &lt;li&gt;M-step is a sum of finite terms: \(\mathbb{E}_Z\log p(X,Z\mid\theta)=\sum_{i=1}^n\mathbb{E}_{z_i}\log p(x_i,z_i\mid\theta)=\sum_{i=1}^n\sum_{k=1}^K q(z_i=k)\log p(x_i,k\mid\theta)\)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Continuous latent variables&lt;/strong&gt;
A mixture of continuous distributions&lt;/p&gt;

\[p(x_i\mid\theta)=\int p(x_i\mid z_i,\theta)p(z_i\mid\theta) dz_i\]

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;E-step: only done in closed form when &lt;strong&gt;conjugate distributions&lt;/strong&gt;, otherwise the true posterior is intractable&lt;/p&gt;

\[q(z_i)=p(z_i\mid x_i,\theta)=\frac{p(x_i\mid z_i,\theta)p(z_i\mid\theta)}{\int p(x_i\mid z_i,\theta)p(z_i\mid\theta) dz_i}\]
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Typically continuous latent variable are used for dimensionality reduction a.k.a. &lt;em&gt;representation learning&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;log-derivative-trick&quot;&gt;Log-derivative trick&lt;/h1&gt;

\[\frac{\partial}{\partial x}p(y\mid x)=p(y\mid x)\frac{\partial}{\partial x}\log p(y\mid x)\]

&lt;p&gt;For example, we commonly find expressions as follows:&lt;/p&gt;

\[\begin{align*}
    \frac{\partial}{\partial x}\int p(y\mid x)h(x,y)dy &amp;amp;= \int \frac{\partial}{\partial x} p(y\mid x)h(x,y)dy\\
                                  &amp;amp;= \int \left(h(x,y)\frac{\partial}{\partial x} p(y\mid x) + p(y\mid x)\frac{\partial}{\partial x} h(x,y) \right)dy \\
                                  &amp;amp;= \int p(y\mid x)\frac{\partial}{\partial x} h(x,y) dy + \int h(x,y)\frac{\partial}{\partial x} p(y\mid x)dy \\
                                  &amp;amp;= \int p(y\mid x)\frac{\partial}{\partial x} h(x,y) dy + \int p(y\mid x)h(x,y)\frac{\partial}{\partial x} \log p(y\mid x)dy
  \end{align*}\]

&lt;p&gt;Now, the first term can be replaced with Monte Carlo estimate of expectation. Using the log-derivative trick, the second expectation can also be estimated via Monte Carlo.&lt;/p&gt;

&lt;h1 id=&quot;score-function&quot;&gt;Score function&lt;/h1&gt;

&lt;p&gt;It is the gradient of the log-likelihood function with respect to the parameter vector. Since it has zero mean, the value \(z_i^*\) in \(\nabla_{\phi}\log q(z_i^*\mid\theta)\) oscillates around zero.&lt;/p&gt;

\[\begin{align*}
    \nabla_{\phi}\log q(z_i\mid\theta)
  \end{align*}\]

&lt;p&gt;Proof it has zero mean:&lt;/p&gt;

\[\begin{align*}
  \int q(z_i\mid\theta)\nabla_{\phi}\log q(z_i\mid\theta) d z_i&amp;amp;=\int\frac{q(z_i\mid\theta)}{q(z_i\mid\theta)}\nabla_{\phi}q(z_i\mid\theta)d z_i\\
                                                              &amp;amp;= \nabla_{\phi}\int q(z_i\mid\theta)dz_i\\
                                                              &amp;amp;= \nabla_{\phi} 1 =0
\end{align*}\]

&lt;h1 id=&quot;reinforce&quot;&gt;REINFORCE&lt;/h1&gt;</content><author><name></name></author><category term="Bayesian-inference" /><summary type="html">Some tools: Stochastic variational inference Variance reduction Normalizing flows Gaussian processes Scalable MCMC algorithms Semi-implicit variational inference</summary></entry><entry><title type="html">Expectation Propagation Notes</title><link href="/blog/2019/expectation-propagation/" rel="alternate" type="text/html" title="Expectation Propagation Notes" /><published>2019-07-25T21:41:19-04:00</published><updated>2019-07-25T21:41:19-04:00</updated><id>/blog/2019/expectation-propagation</id><content type="html" xml:base="/blog/2019/expectation-propagation/">&lt;p&gt;Commonly in probabilistic models, we deal with expectations that are either too hard to compute or intractable. In Bayesian inference setting, we usually need to calculate the posterior \(p(z\mid x)\) for parameter estimation or the evidence \(p(x)\) for model selection, where \(z\) is a latent variable and \(x\) is known. We can use approximate inference to solve this complex integrals. There is many work that have been done in this area, but approximate methods can be classified in deterministic and sampling methods. The former evaluates the integral in several locations and constructs an approximate function. The latter relies in the law of large numbers and given enough samples, the integral will converge to the true value.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Sampling methods: Monte Carlo methods such as Importance sampling, Gibbs sampling, MCMC&lt;/li&gt;
  &lt;li&gt;Deterministic methods: Variational inference, Laplace approximation, Expectation Propagation&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This notes include explanations for ADF and Expectation Propagation.&lt;/p&gt;

&lt;h2 id=&quot;assumed-density-filtering-adf&quot;&gt;Assumed-density Filtering (ADF)&lt;/h2&gt;

&lt;p&gt;ADF (aka. moment matching, online Bayesian learning and weak marginalization) can be used when given the joint \(p(x, z)\), we want to calculate the posterior over the latent variable \(p(z \mid x)\) and the evidence \(p(x)\). This is a common task in statistical machine learning where we want to fit a parametric distribution.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The common setting&lt;/strong&gt;: we are given a set of data points \(D=x_1,\dots,x_n\), which we assume are i.i.d. Thus, we can model the joint distribution as the product of its observations and the prior \(p(D,z) = p(z)\prod_i p(x_i\mid z)\).&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Factorization&lt;/strong&gt;: We can factorize the joint distribution as needed, but it is recommendable that &lt;strong&gt;each factor is simple&lt;/strong&gt; enough for being propagated. Also the &lt;strong&gt;fewer terms&lt;/strong&gt;, entails fewer approximations. In general, we can define the joint distribution as follows where \(t_0\) is the prior.&lt;/p&gt;

\[p(D,z) = \prod_{i=0}^n t_i(z)\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Choose a parametric approximating distribution&lt;/strong&gt;: our goal is to approximate the posterior with a simple distribution; \(p(z\mid D) \approx q(z)\).&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;This distribution has to belong to the exponential family, so we can propagate its sufficient statistics.&lt;/li&gt;
      &lt;li&gt;Pick \(q\) based in the nature and domain of \(z\).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Incorporate each \(t_i\) term&lt;/strong&gt;: Sequence and incorporate the term \(t_i\) into the approximate posterior \(q(z\)) going from an old posterior to a new posterior.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;incorporate&lt;/strong&gt;: \(\hat p(z) = \frac{q(z)t_i(z)}{\int_z q(z)t_i(z) dz}\)&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;from old to new&lt;/strong&gt;: \(\min D_{KL}(\hat p \parallel q^{new})\), which equivalent to match the moments.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;intuition&quot;&gt;Intuition&lt;/h3&gt;
&lt;p&gt;Here is a graphical example of ADF in practice.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;\(q(z)\) can be initialized to \(1\), and it’s not necessary to approximate.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/adf/adf_step1.png&quot; alt=&quot;ADF step 1&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Incorporate \(t_1\), resulting in a one-step step posterior \(\hat p\). We approximate \(\hat p\) with \(q^{new}\).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/adf/adf_step2.png&quot; alt=&quot;ADF step 2&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Repeat step 2, including \(t_2\).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/adf/adf_step3.png&quot; alt=&quot;ADF step 3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can notice that there’s a dependence in the order of the data points; &lt;strong&gt;order matters&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The error of ADF tend to increase when similar data points are processed&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;expectation-propagation&quot;&gt;Expectation Propagation&lt;/h2&gt;
&lt;p&gt;Expectation Propagation (&lt;a href=&quot;https://arxiv.org/pdf/1301.2294.pdf&quot;&gt;Minka&lt;/a&gt;., NIPS 2001) is a generalization of ADF. We can notice that ADF is an iterative method that performs a one-pass to all data points. EP solves this problem by performing iterative refinements.&lt;/p&gt;

&lt;p&gt;ADF:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Treat \(t_i\) as it is.&lt;/li&gt;
  &lt;li&gt;Approximate \(q(z)\).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;EP:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Approximate \(t_i\) with \(\tilde t_i\).&lt;/li&gt;
  &lt;li&gt;Use exact posterior&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This refinements are always possible. It is the ratio of the new posterior to the old posterior times a constant:&lt;/p&gt;

\[\tilde t_i(z)=Z\frac{q^{new}(z)}{q(z)}\]

&lt;p&gt;Let’s see how this looks in practice:&lt;/p&gt;

&lt;p&gt;First, let’s derive an intuition of ADF using \(\tilde t_i\).&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;We approximate \(t_1\) with \(\tilde t_1\) using the prior \(t_0=1\)m and go from \(q^{old}\) to \(q^{new}\).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/adf/adf_approx_step1.png&quot; alt=&quot;ADF approx step 1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;2.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/adf/adf_approx_step2.png&quot; alt=&quot;ADF approx step 2&quot; /&gt;&lt;/p&gt;</content><author><name></name></author><category term="Bayesian-inference" /><summary type="html">Commonly in probabilistic models, we deal with expectations that are either too hard to compute or intractable. In Bayesian inference setting, we usually need to calculate the posterior \(p(z\mid x)\) for parameter estimation or the evidence \(p(x)\) for model selection, where \(z\) is a latent variable and \(x\) is known. We can use approximate inference to solve this complex integrals. There is many work that have been done in this area, but approximate methods can be classified in deterministic and sampling methods. The former evaluates the integral in several locations and constructs an approximate function. The latter relies in the law of large numbers and given enough samples, the integral will converge to the true value.</summary></entry><entry><title type="html">Probabilistic Graphical Models</title><link href="/blog/2019/pgm-intro/" rel="alternate" type="text/html" title="Probabilistic Graphical Models" /><published>2019-05-17T21:41:19-04:00</published><updated>2019-05-17T21:41:19-04:00</updated><id>/blog/2019/pgm-intro</id><content type="html" xml:base="/blog/2019/pgm-intro/">&lt;p&gt;A graphical model is a method for modeling probability distributions under certain uncertainty.&lt;/p&gt;

&lt;h2 id=&quot;toolbox&quot;&gt;Toolbox:&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Representation:&lt;/strong&gt; model uncertainty and encode domain knowledge&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Inference:&lt;/strong&gt; answer questions \(P(X\mid m)\), where m is the model or data&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Learning:&lt;/strong&gt; what model fits my data \(m = \operatorname*{argmax}_{m\in M} F(D,m)\).&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;benefits&quot;&gt;Benefits:&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Efficient&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;(Expensive)&lt;/strong&gt; The &lt;a href=&quot;https://en.wikipedia.org/wiki/Chain_rule_(probability)&quot;&gt;chain rule&lt;/a&gt; (aka product rule) allows to calculate joint probabilities.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;(Cheaper)&lt;/strong&gt; Using GM, we can model only those dependencies inferred by the graph, generating fewer parameters; encodes independence.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Encode domain knowledge through priors and incorporate them in inference via Bayes theorem.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;gms-vs-pgms&quot;&gt;GMs vs PGMs:&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;GMs use multivariate function.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;PGMs use multivariate distributions.&lt;/p&gt;

    &lt;h3 id=&quot;structure&quot;&gt;Structure&lt;/h3&gt;

    &lt;ol&gt;
      &lt;li&gt;Edges represent relationship among the RVs.&lt;/li&gt;
      &lt;li&gt;Directed nodes represent &lt;strong&gt;causality&lt;/strong&gt; while undirected nodes represent &lt;strong&gt;correlation&lt;/strong&gt;.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;bayesian-network-and-markov-random-field&quot;&gt;Bayesian Network and Markov Random Field&lt;/h2&gt;

&lt;h3 id=&quot;bayesian-network&quot;&gt;Bayesian Network&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/bayesian-network.png&quot; alt=&quot;Bayesian Network&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;It is a directed acyclic graph (DAG) where each node has a &lt;a href=&quot;https://en.wikipedia.org/wiki/Markov_blanket&quot;&gt;Markov blanked&lt;/a&gt; (its parents, children and children’s parents).&lt;/li&gt;
  &lt;li&gt;A node is &lt;em&gt;conditionally independent&lt;/em&gt; of the nodes &lt;strong&gt;outside&lt;/strong&gt; its Markov Blanket.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Joint probability distribution&lt;/em&gt; is determined by the local conditional probabilities as well as the graph structure.&lt;/li&gt;
  &lt;li&gt;Model &lt;em&gt;can&lt;/em&gt; be used to generate new data.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;markov-random-field&quot;&gt;Markov Random Field&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/markov-random-field.png&quot; alt=&quot;Markov Random Field&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;It is an undirected graph.&lt;/li&gt;
  &lt;li&gt;A node is conditionally independent of the other graph nodes, except for its &lt;strong&gt;immediate neighbors&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;To determine the joint probability distribution, we need to know local contingency functions (&lt;em&gt;potentials&lt;/em&gt;) as well as structural &lt;em&gt;cliques&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;This model &lt;em&gt;cannot&lt;/em&gt; explicitly generate new data.&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><category term="Bayesian-inference" /><summary type="html">A graphical model is a method for modeling probability distributions under certain uncertainty.</summary></entry><entry><title type="html">Sampling methods notes</title><link href="/blog/2019/sampling-methods/" rel="alternate" type="text/html" title="Sampling methods notes" /><published>2019-05-17T21:41:19-04:00</published><updated>2019-05-17T21:41:19-04:00</updated><id>/blog/2019/sampling-methods</id><content type="html" xml:base="/blog/2019/sampling-methods/">&lt;h1 id=&quot;monte-carlo-methods&quot;&gt;Monte Carlo Methods&lt;/h1&gt;

&lt;p&gt;Randomized algorithms have two categories: Las Vegas and Monte Carlo algorithms.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;em&gt;Las Vegas:&lt;/em&gt; always return an answer if available.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Monte Carlo&lt;/em&gt;: return answers with a random amount of error.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;monte-carlo-sampling&quot;&gt;Monte Carlo sampling&lt;/h2&gt;
&lt;p&gt;Probability distributions are commonly used in Machine Learning. Sampling provides a flexible way to approximate many sums and integrals. When a sum or integral cannot be computed, it is possible to approximate with &lt;strong&gt;Monte Carlo sampling&lt;/strong&gt;. The main idea is to see the sum or integral as an expectation, and approximate it by an avearge.&lt;/p&gt;

\[s=\sum_x p(x)f(x)=\mathbb{E}_p[f(\mathbf{x})]\]

\[s=\int p(x)f(x) dx=\mathbb{E}_p[f(\mathbf{x})]\]

&lt;p&gt;, where \(p\) is a PMF or PDF over the random variable \(\mathbf{x}\).&lt;/p&gt;

&lt;p&gt;\(s\) can be approximated by drawing \(n\) samples \(x^{(1)}, \dots, x^{(n)}\) from \(p\) and calculate the &lt;strong&gt;empirical average&lt;/strong&gt; as follows:&lt;/p&gt;

\[\hat s_n=\frac{1}{n}\sum_{i=1}^n f(x^{(i)})\]

&lt;p&gt;Some properties:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Estimator \(\hat s\) is unbiased.&lt;/li&gt;
  &lt;li&gt;By the &lt;em&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Law_of_large_numbers&quot;&gt;law of large numbers&lt;/a&gt;&lt;/em&gt;, we say that if \(x^{(i)}\) are i.i.d. the average converges to the expected values (\(\lim_{n\to\infty} \hat s_n=s\)).&lt;/li&gt;
  &lt;li&gt;By calculating the variance \(Var[\hat s_n]\) as \(n\) increases, we can estimate the uncertainty of the Monte Carlo approximation.&lt;/li&gt;
  &lt;li&gt;MC sampling relies in sampling from \(p\). When &lt;em&gt;sampling&lt;/em&gt; \(p\) &lt;em&gt;is not possible&lt;/em&gt;, importance sampling or MCMC can be used.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;importance-sampling&quot;&gt;Importance Sampling&lt;/h2&gt;
&lt;p&gt;In Monte Carlo, we can use any decomposition of \(p(\mathbf{x})\) and \(f(\mathbf{x})\) since \(f\) can also be a probability (what factor plays which role of \(p\) or \(f\)). We can even have a different decomposition as follows (i.e. we sample \(q\) and average \(\frac{pf}{q})\):&lt;/p&gt;

\[p(\mathbf{x})f(\mathbf{x})=q(\mathbf{x})\frac{p(\mathbf{x})f(\mathbf{x})}{q(\mathbf{x})}\]

&lt;p&gt;However, we might not be able to sample from \(p\) or we can pick another distribution to reach an optimal approximation. This optimal choice is \(q*\), know as &lt;strong&gt;optimal importance sampling&lt;/strong&gt;. So, we replace the empirical average with an &lt;strong&gt;importance sampling estimator&lt;/strong&gt;:&lt;/p&gt;

\[\hat s_q=\frac{1}{n}\sum_{i=1, \mathbf{x}^{(i)}\sim q}^n \frac{p(\mathbf{x}^{(i)})f(\mathbf{x}^{(i)})}{q(\mathbf{x}^{(i)})}\]

&lt;p&gt;Essentially, any distribution \(q\) is valid. However, the choice of \(q\) can be sensitive to the variance of the &lt;em&gt;importance sampling estimator&lt;/em&gt;.&lt;/p&gt;

&lt;h4 id=&quot;biased-importance-sampling&quot;&gt;Biased Importance Sampling&lt;/h4&gt;
&lt;p&gt;BIS doesn’t require to normalize \(p\) or \(q\), the estimator is given by:&lt;/p&gt;

\[\hat s_{BIS}=\frac{\sum_{i=1}^n\frac{\tilde p(\mathbf{x^{(i)}})}{\tilde q(\mathbf{x^{(i)}})}f(\mathbf{x^{(i)}})}{\sum_{i=1}^n\frac{\tilde p(\mathbf{x^{(i)}})}{\tilde q(\mathbf{x^{(i)}})}}\]

&lt;p&gt;, where \(\tilde p\) and \(\tilde q\) are the unnormalized forms of \(p\) and \(q\). We take n samples \(\mathbf{x^{(i)}}\) from \(q\).&lt;/p&gt;

&lt;p&gt;In general importance sampling is handy when it comes to:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;accelerate training in neural language models with a large vocabulary.&lt;/li&gt;
  &lt;li&gt;estimate a partition function.&lt;/li&gt;
  &lt;li&gt;estimate log-likelihood in deep directed models.&lt;/li&gt;
  &lt;li&gt;improve estimates of the gradient of the cost function.&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- 
## Markov Chain Monte Carlo

## Gibbs Sampling --&gt;</content><author><name></name></author><category term="Bayesian-inference" /><summary type="html">Monte Carlo Methods</summary></entry><entry><title type="html">CS224W notes - Machine learning with graphs</title><link href="/blog/2019/compcs224w-notes/" rel="alternate" type="text/html" title="CS224W notes - Machine learning with graphs" /><published>2019-05-17T21:41:19-04:00</published><updated>2019-05-17T21:41:19-04:00</updated><id>/blog/2019/compcs224w-notes</id><content type="html" xml:base="/blog/2019/compcs224w-notes/">&lt;h2 id=&quot;introduction-and-graph-structure&quot;&gt;&lt;a href=&quot;(http://web.stanford.edu/class/cs224w/slides/01-intro.pdf)&quot;&gt;Introduction and graph structure&lt;/a&gt;&lt;/h2&gt;

&lt;h3 id=&quot;why-graphs&quot;&gt;Why graphs?&lt;/h3&gt;
&lt;p&gt;Networks are general languages for describing complex systems of interacting entities. In other words, a network is a universal language for describing complex data.&lt;/p&gt;

&lt;p&gt;Network structure affects the robustness of a system&lt;/p&gt;

&lt;p&gt;Most real networks are sparse&lt;/p&gt;

&lt;p&gt;Tasks to perform in graphs:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;node classification&lt;/li&gt;
  &lt;li&gt;link prediction&lt;/li&gt;
  &lt;li&gt;community detection&lt;/li&gt;
  &lt;li&gt;network similarity&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Embedding nodes:&lt;/strong&gt; map nodes to \(d\)-dimensional embeddings. Thus, nodes wit similar network neighbourhoods are embedded close together.&lt;/p&gt;

&lt;p&gt;Terminology:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Networks refer to a real system, e.g. Social Networks, Web.&lt;/li&gt;
  &lt;li&gt;Graph is a mathematical representation of a network e.g. graph, vertex, edge&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Graphs can be directed (followers on twitter) or undirected (friends on Facebook)&lt;/p&gt;

&lt;p&gt;Node degrees:
  Undirected:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;node degree: the number of edges adjacent to node i&lt;/li&gt;
  &lt;li&gt;avg. degree&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Directed:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;in-degree&lt;/li&gt;
  &lt;li&gt;out-degree&lt;/li&gt;
  &lt;li&gt;the total degree of a node is the sum of in and out-degrees&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Complete Graph&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;maximum number of edges in an undirected graph on $N$ nodes is \(E_max=\frac{N(N-1)}{2}\) and when the number of edges is $E=E_max$, the undirected graph is called compete graph.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Bipartite Graph&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;It is a graph whose nodes can be divided into two disjoints sets $U$ and $V$, which are independent sets.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Representing Graphs
  Adjacency matrix&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;\(A_{ij}=1\) if there is a link from node \(i\) to \(j\)&lt;/li&gt;
  &lt;li&gt;\(A_{ij}=0\) otherwise&lt;/li&gt;
  &lt;li&gt;they are sparse&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Edge List&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;graph as a set of edges&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Adjacency list&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;each row is a node and its line is the connected nodes
1:
2: 3,4
3: 2,4
4: 5&lt;/li&gt;
  &lt;li&gt;Easier to work with if network is large or sparse&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Edge attributes&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;weight (frequency of communication)&lt;/li&gt;
  &lt;li&gt;raking (best friend, 2nd best friend)&lt;/li&gt;
  &lt;li&gt;type (friend, relative)&lt;/li&gt;
  &lt;li&gt;Sign (Friend vs Foe)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Other types of graphs:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;self-edges: contain self-loops (proteins, hyperlinks)&lt;/li&gt;
  &lt;li&gt;multigraph: more than one edge between nodes (communication, collaboration)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Connectivity:
  Disconnected graph&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;a graph can become a disconnected graph, if the a subgraph becomes isolated. Thus it has more than one component…&lt;/li&gt;
  &lt;li&gt;bridge edge: if we delete an edge,&lt;/li&gt;
  &lt;li&gt;articulation node: if we erase a node&lt;/li&gt;
  &lt;li&gt;Each component can be represented in a block-diagonal form&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Strongly connected directed graph&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;has a path from each node to every other node and vice-versa.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Weakly connected directed graph&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;is connected if we disregard the edge directions&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;properties-of-networks-and-random-graph-models&quot;&gt;&lt;a href=&quot;(http://web.stanford.edu/class/cs224w/slides/02-gnp-smallworld.pdf)&quot;&gt;Properties of networks and random graph models&lt;/a&gt;&lt;/h2&gt;

&lt;h3 id=&quot;how-to-measure-a-network&quot;&gt;How to Measure a Network&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Degree distribution \(p(k)\): probability that a randomly node has degree \(k\).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;\(N_k\rightarrow\): number of nodes with degree \(k\)&lt;/p&gt;

\[p(k)=\frac{N_k}{N}\]

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; directed graphs have separate in- and out-degree distributions&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Path: sequence of nodes in which each node is linked to the next one.&lt;/li&gt;
&lt;/ul&gt;

\[P_n=\{i_0,\dots,i_n\}\]

\[P_n=\{(i_0,i_1),\dots,(i_{n-1},i_n)\}\]

&lt;ul&gt;
  &lt;li&gt;Distance between a pair of nodes \(h_{A,B}\): number of edges along the shortest path connecting the nodes.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If node are not connected, distance is defined as &lt;em&gt;zero&lt;/em&gt; or &lt;em&gt;infinity&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Directed graphs, paths need to follow the direction of the arrows, and as a consequence, distance is not symmetric \(h_{B,C}\neq h_{c,B}\)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Diameter: shortest path distance between any pair of nodes in a graph&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Average path length \(\bar h\):&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

\[\bar h= \frac{1}{2E_{max}}\sum_{i,j\neq i}h_{ij}\]

&lt;p&gt;\(E_{max}\): max number of edges (total number of node pairs) \(=\frac{n(n-1)}{2}\)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Clustering Coefficient&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><category term="GRL" /><summary type="html">Introduction and graph structure</summary></entry><entry><title type="html">Linear Algebra</title><link href="/blog/2019/linear_algebra/" rel="alternate" type="text/html" title="Linear Algebra" /><published>2019-05-15T22:41:19-04:00</published><updated>2019-05-15T22:41:19-04:00</updated><id>/blog/2019/linear_algebra</id><content type="html" xml:base="/blog/2019/linear_algebra/">&lt;p&gt;Some interesting links:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;(https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf)&quot;&gt;Matrix cookbook&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;(https://people.maths.ox.ac.uk/gilesm/files/NA-08-01.pdf)&quot;&gt;Matrix derivatives&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Notes of &lt;a href=&quot;(http://www.deeplearningbook.org/contents/linear_algebra.html)&quot;&gt;chapter 2&lt;/a&gt; of Deep Learning book.&lt;/p&gt;

&lt;h4 id=&quot;mathematical-objects&quot;&gt;Mathematical objects&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Scalar:&lt;/strong&gt; single number \(x=1\)&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Vector:&lt;/strong&gt; array of numbers \(\mathbf{x}=\begin{vmatrix}x_1\\\vdots\\x_n\end{vmatrix}\)&lt;/p&gt;

    &lt;p&gt;\(\mathbf{x_{-1}}\): ignore element \(x_1\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Matrix:&lt;/strong&gt; 2D array of numbers \(\mathbf{A}\in\mathbb{R}^{mxn}\)&lt;/p&gt;

    &lt;p&gt;horizontal coordinate (all), ith row: \(\mathbf{A}_{i,:}\)&lt;/p&gt;

    &lt;p&gt;vertical coordinate (all), ith column: \(\mathbf{A}_{:,i}\)&lt;/p&gt;

    &lt;p&gt;value: \(f(\mathbf{A})_{i,j}\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Tensor:&lt;/strong&gt; array with more than two axes \(\mathbf{A}\)&lt;/p&gt;

    &lt;p&gt;value: \(\mathbf{A}_{i,j,k}\)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;transpose&quot;&gt;Transpose&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Mirror across &lt;em&gt;main diagonal&lt;/em&gt; \(\mathbf{A} \rightarrow \mathbf{A}^\intercal\)&lt;/li&gt;
  &lt;li&gt;Scalar matrix, one item \(\mathbf{a}=\mathbf{a}^\intercal\)&lt;/li&gt;
  &lt;li&gt;Vector \(\mathbf{x}^\intercal\)&lt;/li&gt;
  &lt;li&gt;Matrix product \((\mathbf{A}\mathbf{B})^\intercal=\mathbf{B}^\intercal\mathbf{A}^\intercal\)&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;addition&quot;&gt;Addition&lt;/h4&gt;
&lt;p&gt;If \(\mathbf{A}\) and \(\mathbf{B}\) same shape, \(\mathbf{C}=\mathbf{A}+\mathbf{B}\), where \(\mathbf{C}_{i,j}=\mathbf{A}_{i,j}+\mathbf{B}_{i,j}\).&lt;/p&gt;

&lt;h4 id=&quot;scalar-addition-and-multiplication&quot;&gt;Scalar addition and multiplication&lt;/h4&gt;
&lt;p&gt;\(\mathbf{D}=a\cdot\mathbf{B}+c\), where \(\mathbf{D}_{i,j}=a\cdot\mathbf{B}_{i,j}+c\)&lt;/p&gt;

&lt;h4 id=&quot;addition-matrix-and-vector&quot;&gt;Addition matrix and vector&lt;/h4&gt;
&lt;p&gt;\(\mathbf{C}=\mathbf{A}+\mathbf{b}\), where \(\mathbf{C}_{i,j}=\mathbf{A}_{i,j}+\mathbf{b}_j\)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;broadcasting&lt;/strong&gt;: \(\mathbf{b}\) is addded to each &lt;strong&gt;row&lt;/strong&gt; of the matrix \(\mathbf{A}\).&lt;/p&gt;

&lt;h4 id=&quot;operations&quot;&gt;Operations&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Matrix product \(\mathbf{C}=\mathbf{A}\mathbf{B} \rightarrow \mathbf{C}_{m\times p}=\mathbf{A}_{m\times n}\mathbf{B}_{n\times p}\) and \(\mathbf{C}_{i,j}=\sum_k\mathbf{A}_{i,k}\mathbf{B}_{k,j}\)&lt;/p&gt;

    &lt;p&gt;Vectors of the same size: \(\mathbf{x}^\intercal\mathbf{y}\)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Elementwise product / Hadamard product \(\mathbf{A}\odot\mathbf{B}\)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;properties&quot;&gt;Properties&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Distributive: \(\mathbf{A}(\mathbf{B}+\mathbf{C})=\mathbf{A}\mathbf{B}+\mathbf{A}\mathbf{C}\)&lt;/li&gt;
  &lt;li&gt;Associative: \(\mathbf{A}(\mathbf{B}\mathbf{C})=(\mathbf{A}\mathbf{B})\mathbf{C}\)&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Commutative: &lt;strong&gt;not&lt;/strong&gt; \(\mathbf{A}\mathbf{B}\neq \mathbf{B}\mathbf{A}\) (not always)&lt;/p&gt;

    &lt;p&gt;&lt;strong&gt;yes&lt;/strong&gt; for vectors \(\rightarrow\) \(\mathbf{x}^\intercal \mathbf{y}=\mathbf{y}^\intercal\mathbf{x}\)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;linear-equations&quot;&gt;Linear equations&lt;/h4&gt;

&lt;p&gt;\(\mathbf{A}\mathbf{x}=\mathbf{b}\) (notation compact)&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;\(\mathbf{A}\in\mathbb{R}^{m\times n}\) known&lt;/li&gt;
  &lt;li&gt;\(\mathbf{x}\in\mathbb{R}^m\) variable&lt;/li&gt;
  &lt;li&gt;\(\mathbf{b}\in\mathbb{R}^m\) known&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Notation not compact, equations:&lt;/p&gt;

\[A_{1,:}\mathbf{x}=b_1 \rightarrow A_{1,1}x_1+\dots+A_{1,n}x_n=b_1\\
 \vdots\\
 A_{m,:}\mathbf{x}=b_m \rightarrow A_{m,1}x_1+\dots+A_{m,n}x_n=b_m\]

&lt;h4 id=&quot;identity-matrix&quot;&gt;Identity matrix&lt;/h4&gt;
&lt;p&gt;\(\mathbf{I}_n\in\mathbb{R}^{nxn} \rightarrow \mathbf{I}_n\mathbf{x}=\mathbf{x}\:\:\:\:\:\:\: \forall_{\mathbf{x}}\in\mathbb{R}^n\)&lt;/p&gt;

&lt;h4 id=&quot;inverse-matrix&quot;&gt;Inverse Matrix&lt;/h4&gt;
&lt;p&gt;\(A^{-1}A=\mathbf{I}_n\), \(AA^{-1}=\mathbf{I}_n\), for square matrix left and right are the same.&lt;/p&gt;

&lt;p&gt;For example:
\(\begin{align}
A\mathbf{x}&amp;amp;=\mathbf{b} \\
A^{-1}A\mathbf{x}&amp;amp;=A^{-1}\mathbf{b} \\
\mathbf{I}_n\mathbf{x}&amp;amp;=A^{-1}\mathbf{b}\\
\mathbf{x}&amp;amp;=A^{-1}\mathbf{b}
\end{align}\)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Should &lt;strong&gt;not&lt;/strong&gt; be used in practical applications because &lt;strong&gt;limited precision&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;\(A^{-1}\) might not be possible to find (singular matrix).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Matrix should be &lt;strong&gt;square and singular&lt;/strong&gt;. If not, we cannot get \(A^{-1}\).&lt;/p&gt;
&lt;h4 id=&quot;linear-combination&quot;&gt;Linear Combination&lt;/h4&gt;
&lt;p&gt;\({\mathbf{v}^{(1)},\dots,\mathbf{v}^{(n)}}\) Given by multiplying each vector \(\mathbf{v}^{(i)}\) by a scalar and adding.&lt;/p&gt;

\[\sum_i c_i\mathbf{v}^{(i)}\]

&lt;h5 id=&quot;span&quot;&gt;Span&lt;/h5&gt;
&lt;p&gt;Set of all points obtained by linear combination of the original vectors&lt;/p&gt;

&lt;p&gt;\(A\mathbf{x}=\mathbf{b} \rightarrow\) solution if \(\mathbf{b}\) is in the &lt;strong&gt;span&lt;/strong&gt; (known as &lt;strong&gt;column space&lt;/strong&gt; / &lt;strong&gt;range&lt;/strong&gt; of \(A\)) of columns \(A\)&lt;/p&gt;

&lt;p&gt;\(A\) square matrix \(m=n\) and all columns linear independent.&lt;/p&gt;

&lt;h4 id=&quot;linear-dependence&quot;&gt;Linear Dependence&lt;/h4&gt;
&lt;p&gt;Same column space / a combination&lt;/p&gt;
&lt;h4 id=&quot;linear-independence&quot;&gt;Linear Independence&lt;/h4&gt;
&lt;p&gt;No vector in the set is a &lt;strong&gt;linear combination&lt;/strong&gt; of the other vectors.&lt;/p&gt;
&lt;h4 id=&quot;square-matrix&quot;&gt;Square matrix&lt;/h4&gt;
&lt;p&gt;\(A \in\mathbb{R}^{m\times n} m=n\)&lt;/p&gt;
&lt;h4 id=&quot;singular-matrix&quot;&gt;Singular matrix&lt;/h4&gt;
&lt;p&gt;Matrix with linear independent columns&lt;/p&gt;
&lt;h4 id=&quot;norms&quot;&gt;Norms&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Norms are functions mapping vectors to non-negative values (distance from origin to point \(\mathbf{x}\))&lt;/li&gt;
  &lt;li&gt;Measure the size of a vector&lt;/li&gt;
  &lt;li&gt;Any function \(f\) is a norma as long as:
    &lt;ol&gt;
      &lt;li&gt;\(f(\mathbf{x})=0 \rightarrow \mathbf{x}=0\).&lt;/li&gt;
      &lt;li&gt;\(f(\mathbf{x} + \mathbf{y}) \leq f(\mathbf{x}) + f(\mathbf{y})\) triangle inequality&lt;/li&gt;
      &lt;li&gt;\(\forall \alpha\in\mathbb{R}, f(\alpha\mathbf{x}) = \mid\alpha\mid f(\mathbf{x})\).&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;list&quot;&gt;List:&lt;/h5&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;\(L^p\) norm&lt;/strong&gt;: \(\mid\mid x\mid\mid_p=(\sum_i\mid x_i\mid)^{\frac{1}{p}}\) where \(p\in\mathbb{R}, p\geq 1\)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;\(L^2\) Euclidean norm (distance)&lt;/strong&gt;: \(\parallel\mathbf{x}\parallel_2=\parallel\mathbf{x}\parallel=\mathbf{x}^\intercal\mathbf{x}\)
    &lt;ul&gt;
      &lt;li&gt;increases really slowly near the original&lt;/li&gt;
      &lt;li&gt;squared \(L^2\) norm is better to work: 1) mathematically and 2) computationally.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;\(L^1\) norm&lt;/strong&gt;: \(\parallel\mathbf{x}\parallel_1=\sum_i\mid x_i\mid\)
    &lt;ul&gt;
      &lt;li&gt;chooses a function that grows at the &lt;strong&gt;same rate&lt;/strong&gt; in all locations.&lt;/li&gt;
      &lt;li&gt;Discriminate elements that are 1) exactly zero; 2) small, but not zero.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;\(L^\infty\) max norm&lt;/strong&gt;: \(\parallel\mathbf{x}\parallel_\infty=\max_i\mid x_i\mid\)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Frobenius norm&lt;/strong&gt;: &lt;em&gt;measure size of matrix&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;Analogous to \(L^2\) of a vector&lt;/li&gt;
      &lt;li&gt;Dot product in terms of norms \(\mathbf{x}^\intercal\mathbf{y}=\parallel\mathbf{x}\parallel_2\parallel\mathbf{y}\parallel_2\cos\theta\)&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;\(\parallel A\parallel_F=\sqrt{\sum_{ij}A_{ij}^2}\)&lt;/p&gt;
    &lt;h3 id=&quot;special-kind-of-matrices-and-vectors&quot;&gt;Special kind of matrices and Vectors&lt;/h3&gt;
  &lt;/li&gt;
  &lt;li&gt;Diagonal matrix (\(D\))
    &lt;ul&gt;
      &lt;li&gt;\(diag(\mathbf{v})\): square&lt;/li&gt;
      &lt;li&gt;\(diag(\mathbf{v})^{-1}=diag(\left[1/v_1,\dots,1/v_n\right]^\intercal)\).&lt;/li&gt;
      &lt;li&gt;\(diag(\mathbf{v})\mathbf{x}=\mathbf{v}\odot\mathbf{x} \rightarrow x_i*v_i\).&lt;/li&gt;
      &lt;li&gt;non-square: &lt;strong&gt;no inverse&lt;/strong&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Symmetric \(A=A^\intercal\)&lt;/li&gt;
  &lt;li&gt;Unit vector (unit norm): \(\parallel\mathbf{x}\parallel_2=1\)&lt;/li&gt;
  &lt;li&gt;Orthogonal: \(\mathbf{x}^\intercal\mathbf{y}=0\) Vectors are perpendicular (90 degrees)
    &lt;ul&gt;
      &lt;li&gt;mutually orthogonal: Set of vectors are orthogonal&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;orthonormal&lt;/strong&gt;: vector is &lt;strong&gt;orthogonal&lt;/strong&gt; and have &lt;strong&gt;unit norm&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;orthogonal matrix&lt;/strong&gt;: square matrix, rows and columns are mutually orthonormal&lt;/p&gt;

\[\begin{align}
    A^\intercal A&amp;amp;=AA^\intercal=I\\
    A^{-1}&amp;amp;=A^\intercal
  \end{align}\]
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;eigendecomposition&quot;&gt;Eigendecomposition&lt;/h3&gt;
&lt;h4 id=&quot;matrix-decomposition&quot;&gt;Matrix Decomposition&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;Eigenvectors (\(\mathbf{v}\) vector)&lt;/li&gt;
  &lt;li&gt;Eigenvalues (\(\lambda\) scalar)&lt;/li&gt;
&lt;/ol&gt;

\[A\mathbf{v}=\lambda\mathbf{v}\]

&lt;h4 id=&quot;eigendecomposition-1&quot;&gt;Eigendecomposition&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;\(\mathbf{v}^{-1}\) ,linear independent eigenvectors&lt;/li&gt;
  &lt;li&gt;\(\mathbf{\lambda}\), corresponding eigenvalue&lt;/li&gt;
&lt;/ol&gt;

\[A=\mathbf{v}diag(\mathbf{\lambda})\mathbf{v}^{-1}\]

&lt;p&gt;&lt;strong&gt;Not every matrix can be decomposed&lt;/strong&gt;&lt;/p&gt;

&lt;h4 id=&quot;real-symmetric-matrix&quot;&gt;Real Symmetric matrix&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;\(Q\), orthogonal matrix&lt;/li&gt;
  &lt;li&gt;\(\Lambda\), diagonal matrix&lt;/li&gt;
&lt;/ol&gt;

\[A=Q\Lambda Q^\intercal\]

&lt;p&gt;&lt;strong&gt;Not defined for non-squared matrices&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;singular-value-decomposition-svd&quot;&gt;Singular Value Decomposition (SVD)&lt;/h3&gt;
&lt;p&gt;It’s more generally applicable&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;\(D\), diagonal matrix (singular values=\(diag(D)\), left-singular vector and right-singular vector are the columns of \(U\) and \(V\) respectively.)&lt;/li&gt;
  &lt;li&gt;\(U\) and \(V\), orthogonal matrices&lt;/li&gt;
&lt;/ol&gt;

\[A=UDV^\intercal\]

\[m\times n = (m\times m) (m\times n) (n\times n)\]

&lt;p&gt;&lt;strong&gt;Useful to partially generalize matrix inversion to non-square matrices.&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;the-moore-penrose-pseudoinverse&quot;&gt;The Moore-Penrose Pseudoinverse&lt;/h3&gt;
&lt;p&gt;Matrix inversion &lt;strong&gt;not defined&lt;/strong&gt; for non-square matrices, these can be calculated with this method.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;real (pseudoinverse): \(A^+=\lim_{\alpha\searrow 0}(A^\intercal A+\alpha I)^{-1}A^\intercal\)&lt;/li&gt;
  &lt;li&gt;practical \(A^+=VD^+U^\intercal\)&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;\(U, D, V\): singular value decomposition of \(A\)&lt;/li&gt;
  &lt;li&gt;\(D^+\): diagonal matrix. Take \(D\) and get the &lt;strong&gt;reciprocal&lt;/strong&gt; of its nonzero element and take the &lt;strong&gt;transpose&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;trace-operator&quot;&gt;Trace operator&lt;/h3&gt;

&lt;h3 id=&quot;determinant&quot;&gt;Determinant&lt;/h3&gt;</content><author><name></name></author><category term="Math" /><summary type="html">Some interesting links: Matrix cookbook Matrix derivatives</summary></entry><entry><title type="html">Probability and Information Theory</title><link href="/blog/2019/probability/" rel="alternate" type="text/html" title="Probability and Information Theory" /><published>2019-05-15T22:41:19-04:00</published><updated>2019-05-15T22:41:19-04:00</updated><id>/blog/2019/probability</id><content type="html" xml:base="/blog/2019/probability/">&lt;p&gt;Some interesting links:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;(http://www.tina-vision.net/docs/memos/2003-003.pdf)&quot;&gt;Product and convolution of gaussians&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;(https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/other-readings/chapter13.pdf)&quot;&gt;Multivariate Gaussian&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;(http://www.herbrich.me/papers/EP.pdf)&quot;&gt;EP with Gaussians&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Notes of &lt;a href=&quot;(http://www.deeplearningbook.org/contents/linear_algebra.html)&quot;&gt;chapter 3&lt;/a&gt; of Deep Learning book.&lt;/p&gt;</content><author><name></name></author><category term="Math" /><summary type="html">Some interesting links: Product and convolution of gaussians Multivariate Gaussian EP with Gaussians</summary></entry></feed>