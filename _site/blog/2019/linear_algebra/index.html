<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Xavier Sumba


  | Linear Algebra

</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Open Graph -->

<meta property="og:site_name" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
" />
<meta property="og:type" content="object" />
<meta property="og:title" content="" />
<meta property="og:url" content="//blog/2019/linear_algebra/" />
<meta property="og:description" content="Linear Algebra" />
<meta property="og:image" content="" />


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üèÉ</text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/blog/2019/linear_algebra/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-QCYXRLMN9B"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() { dataLayer.push(arguments); }
  gtag('js', new Date());

  gtag('config', 'G-QCYXRLMN9B');
</script>



<!-- Panelbear Analytics - We respect your privacy -->
<script async src="https://cdn.panelbear.com/analytics.js?site=5LB6elP1kZB"></script>
<script>
    window.panelbear = window.panelbear || function() { (window.panelbear.q = window.panelbear.q || []).push(arguments); };
    panelbear('config', { site: '5LB6elP1kZB' });
</script>


    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="//">
       Xavier Sumba
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item active">
            <a class="nav-link" href="/blog/">
              blog
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                publications
                
              </a>
          </li>
          
          
          
          
          
          
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      

<div class="post">

  <header class="post-header">
    <h1 class="post-title">Linear Algebra</h1>
    <p class="post-meta">May 15, 2019</p>
  </header>

  <article class="post-content">
    <p>Some interesting links:</p>
<ul>
  <li><a href="(https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf)">Matrix cookbook</a></li>
  <li><a href="(https://people.maths.ox.ac.uk/gilesm/files/NA-08-01.pdf)">Matrix derivatives</a></li>
</ul>

<p>Notes of <a href="(http://www.deeplearningbook.org/contents/linear_algebra.html)">chapter 2</a> of Deep Learning book.</p>

<h4 id="mathematical-objects">Mathematical objects</h4>

<ol>
  <li><strong>Scalar:</strong> single number \(x=1\)</li>
  <li>
    <p><strong>Vector:</strong> array of numbers \(\mathbf{x}=\begin{vmatrix}x_1\\\vdots\\x_n\end{vmatrix}\)</p>

    <p>\(\mathbf{x_{-1}}\): ignore element \(x_1\)</p>
  </li>
  <li>
    <p><strong>Matrix:</strong> 2D array of numbers \(\mathbf{A}\in\mathbb{R}^{mxn}\)</p>

    <p>horizontal coordinate (all), ith row: \(\mathbf{A}_{i,:}\)</p>

    <p>vertical coordinate (all), ith column: \(\mathbf{A}_{:,i}\)</p>

    <p>value: \(f(\mathbf{A})_{i,j}\)</p>
  </li>
  <li>
    <p><strong>Tensor:</strong> array with more than two axes \(\mathbf{A}\)</p>

    <p>value: \(\mathbf{A}_{i,j,k}\)</p>
  </li>
</ol>

<h4 id="transpose">Transpose</h4>
<ul>
  <li>Mirror across <em>main diagonal</em> \(\mathbf{A} \rightarrow \mathbf{A}^\intercal\)</li>
  <li>Scalar matrix, one item \(\mathbf{a}=\mathbf{a}^\intercal\)</li>
  <li>Vector \(\mathbf{x}^\intercal\)</li>
  <li>Matrix product \((\mathbf{A}\mathbf{B})^\intercal=\mathbf{B}^\intercal\mathbf{A}^\intercal\)</li>
</ul>

<h4 id="addition">Addition</h4>
<p>If \(\mathbf{A}\) and \(\mathbf{B}\) same shape, \(\mathbf{C}=\mathbf{A}+\mathbf{B}\), where \(\mathbf{C}_{i,j}=\mathbf{A}_{i,j}+\mathbf{B}_{i,j}\).</p>

<h4 id="scalar-addition-and-multiplication">Scalar addition and multiplication</h4>
<p>\(\mathbf{D}=a\cdot\mathbf{B}+c\), where \(\mathbf{D}_{i,j}=a\cdot\mathbf{B}_{i,j}+c\)</p>

<h4 id="addition-matrix-and-vector">Addition matrix and vector</h4>
<p>\(\mathbf{C}=\mathbf{A}+\mathbf{b}\), where \(\mathbf{C}_{i,j}=\mathbf{A}_{i,j}+\mathbf{b}_j\)</p>

<p><strong>broadcasting</strong>: \(\mathbf{b}\) is addded to each <strong>row</strong> of the matrix \(\mathbf{A}\).</p>

<h4 id="operations">Operations</h4>
<ol>
  <li>
    <p>Matrix product \(\mathbf{C}=\mathbf{A}\mathbf{B} \rightarrow \mathbf{C}_{m\times p}=\mathbf{A}_{m\times n}\mathbf{B}_{n\times p}\) and \(\mathbf{C}_{i,j}=\sum_k\mathbf{A}_{i,k}\mathbf{B}_{k,j}\)</p>

    <p>Vectors of the same size: \(\mathbf{x}^\intercal\mathbf{y}\)</p>
  </li>
  <li>
    <p>Elementwise product / Hadamard product \(\mathbf{A}\odot\mathbf{B}\)</p>
  </li>
</ol>

<h4 id="properties">Properties</h4>
<ul>
  <li>Distributive: \(\mathbf{A}(\mathbf{B}+\mathbf{C})=\mathbf{A}\mathbf{B}+\mathbf{A}\mathbf{C}\)</li>
  <li>Associative: \(\mathbf{A}(\mathbf{B}\mathbf{C})=(\mathbf{A}\mathbf{B})\mathbf{C}\)</li>
  <li>
    <p>Commutative: <strong>not</strong> \(\mathbf{A}\mathbf{B}\neq \mathbf{B}\mathbf{A}\) (not always)</p>

    <p><strong>yes</strong> for vectors \(\rightarrow\) \(\mathbf{x}^\intercal \mathbf{y}=\mathbf{y}^\intercal\mathbf{x}\)</p>
  </li>
</ul>

<h4 id="linear-equations">Linear equations</h4>

<p>\(\mathbf{A}\mathbf{x}=\mathbf{b}\) (notation compact)</p>
<ul>
  <li>\(\mathbf{A}\in\mathbb{R}^{m\times n}\) known</li>
  <li>\(\mathbf{x}\in\mathbb{R}^m\) variable</li>
  <li>\(\mathbf{b}\in\mathbb{R}^m\) known</li>
</ul>

<p>Notation not compact, equations:</p>

\[A_{1,:}\mathbf{x}=b_1 \rightarrow A_{1,1}x_1+\dots+A_{1,n}x_n=b_1\\
 \vdots\\
 A_{m,:}\mathbf{x}=b_m \rightarrow A_{m,1}x_1+\dots+A_{m,n}x_n=b_m\]

<h4 id="identity-matrix">Identity matrix</h4>
<p>\(\mathbf{I}_n\in\mathbb{R}^{nxn} \rightarrow \mathbf{I}_n\mathbf{x}=\mathbf{x}\:\:\:\:\:\:\: \forall_{\mathbf{x}}\in\mathbb{R}^n\)</p>

<h4 id="inverse-matrix">Inverse Matrix</h4>
<p>\(A^{-1}A=\mathbf{I}_n\), \(AA^{-1}=\mathbf{I}_n\), for square matrix left and right are the same.</p>

<p>For example:
\(\begin{align}
A\mathbf{x}&amp;=\mathbf{b} \\
A^{-1}A\mathbf{x}&amp;=A^{-1}\mathbf{b} \\
\mathbf{I}_n\mathbf{x}&amp;=A^{-1}\mathbf{b}\\
\mathbf{x}&amp;=A^{-1}\mathbf{b}
\end{align}\)</p>

<ul>
  <li>Should <strong>not</strong> be used in practical applications because <strong>limited precision</strong></li>
  <li>\(A^{-1}\) might not be possible to find (singular matrix).</li>
</ul>

<p><strong>Note:</strong> Matrix should be <strong>square and singular</strong>. If not, we cannot get \(A^{-1}\).</p>
<h4 id="linear-combination">Linear Combination</h4>
<p>\({\mathbf{v}^{(1)},\dots,\mathbf{v}^{(n)}}\) Given by multiplying each vector \(\mathbf{v}^{(i)}\) by a scalar and adding.</p>

\[\sum_i c_i\mathbf{v}^{(i)}\]

<h5 id="span">Span</h5>
<p>Set of all points obtained by linear combination of the original vectors</p>

<p>\(A\mathbf{x}=\mathbf{b} \rightarrow\) solution if \(\mathbf{b}\) is in the <strong>span</strong> (known as <strong>column space</strong> / <strong>range</strong> of \(A\)) of columns \(A\)</p>

<p>\(A\) square matrix \(m=n\) and all columns linear independent.</p>

<h4 id="linear-dependence">Linear Dependence</h4>
<p>Same column space / a combination</p>
<h4 id="linear-independence">Linear Independence</h4>
<p>No vector in the set is a <strong>linear combination</strong> of the other vectors.</p>
<h4 id="square-matrix">Square matrix</h4>
<p>\(A \in\mathbb{R}^{m\times n} m=n\)</p>
<h4 id="singular-matrix">Singular matrix</h4>
<p>Matrix with linear independent columns</p>
<h4 id="norms">Norms</h4>
<ul>
  <li>Norms are functions mapping vectors to non-negative values (distance from origin to point \(\mathbf{x}\))</li>
  <li>Measure the size of a vector</li>
  <li>Any function \(f\) is a norma as long as:
    <ol>
      <li>\(f(\mathbf{x})=0 \rightarrow \mathbf{x}=0\).</li>
      <li>\(f(\mathbf{x} + \mathbf{y}) \leq f(\mathbf{x}) + f(\mathbf{y})\) triangle inequality</li>
      <li>\(\forall \alpha\in\mathbb{R}, f(\alpha\mathbf{x}) = \mid\alpha\mid f(\mathbf{x})\).</li>
    </ol>
  </li>
</ul>

<h5 id="list">List:</h5>
<ul>
  <li><strong>\(L^p\) norm</strong>: \(\mid\mid x\mid\mid_p=(\sum_i\mid x_i\mid)^{\frac{1}{p}}\) where \(p\in\mathbb{R}, p\geq 1\)</li>
  <li><strong>\(L^2\) Euclidean norm (distance)</strong>: \(\parallel\mathbf{x}\parallel_2=\parallel\mathbf{x}\parallel=\mathbf{x}^\intercal\mathbf{x}\)
    <ul>
      <li>increases really slowly near the original</li>
      <li>squared \(L^2\) norm is better to work: 1) mathematically and 2) computationally.</li>
    </ul>
  </li>
  <li><strong>\(L^1\) norm</strong>: \(\parallel\mathbf{x}\parallel_1=\sum_i\mid x_i\mid\)
    <ul>
      <li>chooses a function that grows at the <strong>same rate</strong> in all locations.</li>
      <li>Discriminate elements that are 1) exactly zero; 2) small, but not zero.</li>
    </ul>
  </li>
  <li><strong>\(L^\infty\) max norm</strong>: \(\parallel\mathbf{x}\parallel_\infty=\max_i\mid x_i\mid\)</li>
  <li><strong>Frobenius norm</strong>: <em>measure size of matrix</em>
    <ul>
      <li>Analogous to \(L^2\) of a vector</li>
      <li>Dot product in terms of norms \(\mathbf{x}^\intercal\mathbf{y}=\parallel\mathbf{x}\parallel_2\parallel\mathbf{y}\parallel_2\cos\theta\)</li>
    </ul>

    <p>\(\parallel A\parallel_F=\sqrt{\sum_{ij}A_{ij}^2}\)</p>
    <h3 id="special-kind-of-matrices-and-vectors">Special kind of matrices and Vectors</h3>
  </li>
  <li>Diagonal matrix (\(D\))
    <ul>
      <li>\(diag(\mathbf{v})\): square</li>
      <li>\(diag(\mathbf{v})^{-1}=diag(\left[1/v_1,\dots,1/v_n\right]^\intercal)\).</li>
      <li>\(diag(\mathbf{v})\mathbf{x}=\mathbf{v}\odot\mathbf{x} \rightarrow x_i*v_i\).</li>
      <li>non-square: <strong>no inverse</strong></li>
    </ul>
  </li>
  <li>Symmetric \(A=A^\intercal\)</li>
  <li>Unit vector (unit norm): \(\parallel\mathbf{x}\parallel_2=1\)</li>
  <li>Orthogonal: \(\mathbf{x}^\intercal\mathbf{y}=0\) Vectors are perpendicular (90 degrees)
    <ul>
      <li>mutually orthogonal: Set of vectors are orthogonal</li>
      <li><strong>orthonormal</strong>: vector is <strong>orthogonal</strong> and have <strong>unit norm</strong></li>
      <li>
        <p><strong>orthogonal matrix</strong>: square matrix, rows and columns are mutually orthonormal</p>

\[\begin{align}
    A^\intercal A&amp;=AA^\intercal=I\\
    A^{-1}&amp;=A^\intercal
  \end{align}\]
      </li>
    </ul>
  </li>
</ul>

<h3 id="eigendecomposition">Eigendecomposition</h3>
<h4 id="matrix-decomposition">Matrix Decomposition</h4>
<ol>
  <li>Eigenvectors (\(\mathbf{v}\) vector)</li>
  <li>Eigenvalues (\(\lambda\) scalar)</li>
</ol>

\[A\mathbf{v}=\lambda\mathbf{v}\]

<h4 id="eigendecomposition-1">Eigendecomposition</h4>
<ol>
  <li>\(\mathbf{v}^{-1}\) ,linear independent eigenvectors</li>
  <li>\(\mathbf{\lambda}\), corresponding eigenvalue</li>
</ol>

\[A=\mathbf{v}diag(\mathbf{\lambda})\mathbf{v}^{-1}\]

<p><strong>Not every matrix can be decomposed</strong></p>

<h4 id="real-symmetric-matrix">Real Symmetric matrix</h4>
<ol>
  <li>\(Q\), orthogonal matrix</li>
  <li>\(\Lambda\), diagonal matrix</li>
</ol>

\[A=Q\Lambda Q^\intercal\]

<p><strong>Not defined for non-squared matrices</strong></p>

<h3 id="singular-value-decomposition-svd">Singular Value Decomposition (SVD)</h3>
<p>It‚Äôs more generally applicable</p>

<ol>
  <li>\(D\), diagonal matrix (singular values=\(diag(D)\), left-singular vector and right-singular vector are the columns of \(U\) and \(V\) respectively.)</li>
  <li>\(U\) and \(V\), orthogonal matrices</li>
</ol>

\[A=UDV^\intercal\]

\[m\times n = (m\times m) (m\times n) (n\times n)\]

<p><strong>Useful to partially generalize matrix inversion to non-square matrices.</strong></p>

<h3 id="the-moore-penrose-pseudoinverse">The Moore-Penrose Pseudoinverse</h3>
<p>Matrix inversion <strong>not defined</strong> for non-square matrices, these can be calculated with this method.</p>

<ol>
  <li>real (pseudoinverse): \(A^+=\lim_{\alpha\searrow 0}(A^\intercal A+\alpha I)^{-1}A^\intercal\)</li>
  <li>practical \(A^+=VD^+U^\intercal\)</li>
</ol>

<ul>
  <li>\(U, D, V\): singular value decomposition of \(A\)</li>
  <li>\(D^+\): diagonal matrix. Take \(D\) and get the <strong>reciprocal</strong> of its nonzero element and take the <strong>transpose</strong>.</li>
</ul>

<h3 id="trace-operator">Trace operator</h3>

<h3 id="determinant">Determinant</h3>


  </article>

  

</div>

    </div>

    <!-- Footer -->

    
<footer class="sticky-bottom mt-5">
  <div class="container">
    &copy; Copyright 2021 Francisco Xavier Sumba T..
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a>

    
    
    Last updated: December 07, 2021.
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
