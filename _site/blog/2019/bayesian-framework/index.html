<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Xavier Sumba


  | Notes on the Bayesian Framework

</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Open Graph -->

<meta property="og:site_name" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
" />
<meta property="og:type" content="object" />
<meta property="og:title" content="" />
<meta property="og:url" content="http://localhost:4000//blog/2019/bayesian-framework/" />
<meta property="og:description" content="Notes on the Bayesian Framework" />
<meta property="og:image" content="" />


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üèÉ</text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/blog/2019/bayesian-framework/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-QCYXRLMN9B"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() { dataLayer.push(arguments); }
  gtag('js', new Date());

  gtag('config', 'G-QCYXRLMN9B');
</script>



<!-- Panelbear Analytics - We respect your privacy -->
<script async src="https://cdn.panelbear.com/analytics.js?site=5LB6elP1kZB"></script>
<script>
    window.panelbear = window.panelbear || function() { (window.panelbear.q = window.panelbear.q || []).push(arguments); };
    panelbear('config', { site: '5LB6elP1kZB' });
</script>


    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="http://localhost:4000//">
       Xavier Sumba
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item active">
            <a class="nav-link" href="/blog/">
              blog
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                publications
                
              </a>
          </li>
          
          
          
          
          
          
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      

<div class="post">

  <header class="post-header">
    <h1 class="post-title">Notes on the Bayesian Framework</h1>
    <p class="post-meta">November 1, 2019</p>
  </header>

  <article class="post-content">
    <p>Some tools:</p>
<ul>
  <li>Stochastic variational inference</li>
  <li>Variance reduction</li>
  <li>Normalizing flows</li>
  <li>Gaussian processes</li>
  <li>Scalable MCMC algorithms</li>
  <li>Semi-implicit variational inference</li>
</ul>

<h1 id="bayesian-framework">Bayesian framework</h1>
<ul>
  <li><strong>Bayes theorem</strong>:</li>
</ul>

\[conditional = \frac{joint}{marginal}\]

\[p(x\mid y)=\frac{p(x,y)}{p(y)}\]

<p>It defines a rule for uncertainty conversion when new information arrives</p>

\[posterior = \frac{likelihood \times prior}{evidence}\]

<ul>
  <li><strong>Product rule</strong>: any joint distribution can be expressed with conditional distributions</li>
</ul>

\[p(x,y,z)=p(x\mid y,z)p(y\mid z)p(z)\]

<ul>
  <li><strong>Sum rule</strong>: any marginal distribution can be obtained from the joint distribution by integrating out</li>
</ul>

\[p(y)=\int p(x,y)dx\]

<ul>
  <li>
    <p><strong>Statistical inference</strong></p>

    <p><strong>Problem</strong>: given i.i.d. data \(X=\{x_i\}_{i=1}^n\) from distribution \(p(x\mid\theta)\), estimate \(\theta\)</p>

    <ol>
      <li><strong>Frequentist framework</strong>: use maximum likelihood estimation (MLE)</li>
    </ol>

\[\theta_{ML}=\operatorname*{arg\,max}p(X\mid\theta)=\operatorname*{arg\,max}\prod_{i=1}^n p(x_i\mid\theta)=\operatorname*{arg\,max}\sum_i^n\log p(x_i\mid\theta)\]

    <p>Applicability: \(n\ll d\)</p>

    <ol>
      <li><strong>Bayesian framework</strong>: encode uncertainty about \(\theta\) in a prior \(p(\theta)\) and apply Bayesian inference</li>
    </ol>

\[p(\theta\mid X)=\frac{\prod_i^n p(x_i\mid\theta)p(\theta)}{\int\prod_i^n p(x_i\mid\theta)p(\theta)d\theta}\]

    <p>Applicability: \(\forall_nd\)</p>

    <p>Advantages:
    - we can encode prior knowledge/desired properties into a prior distribution
    - prior is a form of regularization
    - additionally to the point estimate of \(\theta\), posterior contains information about the uncertainty of the estimate
    - frequentist case is a limit case of Bayesian one
      \(\lim_{n/d\to\infty}p(\theta\mid x_1,\dots,x_n)=\delta(\theta-\theta_{ML})\)</p>
  </li>
</ul>

<h1 id="bayesian-ml-models">Bayesian ML models</h1>
<p>In ML, we have \(x\) features (observed variables) and \(y\) class labels or hidden representations (hidden or latent variables) with some model parameters \(\theta\) (e.g. weights of a linear model).</p>

<ul>
  <li>
    <p><strong>Discriminative approach, models \(p(y,\theta\mid x)\)</strong></p>

    <ul>
      <li>Cannot generate new objects since it needs \(x\) as an input and assumes that the prior over \(\theta\) does not depend on \(x\): \(p(y,\theta)=p(y\mid x,\theta)p(\theta)\)</li>
      <li>Examples: 1) classification/regression (hidden space is small) 2) Machine translation (complex hidden space)</li>
    </ul>
  </li>
  <li>
    <p><strong>Generative approach, models \(p(x,y,\theta)=p(x,y\mid\theta)p(\theta)\)</strong></p>

    <ul>
      <li>It can generate objects (pairs \(p(x,y)\)), but it can be hard to train since the observed space is most often more complicated.</li>
      <li>Example: Generation of text, speech, images, etc.</li>
    </ul>
  </li>
  <li>
    <p><strong>Training</strong></p>

    <p>Given data points \((X_{tr}, Y_{tr})\) and a discriminative model \(p(y,\theta\mid x)\).</p>

    <p><em>Use the Bayesian framework:</em></p>

\[p(\theta\mid X_{tr}, Y_{tr})=\frac{p(Y_{tr}\mid X_{tr},\theta)p(\theta)}{\int p(Y_{tr}\mid X_{tr},\theta)p(\theta) d\theta}\]

    <p>This results in a ensemble of algorithms rather than a single one \(\theta_{ML}\). Ensembles usually performs better than a single model.</p>

    <p>In addition, the posterior captures all dependencies from the training data and can be used later as a new prior.</p>
  </li>
  <li>
    <p><strong>Testing</strong></p>

    <p>We have the posterior \(p(\theta\mid X_{tr},Y_{tr})\) and a new data point \(x\). We can use the predictive distribution on its hidden value \(y\)</p>

\[p(y\mid x, X_{tr},Y_{tr}) = \int p(y\mid x,\theta)p(\theta\mid X_{tr},Y_{tr})d\theta\]
  </li>
  <li>
    <p><strong>Full Bayesian inference</strong></p>
  </li>
</ul>

<p>During training the evidence \(\int p(Y_{tr}\mid X_{tr},\theta)p(\theta) d\theta\) or in testing the predictive distribution \(\int p(y\mid x,\theta)p(\theta\mid X_{tr},Y_{tr})d\theta\) might be intractable, so it is impractical or impossible to perform full Bayesian inference. In other words, there is not closed form.</p>

<h1 id="conjugacy">Conjugacy</h1>
<h2 id="conjugate-distributions">Conjugate distributions</h2>

<p>Distribution \(p(y)\) and \(p(x\mid y)\) are <a href="(https://en.wikipedia.org/wiki/Conjugate_prior)">conjugate</a> \(\iff\) \(p(y\mid x)\) belongs to the same parametric family as \(p(y)\)</p>

\[p(y)\in\mathcal{A}(\alpha), p(x\mid y)\in\mathcal{B}(y) \rightarrow p(y\mid x)\in\mathcal{A}(\alpha')\]

<ul>
  <li><strong>There‚Äôs not conjugacy</strong> We can perform MAP to approximate the posterior with \(\theta_{MP}\) since we don‚Äôt need to calculate the normalization constant, but we cannot compute the true posterior.</li>
</ul>

\[\theta_{MP}=\operatorname*{arg\,max}p(\theta\mid X_{tr},Y_{tr})=\operatorname*{arg\,max}p(Y_{tr}\mid X_{tr},\theta)p(\theta)\]

<p>During testing:</p>

\[p(y\mid x,X_{tr},Y_{tr})=\int p(y\mid x,\theta)p(\theta\mid X_{tr},Y_{tr})d\theta\approx p(y\mid x,\theta_{MP})\]

<h2 id="conditional-conjugacy">Conditional conjugacy</h2>
<p>Given the model: \(p(x,\theta)=p(x\mid\theta)p(\theta)\) where \(\theta=[\theta_1,\dots,\theta_m]\)</p>

<p>Conditional conjugacy of likelihood and prior on each \(\theta_j\) conditional on all other \(\{\theta_i\}_{i\neq j}\)</p>

\[p(\theta_j\mid\theta_{i\neq j})\in\mathcal{A}(\alpha), p(x\mid\theta_j,\theta_{i\neq j})\in\mathcal{B}(\theta_j) \rightarrow p(\theta_j\mid x,\theta_{i\neq j})\in\mathcal{A}(\alpha')\]

<p>Check conditional conjugacy in practice:
For each \(\theta_j\)</p>
<ul>
  <li>Fix all other \(\{\theta_i\}_{i\neq j}\) (look at them as constants)</li>
  <li>Check whether \(p(x\mid\theta)\) and \(p(\theta)\) are conjugate w.r.t. \(\theta_j\)</li>
</ul>

<h1 id="variational-inference">Variational Inference</h1>
<p>Given the model \(p(x,\theta)=p(x\mid \theta)p(\theta)\), find a posterior approximation \(p(\theta\mid x) \approx q(\theta)\in\mathcal{Q}\), such that:</p>

\[KL(q(\theta)\parallel p(\theta\mid x)) \rightarrow \min_{q(\theta)\in\mathcal{Q}}\]

<p>KL is a good mismatch measure between two distributions over the <strong>same domain</strong> (see figure). And it has the following properties:</p>

<ol>
  <li>
\[KL(q\parallel p) \geq 0\]
  </li>
  <li>
\[KL(q\parallel p)=0 \Leftrightarrow q=p\]
  </li>
  <li>
\[KL(q\parallel p \neq KL(p\parallel q))\]
  </li>
</ol>

<p><img src="/assets/img/kl/kl_mismatch.png" alt="KL" /></p>

<h2 id="evidence-lower-bound-elbo-derivation">Evidence Lower Bound (ELBO) derivation</h2>

<ul>
  <li><strong>Posterior</strong>: \(p(\theta\mid x)\)</li>
  <li><strong>Evidence</strong>: \(p(x)\), shows the total probability of the observing data.</li>
  <li><strong>Lower bound</strong>: \(\log p(x) \geq \mathcal{L}(q(\theta))\)</li>
</ul>

\[\begin{align*}
  \log p(x)  &amp;= \int q(\theta) \log p(x)d\theta\\
            &amp;= \int q(\theta) \log\frac{p(x,\theta)}{p(\theta\mid x)}d\theta\\
            &amp;= \int q(\theta) \log\frac{p(x,\theta)q(\theta)}{p(\theta\mid x)q(\theta)}d\theta\\
            &amp;= \int q(\theta) \log\frac{p(x,\theta)}{q(\theta)}d\theta + \int q(\theta) \log\frac{q(\theta)}{p(\theta\mid x)}d\theta\\
            &amp;= \mathcal{L}(q(\theta)) + KL(q(\theta)\parallel p(\theta\mid x))
\end{align*}\]

<p><strong>Note</strong>:</p>
<ul>
  <li>\(\log p(x)\) does <strong>not depend</strong> on \(q\)</li>
  <li>\(\mathcal{L}\) and \(KL\) <strong>depend</strong> on \(q\)</li>
  <li>minimizing \(KL\) is the same as maximizing \(\mathcal{L}\).</li>
</ul>

\[KL(q(\theta)\parallel p(\theta\mid x)) \rightarrow \min_{q(\theta)\in\mathcal{Q}} \Leftrightarrow \mathcal{L}(q(\theta)) \rightarrow\max_{q(\theta)\in\mathcal{Q}}\]

<h3 id="optimizing-elbo-mathcall">Optimizing ELBO \(\mathcal{L}\)</h3>
<p><strong>Goal:</strong> \(\mathcal{L}(q(\theta)) \rightarrow\max_{q(\theta)\in\mathcal{Q}}\)</p>

\[\begin{align*}
      \mathcal{L}(q(\theta)) &amp;= \int q(\theta) \log\frac{p(x,\theta)}{q(\theta)}d\theta\\    
                             &amp;= \int q(\theta) \log p(x\mid\theta)d\theta +
                                \int q(\theta) \log\frac{p(\theta)}{q(\theta)}d\theta\\
                             &amp;= \mathbb{E}_{q(\theta)} \log p(x\mid\theta)
                                - KL(q(\theta)\parallel p(\theta))
  \end{align*}\]

<ul>
  <li>Data term: \(\mathbb{E}_{q(\theta)} \log p(x\mid\theta)\)</li>
  <li>Regularizer: \(KL(q(\theta)\parallel p(\theta))\)</li>
</ul>

<p>Necessary to perform optimization w.r.t. a distribution \(\max_{q(\theta)\in\mathcal{Q}} \mathcal{L}(q(\theta))\). Hard problem! In VI, we approximate with an approximate distribution \(q\). This approximate distribution can belong to a factorized or parametric family.</p>
<ol>
  <li><strong>Mean field approximation</strong>: Factorized family, \(q(\theta)=\prod_{j=1}^m q_j(\theta_j)\), \(\theta=[\theta_1,\dots,\theta_m]\)</li>
  <li><strong>Parametric approximation</strong>: Parametric family, \(q(\theta)=q(\theta\mid \lambda)\)</li>
</ol>

<h4 id="mean-field-approximation">Mean Field Approximation</h4>
<p>Mean field assumes that \(\theta_1,\dots,\theta_m\) are independent.</p>
<ul>
  <li>Apply product rule to distribution \(q\): \(q(\theta)=\prod_{j=1}^m q_j(\theta_j\mid\theta_{&lt;j})\)</li>
  <li>Apply i.i.d. assumption: \(q(\theta)=\prod_{j=1}^m q_j(\theta_j)\)</li>
</ul>

<p>The optimization problem becomes:</p>

\[\max_{\prod_{j=1}^m q_j(\theta_j)\in\mathcal{Q}} \mathcal{L}(q(\theta))\]

<p>This can be solved with <strong>block coordinate assent</strong> as follows: <strong>at each step fix all factors \(\{q_i(\theta_i)\}_{i\neq j}\) except one and optimize w.r.t. to it \(\max_{q_j(\theta_j)}\mathcal{L}(q(\theta))\)</strong></p>

<p><strong>Derivation</strong></p>

\[\begin{align*}
      \mathcal{L}(q(\theta)) &amp;= \mathbb{E}_{q(\theta)} \log p(x,\theta)
                              - \mathbb{E}_{q(\theta)} \log q(\theta) \\
                             &amp;= \mathbb{E}_{q(\theta)} \log p(x,\theta)
                                -  \sum_{k=1}^m \mathbb{E}_{q_k(\theta_k)} \log q_k(\theta_k) \\
                             &amp;= \mathbb{E}_{q(\theta)} \log p(x,\theta)
                                -  \mathbb{E}_{q_j(\theta_j)} \log q_j(\theta_j) + C \\
                                &amp;= \{r_j(\theta_j)=\frac{1}{Z_j}\exp(\mathbb{E}_{q_{i\neq j}} \log p(x,\theta))\}\\
                                &amp;= \mathbb{E}_{q_j(\theta_j)} \log r_j(\theta_j)
                                   -  \mathbb{E}_{q_j(\theta_j)} \log q_j(\theta_j) + C \\
                                &amp;= - KL(q_j(\theta_j)\parallel r_j(\theta_j)) + C
  \end{align*}\]

<p>So, the optimization problem for step \(j\) is:</p>

\[\max_{q_j(\theta_j)}\mathcal{L}(q(\theta)) = \max_{q_j(\theta_j)} - KL(q_j(\theta_j)\parallel r_j(\theta_j)) + C\]

<p>Where this happens when:</p>

\[q_j(\theta_j) = r_j(\theta_j) = \frac{1}{Z_j}\exp(\mathbb{E}_{q_{i\neq j}} \log p(x,\theta))\]

\[\log q_j(\theta_j) = \mathbb{E}_{q_{i\neq j}} \log p(x,\theta) + C\]

<p>Block coordinate assent can be described in two steps 1) initialize; 2) iterate</p>

<ol>
  <li>Initialize: \(q(\theta)=\prod_{j=1}^m q_j(\theta_j)\)</li>
  <li>Iterate (repeat until ELBO converge):
    <ul>
      <li>Update each factor \(q_1,\dots,q_m\): \(q_j(\theta_j)=\frac{1}{Z_j}\exp(\mathbb{E}_{q_{i\neq j}} \log p(x,\theta))\)</li>
      <li>Compute ELBO \(\mathcal{L}(q(\theta))\)</li>
    </ul>
  </li>
</ol>

<p><strong>Note:</strong> Mean-field can be applied when we can compute analytically \(\mathbb{E}_{q_{i\neq j}} \log p(x,\theta)\). In other words, applicable when we can compute the conditional conjugacy.</p>

<h4 id="parametric-approximation">Parametric Approximation</h4>
<p>Select a parametric family of variational distributions, \(q(\theta)=q(\theta\mid\lambda)\), where \(\lambda\) is a variational parameter.</p>

<p>The restriction is that we need to select a family of some fixed form, and as a result:</p>
<ul>
  <li>it might be too simple and insufficient to model the data</li>
  <li>if it is complex enough then there is no guarantee we can train it well to fit the data</li>
</ul>

<p>The ELBO is:</p>

\[\max_{\lambda}\mathcal{L}(q\theta\mid\lambda)=\int q(\theta\mid\lambda)\log\frac{p(x\mid\theta)}{q(\theta\mid\lambda)}d\theta\]

<p>If we‚Äôre able to calculate derivatives of ELBO w.r.t \(\theta\), then we can solve this problem using some numerical optimization solver.</p>

<h3 id="inference-methods">Inference methods</h3>
<p>So we have:</p>
<ol>
  <li>Full Bayesian inference: \(p(\theta\mid x)\)</li>
  <li>MAP inference: \(p(\theta\mid x)\approx \delta (\theta-\theta_{MP})\)</li>
  <li>Mean field variational inference: \(p(\theta\mid x)\approx q(\theta)=\prod_{j=1}^m q_j(\theta_j)\)</li>
  <li>Parametric variational inference: \(p(\theta\mid x)\approx q(\theta)=q(\theta\mid\lambda)\)</li>
</ol>

<h1 id="latent-variable-model">Latent variable model</h1>
<h2 id="mixture-of-gaussians">Mixture of Gaussians</h2>
<p>Establish a latent variable \(z_i\) for each data point \(x_i\) that denotes the \(ith\) gaussian where the model was generated.</p>

<p>Model:</p>

\[\begin{align*}
    p(X,Z\mid\theta) &amp;= \prod_i^n p(x_i,z_i\mid\theta)\\
                     &amp;= \prod_i^n p(x_i\mid z_i,\theta)p(z_i\mid\theta)\\
                     &amp;= \prod_i^n \pi_{z_i}\mathcal{N}(x_i\mid\mu_{z_i},\sigma_{z_i}^2)
  \end{align*}\]

<p>where \(\pi_j=p(z_i=j)\) is the prior of the \(jth\) gaussian and \(\theta=\{\mu_j,\sigma^2_j,\pi_j\}_{j=1}^K\) are the parameters to estimate.</p>

<p><strong>Note:</strong> If \(X\) and \(Z\) are known, we can use ML. For instance:</p>

\[\begin{align*}
    \theta_{ML}&amp;=\operatorname*{arg\,max}_{\theta} p(X,Z\mid\theta)\\
               &amp;=\operatorname*{arg\,max}_{\theta} \log p(X,Z\mid\theta)
  \end{align*}\]

<ul>
  <li>Since \(Z\) is a latent variable, we need to maximize the log of incomplete likelihood w.r.t. \(\theta\).</li>
  <li>Instead of optimizing \(\log p(X\mid \theta)\), we optimize the variational lower bound w.r.t. to both \(\theta\) and \(q(Z)\)</li>
  <li>This can be solved by block-coordinate algorithm a.k.a. EM-algorithm.</li>
</ul>

<blockquote>
  <p><strong>Variational Lower Bound:</strong> \(g(\xi,\theta)\) is the variational lower bound function for \(f(x)\) iff:</p>
  <ol>
    <li>For all \(\xi\) for all \(x\): \(f(x)\geq g(\xi,x)\)</li>
    <li>For any \(x_0\) exists \(\xi(x_0)\) such that: \(f(x_0)=g(\xi(x0),x_0)\)</li>
  </ol>
</blockquote>

<blockquote>
  <p>If we find such variational lower bound, instead of solving
\(f(x)\rightarrow\max_x\), we can interatively perform block coordinate updates of \(g(\xi, x)\).</p>
  <ol>
    <li>
\[x_n=\operatorname*{arg\,max}_{x}g(\xi_{n-1},x)\]
    </li>
    <li>
\[\xi_n=\xi(x_n)=\operatorname*{arg\,max}_{\xi} g(\xi,x_n)\]
    </li>
  </ol>
</blockquote>

<p><strong>Expectation Maximization algorithm</strong>
We want to solve:</p>

\[\operatorname*{arg\,max}_{q,\theta}\mathcal{L}(q, \theta) = \operatorname*{arg\,max}_{q,\theta}\int q(Z)\frac{p(X,Z\mid\theta)}{q(Z)}dZ\]

<p><strong>Algorithm</strong>:
Set an initial point \(\theta_0\)</p>

<p>Repeat iteratively 1 and 2 until convergence</p>
<ol>
  <li>E-step, find:
\(\begin{align*}
 q(Z)&amp;=\operatorname*{arg\,max}_{q}\mathcal{L}(q,\theta_0)\\
    &amp;=\operatorname*{arg\,max}_{q}{KL}(q\parallel p)\\
    &amp;=p(Z\mid X,\theta_0)
  \end{align*}\)</li>
  <li>M-step, solve:
\(\begin{align*}
 \theta_*&amp;=\operatorname*{arg\,max}_{\theta}\mathcal{L}(q,\theta)\\
    &amp;=\operatorname*{arg\,max}_{\theta}\mathbb{E}_Z \log p(X,Z\mid\theta)
  \end{align*}\)
    <ul>
      <li>Set \(\theta_0=\theta_*\) and go to 1</li>
    </ul>
  </li>
</ol>

<p>EM monotonically increases the lower bound and converges to a stationary point of \(\log p(X\mid\theta)\), see figure.</p>

<p><img src="/assets/img/em/em_optimize.gif" alt="EM algorithm" /></p>

<p><strong>Benefits of EM</strong></p>
<ul>
  <li>In some cases E-step and M-step can be solved in closed-information</li>
  <li>Allow to build more complicated models</li>
  <li>If true posterior \(p(Z\mid X,\theta)\) is intractable, we may search for the closest \(q(Z)\) <em>among tractable distributions</em> by solving optimization problem</li>
  <li>Allows to process missing data by treating them as latent variables
    <ul>
      <li>It can deal with both discrete and latent variables</li>
    </ul>
  </li>
</ul>

<p><strong>Categorical latent variables</strong>
Since \(z_i\in\{1,\dots,K\}\) the marginal of a mixture of gaussians is a finite mixture of distributions:</p>

\[p(x_i\mid\theta)=\sum_{k=1}^Kp(x_i\mid k,\theta)p(z_i=k\mid\theta)\]

<ul>
  <li>E-step is closed-form: \(q(z_i=k)=p(z_i=k\mid x_i,\theta)=\frac{p(x_i\mid k,\theta)p(z_i=k\mid\theta)}{\sum_{l=1}^Kp(x_i\mid l,\theta)p(z_i=l\mid\theta)}\)</li>
  <li>M-step is a sum of finite terms: \(\mathbb{E}_Z\log p(X,Z\mid\theta)=\sum_{i=1}^n\mathbb{E}_{z_i}\log p(x_i,z_i\mid\theta)=\sum_{i=1}^n\sum_{k=1}^K q(z_i=k)\log p(x_i,k\mid\theta)\)</li>
</ul>

<p><strong>Continuous latent variables</strong>
A mixture of continuous distributions</p>

\[p(x_i\mid\theta)=\int p(x_i\mid z_i,\theta)p(z_i\mid\theta) dz_i\]

<ul>
  <li>
    <p>E-step: only done in closed form when <strong>conjugate distributions</strong>, otherwise the true posterior is intractable</p>

\[q(z_i)=p(z_i\mid x_i,\theta)=\frac{p(x_i\mid z_i,\theta)p(z_i\mid\theta)}{\int p(x_i\mid z_i,\theta)p(z_i\mid\theta) dz_i}\]
  </li>
</ul>

<p>Typically continuous latent variable are used for dimensionality reduction a.k.a. <em>representation learning</em></p>

<h1 id="log-derivative-trick">Log-derivative trick</h1>

\[\frac{\partial}{\partial x}p(y\mid x)=p(y\mid x)\frac{\partial}{\partial x}\log p(y\mid x)\]

<p>For example, we commonly find expressions as follows:</p>

\[\begin{align*}
    \frac{\partial}{\partial x}\int p(y\mid x)h(x,y)dy &amp;= \int \frac{\partial}{\partial x} p(y\mid x)h(x,y)dy\\
                                  &amp;= \int \left(h(x,y)\frac{\partial}{\partial x} p(y\mid x) + p(y\mid x)\frac{\partial}{\partial x} h(x,y) \right)dy \\
                                  &amp;= \int p(y\mid x)\frac{\partial}{\partial x} h(x,y) dy + \int h(x,y)\frac{\partial}{\partial x} p(y\mid x)dy \\
                                  &amp;= \int p(y\mid x)\frac{\partial}{\partial x} h(x,y) dy + \int p(y\mid x)h(x,y)\frac{\partial}{\partial x} \log p(y\mid x)dy
  \end{align*}\]

<p>Now, the first term can be replaced with Monte Carlo estimate of expectation. Using the log-derivative trick, the second expectation can also be estimated via Monte Carlo.</p>

<h1 id="score-function">Score function</h1>

<p>It is the gradient of the log-likelihood function with respect to the parameter vector. Since it has zero mean, the value \(z_i^*\) in \(\nabla_{\phi}\log q(z_i^*\mid\theta)\) oscillates around zero.</p>

\[\begin{align*}
    \nabla_{\phi}\log q(z_i\mid\theta)
  \end{align*}\]

<p>Proof it has zero mean:</p>

\[\begin{align*}
  \int q(z_i\mid\theta)\nabla_{\phi}\log q(z_i\mid\theta) d z_i&amp;=\int\frac{q(z_i\mid\theta)}{q(z_i\mid\theta)}\nabla_{\phi}q(z_i\mid\theta)d z_i\\
                                                              &amp;= \nabla_{\phi}\int q(z_i\mid\theta)dz_i\\
                                                              &amp;= \nabla_{\phi} 1 =0
\end{align*}\]

<h1 id="reinforce">REINFORCE</h1>


  </article>

  

</div>

    </div>

    <!-- Footer -->

    
<footer class="sticky-bottom mt-5">
  <div class="container">
    &copy; Copyright 2021 Francisco Xavier Sumba T..
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank">Unsplash</a>.

    
    
    Last updated: December 07, 2021.
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
