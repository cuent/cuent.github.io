<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2021-12-07T21:27:54-05:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Xavier Sumba</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">a distill-style blog post</title><link href="http://localhost:4000/blog/2021/distill/" rel="alternate" type="text/html" title="a distill-style blog post" /><published>2021-05-22T00:00:00-04:00</published><updated>2021-05-22T00:00:00-04:00</updated><id>http://localhost:4000/blog/2021/distill</id><content type="html" xml:base="http://localhost:4000/blog/2021/distill/">**NOTE:**
Citations, footnotes, and code blocks do not display correctly in the dark mode since distill does not support the dark mode by default.
If you are interested in correctly adding dark mode support for distill, please open [a discussion](https://github.com/alshedivat/al-folio/discussions) and let us know.


## Equations

This theme supports rendering beautiful math in inline and display modes using [MathJax 3](https://www.mathjax.org/){:target=&quot;\_blank&quot;} engine.
You just need to surround your math expression with `$$`, like `$$ E = mc^2 $$`.
If you leave it inside a paragraph, it will produce an inline expression, just like $$ E = mc^2 $$.

To use display mode, again surround your expression with `$$` and place it as a separate paragraph.
Here is an example:

$$
\left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right)
$$

Note that MathJax 3 is [a major re-write of MathJax](https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html){:target=&quot;\_blank&quot;} that brought a significant improvement to the loading and rendering speed, which is now [on par with KaTeX](http://www.intmath.com/cg5/katex-mathjax-comparison.php){:target=&quot;\_blank&quot;}.


***

## Citations

Citations are then used in the article body with the `&lt;d-cite&gt;` tag.
The key attribute is a reference to the id provided in the bibliography.
The key attribute can take multiple ids, separated by commas.

The citation is presented inline like this: &lt;d-cite key=&quot;gregor2015draw&quot;&gt;&lt;/d-cite&gt; (a number that displays more information on hover).
If you have an appendix, a bibliography is automatically created and populated in it.

Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover.
However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well — the authors are human and it’s nice for them to have the community associate them with their work.

***

## Footnotes

Just wrap the text you would like to show up in a footnote in a `&lt;d-footnote&gt;` tag.
The number of the footnote will be automatically generated.&lt;d-footnote&gt;This will become a hoverable footnote.&lt;/d-footnote&gt;

***

## Code Blocks

Syntax highlighting is provided within `&lt;d-code&gt;` tags.
An example of inline code snippets: `&lt;d-code language=&quot;html&quot;&gt;let x = 10;&lt;/d-code&gt;`.
For larger blocks of code, add a `block` attribute:

&lt;d-code block language=&quot;javascript&quot;&gt;
  var x = 25;
  function(x) {
    return x * x;
  }
&lt;/d-code&gt;

**Note:** `&lt;d-code&gt;` blocks do not look well in the dark mode.
You can always use the default code-highlight using the `highlight` liquid tag:

{% highlight javascript %}
var x = 25;
function(x) {
  return x * x;
}
{% endhighlight %}

***

## Layouts

The main text column is referred to as the body.
It is the assumed layout of any direct descendants of the `d-article` element.

&lt;div class=&quot;fake-img l-body&quot;&gt;
  &lt;p&gt;.l-body&lt;/p&gt;
&lt;/div&gt;

For images you want to display a little larger, try `.l-page`:

&lt;div class=&quot;fake-img l-page&quot;&gt;
  &lt;p&gt;.l-page&lt;/p&gt;
&lt;/div&gt;

All of these have an outset variant if you want to poke out from the body text a little bit.
For instance:

&lt;div class=&quot;fake-img l-body-outset&quot;&gt;
  &lt;p&gt;.l-body-outset&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;fake-img l-page-outset&quot;&gt;
  &lt;p&gt;.l-page-outset&lt;/p&gt;
&lt;/div&gt;

Occasionally you’ll want to use the full browser width.
For this, use `.l-screen`.
You can also inset the element a little from the edge of the browser by using the inset variant.

&lt;div class=&quot;fake-img l-screen&quot;&gt;
  &lt;p&gt;.l-screen&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;fake-img l-screen-inset&quot;&gt;
  &lt;p&gt;.l-screen-inset&lt;/p&gt;
&lt;/div&gt;

The final layout is for marginalia, asides, and footnotes.
It does not interrupt the normal flow of `.l-body` sized text except on mobile screen sizes.

&lt;div class=&quot;fake-img l-gutter&quot;&gt;
  &lt;p&gt;.l-gutter&lt;/p&gt;
&lt;/div&gt;


Emphasis, aka italics, with *asterisks* or _underscores_.

Strong emphasis, aka bold, with **asterisks** or __underscores__.

Combined emphasis with **asterisks and _underscores_**.

Strikethrough uses two tildes. ~~Scratch this.~~

1. First ordered list item
2. Another item
⋅⋅* Unordered sub-list. 
1. Actual numbers don&apos;t matter, just that it&apos;s a number
⋅⋅1. Ordered sub-list
4. And another item.

⋅⋅⋅You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we&apos;ll use three here to also align the raw Markdown).

⋅⋅⋅To have a line break without a paragraph, you will need to use two trailing spaces.⋅⋅
⋅⋅⋅Note that this line is separate, but within the same paragraph.⋅⋅
⋅⋅⋅(This is contrary to the typical GFM line break behaviour, where trailing spaces are not required.)

* Unordered list can use asterisks
- Or minuses
+ Or pluses

[I&apos;m an inline-style link](https://www.google.com)

[I&apos;m an inline-style link with title](https://www.google.com &quot;Google&apos;s Homepage&quot;)

[I&apos;m a reference-style link][Arbitrary case-insensitive reference text]

[I&apos;m a relative reference to a repository file](../blob/master/LICENSE)

[You can use numbers for reference-style link definitions][1]

Or leave it empty and use the [link text itself].

URLs and URLs in angle brackets will automatically get turned into links. 
http://www.example.com or &lt;http://www.example.com&gt; and sometimes 
example.com (but not on Github, for example).

Some text to show that the reference links can follow later.

[arbitrary case-insensitive reference text]: https://www.mozilla.org
[1]: http://slashdot.org
[link text itself]: http://www.reddit.com

Here&apos;s our logo (hover to see the title text):

Inline-style: 
![alt text](https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png &quot;Logo Title Text 1&quot;)

Reference-style: 
![alt text][logo]

[logo]: https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png &quot;Logo Title Text 2&quot;

Inline `code` has `back-ticks around` it.

```javascript
var s = &quot;JavaScript syntax highlighting&quot;;
alert(s);
```
 
```python
s = &quot;Python syntax highlighting&quot;
print s
```
 
```
No language indicated, so no syntax highlighting. 
But let&apos;s throw in a &lt;b&gt;tag&lt;/b&gt;.
```

Colons can be used to align columns.

| Tables        | Are           | Cool  |
| ------------- |:-------------:| -----:|
| col 3 is      | right-aligned | $1600 |
| col 2 is      | centered      |   $12 |
| zebra stripes | are neat      |    $1 |

There must be at least 3 dashes separating each header cell.
The outer pipes (|) are optional, and you don&apos;t need to make the 
raw Markdown line up prettily. You can also use inline Markdown.

Markdown | Less | Pretty
--- | --- | ---
*Still* | `renders` | **nicely**
1 | 2 | 3

&gt; Blockquotes are very handy in email to emulate reply text.
&gt; This line is part of the same quote.

Quote break.

&gt; This is a very long line that will still be quoted properly when it wraps. Oh boy let&apos;s keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can *put* **Markdown** into a blockquote. 


Three or more...

---

Hyphens

***

Asterisks

___

Underscores

Here&apos;s a line for us to start with.

This line is separated from the one above by two newlines, so it will be a *separate paragraph*.

This line is also a separate paragraph, but...
This line is only separated by a single newline, so it&apos;s a separate line in the *same paragraph*.</content><author><name>Albert Einstein</name></author><summary type="html">NOTE: Citations, footnotes, and code blocks do not display correctly in the dark mode since distill does not support the dark mode by default. If you are interested in correctly adding dark mode support for distill, please open a discussion and let us know.</summary></entry><entry><title type="html">a post with github metadata</title><link href="http://localhost:4000/blog/2020/github-metadata/" rel="alternate" type="text/html" title="a post with github metadata" /><published>2020-09-28T17:01:00-04:00</published><updated>2020-09-28T17:01:00-04:00</updated><id>http://localhost:4000/blog/2020/github-metadata</id><content type="html" xml:base="http://localhost:4000/blog/2020/github-metadata/">A sample blog page that demonstrates the accessing of github meta data.

## What does Github-MetaData do?
* Propagates the site.github namespace with repository metadata
* Setting site variables : 
  * site.title
  * site.description
  * site.url
  * site.baseurl
* Accessing the metadata - duh.
* Generating edittable links.

## Additional Reading
* If you&apos;re recieving incorrect/missing data, you may need to perform a Github API&lt;a href=&quot;https://github.com/jekyll/github-metadata/blob/master/docs/authentication.md&quot;&gt; authentication&lt;/a&gt;.
* Go through this &lt;a href=&quot;https://jekyll.github.io/github-metadata/&quot; target=&quot;blank&quot;&gt;README&lt;/a&gt; for more details on the topic.
* &lt;a href= &quot;https://github.com/jekyll/github-metadata/blob/master/docs/site.github.md&quot;&gt;This page&lt;/a&gt; highlights all the feilds you can access with github-metadata.
&lt;br /&gt;

## Example MetaData
* Host Name : {{ site.github.hostname }}
* URL : {{ site.github.url }}
* BaseURL : {{ site.github.baseurl }}
* Archived : {{ site.github.archived}}
* Contributors : 
{% for contributor in site.github.contributors %}
  * {{ contributor.login }}
{% endfor %}</content><author><name></name></author><summary type="html">A sample blog page that demonstrates the accessing of github meta data. What does Github-MetaData do? Propagates the site.github namespace with repository metadata Setting site variables : site.title site.description site.url site.baseurl Accessing the metadata - duh. Generating edittable links. Additional Reading If you’re recieving incorrect/missing data, you may need to perform a Github API authentication. Go through this README for more details on the topic. This page highlights all the feilds you can access with github-metadata. Example MetaData Host Name : URL : BaseURL : Archived : Contributors :</summary></entry><entry><title type="html">a post with twitter</title><link href="http://localhost:4000/blog/2020/twitter/" rel="alternate" type="text/html" title="a post with twitter" /><published>2020-09-28T11:12:00-04:00</published><updated>2020-09-28T11:12:00-04:00</updated><id>http://localhost:4000/blog/2020/twitter</id><content type="html" xml:base="http://localhost:4000/blog/2020/twitter/">A sample blog page that demonstrates the inclusion of Tweets/Timelines/etc.

# Tweet
An example of displaying a tweet:
{% twitter https://twitter.com/rubygems/status/518821243320287232 %}

# Timeline
An example of pulling from a timeline:
{% twitter https://twitter.com/jekyllrb maxwidth=500 limit=3 %}

# Additional Details
For more details on using the plugin visit: [jekyll-twitter-plugin](https://github.com/rob-murray/jekyll-twitter-plugin){:target=&quot;\_blank&quot;}</content><author><name></name></author><summary type="html">A sample blog page that demonstrates the inclusion of Tweets/Timelines/etc. Tweet An example of displaying a tweet: jekyll-twitter-plugin (1.0.0): A Liquid tag plugin for Jekyll that renders Tweets from Twitter API http://t.co/m4EIQPM9h4&amp;mdash; RubyGems (@rubygems) October 5, 2014 Timeline An example of pulling from a timeline: Tweets by jekyllrb Additional Details For more details on using the plugin visit: jekyll-twitter-plugin</summary></entry><entry><title type="html">Graph Representation Learning notes</title><link href="http://localhost:4000/blog/2020/graph-representation-learning/" rel="alternate" type="text/html" title="Graph Representation Learning notes" /><published>2020-01-25T14:41:19-05:00</published><updated>2020-01-25T14:41:19-05:00</updated><id>http://localhost:4000/blog/2020/graph-representation-learning</id><content type="html" xml:base="http://localhost:4000/blog/2020/graph-representation-learning/">## Introduction

Graphs are everywhere and most knowledge can be modeled with graphs. Graphs can be understood as a complex data structure that can be used as a universal language. Therefore, as any language, graphs communicate ideas using nodes (objects) and edges (interaction); thus any complex system can be modeled by creating relationships between points. We focus in the use machine learning on graphs, but the field of network analysis provide other approaches to analyze, understand and learn from graphs.

### Graphs: data structure
Graphs are composed with a set of nodes $$\mathcal{V}$$ on a set de edges $$\mathcal{E}$$ that define a relation $$\mathcal{G}=(\mathcal{V}, \mathcal{E})$$.

$$u,v\in\mathcal{V}$$

Also, notice that *graph* and *network* are terms used interchangeably, but graphs involves properties of real-world data while a network provides mathematical properties for graphs.

- **Adjacency Matrix**: $$A\in \mathbb{R}^{\mid V\mid\times\mid V\mid}
\begin{cases}
    A_{uv}=1&amp; \text{if } (u,v)\in\mathcal{E}\\
    A_{uv}=0              &amp; \text{otherwise}
\end{cases}$$
  - *directed*: no symmetric
  - *undirected*: symmetric
  - *weighted*: real values $$\{0,1\}$$

- **Graph Types**
  - *Simple Graph*: relation $$(u,v)\in\mathcal{E}$$
  - *Multi-relational*: considers type of edge with $$R$$ types, i.e. $$(u,\tau,v)\in\mathcal{E}, A_\tau$$ and $$A\in\mathbb{R}^{\mid V\mid\times\mid V\mid\times\mid R\mid}$$
  - *Attributes*: some graphs have information associated with each node $$X\in\mathbb{R}^{\mid V\mid\times\mid m\mid}$$.

### Machine Learning tasks on Graphs
ML on graphs involves common ML tasks such as 1) supervised learning: predict a target output; 2) unsupervised learning: infer patterns; 3) reinforcement learning: learn to act in an environment, but there are some task on graphs that lie on these categories.

- **Node classification**: Consist in predicting a label $$y_u$$ associated with a node $$u\in\mathcal{V}$$.

  Modeling in graphs is different since nodes are interconnected with each other and they are not i.i.d. Thus, some connection between nodes is needed. For example:
  1. homophily: nodes share attributes.
  2. structural equivalence: nodes with similar local neighborhood structure have similar labels.
  3. monophily: nodes with unrelated labels.

- **Relation prediction**: aka. link prediction or graph completion seeks to infer the relationship of a node with other nodes in the graph. The problems involves that given a set of nodes $$\mathcal{V}$$ and an incomplete set of edges $$\mathcal{E}_{\text{train}}\in\mathcal{E}$$, infer the missing edges $$\mathcal{E}\setminus\mathcal{E}_{\text{train}}$$. Complexity is highly depend on the type of graph (i.e. simple graph or multi-relational graph) and it often requires inductive biases specific for each graph domain.

- **Clustering and community detection**: An unsupervised task that finds a community structure where nodes are more likely to form edges with nodes that belong to the same community. The challenge is to infer the *community structure* given an input graph $$\mathcal{G}=(\mathcal{V},\mathcal{E})$$.


- **Graph classification/clustering**: Task over entire graphs. Graph classification involves that given multiple different graphs, learn predictions specific to each graph. And graph clustering (similarity matching) the goal is to learn an unsupervised measure of similarity between a set of i.i.d. graphs. Challenge is to define useful features that consider the relational structure of each datapoint.

### Traditional approaches
Uses for node classification tasks
- graph statistics
- kernel methods
Link prediction
- measure overlap between node neighborhoods
Clustering / community detection
- spectral clustering (graph Laplacians)

#### Graphs statistics and kernel methods
Extract statistics or features and use them in traditional machine learning classifiers.
- **Node level statistics and features**:
  1. **Node degree**: it is the number of incident edges to a node.

      $$d_u=\sum_{v\in V}A_{u,v}$$
  2. **Node centrality**: measures the importance of a node; it takes into account the importance of a node&apos;s neighbors.

      $$e_u=\frac{1}{\lambda}\sum_{v\in V}A_{u,v}e_v \forall_u \in V \text{   (eigenvector centrality)}$$

      We can compute the vector of node centrality $$\mathbb{e}$$ using the eigenvector equation with the adjacency matrix $$\lambda\mathbb{e}=A\mathbb{e}$$.

      We can use the eigenvector centrality to rank the likelihood that a node has visited infinity nodes on a random walk (due to the power of iteration) $$\mathbb{e}^{(t+1)}=A\mathbb{e^{(t)}}$$.

      Other measures:
        - _betweeness centrality_: measures how often a node lies on the shortest path between two other nodes.
        - _closeness centrality_: measures the average shortest path length between a node and all other nodes.

    3. **Clustering coefficient**:
    3. **Motifs**:




Graph-level stats -&gt; kernel methods</content><author><name></name></author><category term="GRL" /><summary type="html">Introduction</summary></entry><entry><title type="html">Notes on the Bayesian Framework</title><link href="http://localhost:4000/blog/2019/bayesian-framework/" rel="alternate" type="text/html" title="Notes on the Bayesian Framework" /><published>2019-11-01T21:41:19-04:00</published><updated>2019-11-01T21:41:19-04:00</updated><id>http://localhost:4000/blog/2019/bayesian-framework</id><content type="html" xml:base="http://localhost:4000/blog/2019/bayesian-framework/">&lt;p&gt;Some tools:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Stochastic variational inference&lt;/li&gt;
  &lt;li&gt;Variance reduction&lt;/li&gt;
  &lt;li&gt;Normalizing flows&lt;/li&gt;
  &lt;li&gt;Gaussian processes&lt;/li&gt;
  &lt;li&gt;Scalable MCMC algorithms&lt;/li&gt;
  &lt;li&gt;Semi-implicit variational inference&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;bayesian-framework&quot;&gt;Bayesian framework&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Bayes theorem&lt;/strong&gt;:&lt;/li&gt;
&lt;/ul&gt;

\[conditional = \frac{joint}{marginal}\]

\[p(x\mid y)=\frac{p(x,y)}{p(y)}\]

&lt;p&gt;It defines a rule for uncertainty conversion when new information arrives&lt;/p&gt;

\[posterior = \frac{likelihood \times prior}{evidence}\]

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Product rule&lt;/strong&gt;: any joint distribution can be expressed with conditional distributions&lt;/li&gt;
&lt;/ul&gt;

\[p(x,y,z)=p(x\mid y,z)p(y\mid z)p(z)\]

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Sum rule&lt;/strong&gt;: any marginal distribution can be obtained from the joint distribution by integrating out&lt;/li&gt;
&lt;/ul&gt;

\[p(y)=\int p(x,y)dx\]

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Statistical inference&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;&lt;strong&gt;Problem&lt;/strong&gt;: given i.i.d. data \(X=\{x_i\}_{i=1}^n\) from distribution \(p(x\mid\theta)\), estimate \(\theta\)&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;&lt;strong&gt;Frequentist framework&lt;/strong&gt;: use maximum likelihood estimation (MLE)&lt;/li&gt;
    &lt;/ol&gt;

\[\theta_{ML}=\operatorname*{arg\,max}p(X\mid\theta)=\operatorname*{arg\,max}\prod_{i=1}^n p(x_i\mid\theta)=\operatorname*{arg\,max}\sum_i^n\log p(x_i\mid\theta)\]

    &lt;p&gt;Applicability: \(n\ll d\)&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;&lt;strong&gt;Bayesian framework&lt;/strong&gt;: encode uncertainty about \(\theta\) in a prior \(p(\theta)\) and apply Bayesian inference&lt;/li&gt;
    &lt;/ol&gt;

\[p(\theta\mid X)=\frac{\prod_i^n p(x_i\mid\theta)p(\theta)}{\int\prod_i^n p(x_i\mid\theta)p(\theta)d\theta}\]

    &lt;p&gt;Applicability: \(\forall_nd\)&lt;/p&gt;

    &lt;p&gt;Advantages:
    - we can encode prior knowledge/desired properties into a prior distribution
    - prior is a form of regularization
    - additionally to the point estimate of \(\theta\), posterior contains information about the uncertainty of the estimate
    - frequentist case is a limit case of Bayesian one
      \(\lim_{n/d\to\infty}p(\theta\mid x_1,\dots,x_n)=\delta(\theta-\theta_{ML})\)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;bayesian-ml-models&quot;&gt;Bayesian ML models&lt;/h1&gt;
&lt;p&gt;In ML, we have \(x\) features (observed variables) and \(y\) class labels or hidden representations (hidden or latent variables) with some model parameters \(\theta\) (e.g. weights of a linear model).&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Discriminative approach, models \(p(y,\theta\mid x)\)&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Cannot generate new objects since it needs \(x\) as an input and assumes that the prior over \(\theta\) does not depend on \(x\): \(p(y,\theta)=p(y\mid x,\theta)p(\theta)\)&lt;/li&gt;
      &lt;li&gt;Examples: 1) classification/regression (hidden space is small) 2) Machine translation (complex hidden space)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Generative approach, models \(p(x,y,\theta)=p(x,y\mid\theta)p(\theta)\)&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;It can generate objects (pairs \(p(x,y)\)), but it can be hard to train since the observed space is most often more complicated.&lt;/li&gt;
      &lt;li&gt;Example: Generation of text, speech, images, etc.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Training&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;Given data points \((X_{tr}, Y_{tr})\) and a discriminative model \(p(y,\theta\mid x)\).&lt;/p&gt;

    &lt;p&gt;&lt;em&gt;Use the Bayesian framework:&lt;/em&gt;&lt;/p&gt;

\[p(\theta\mid X_{tr}, Y_{tr})=\frac{p(Y_{tr}\mid X_{tr},\theta)p(\theta)}{\int p(Y_{tr}\mid X_{tr},\theta)p(\theta) d\theta}\]

    &lt;p&gt;This results in a ensemble of algorithms rather than a single one \(\theta_{ML}\). Ensembles usually performs better than a single model.&lt;/p&gt;

    &lt;p&gt;In addition, the posterior captures all dependencies from the training data and can be used later as a new prior.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Testing&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;We have the posterior \(p(\theta\mid X_{tr},Y_{tr})\) and a new data point \(x\). We can use the predictive distribution on its hidden value \(y\)&lt;/p&gt;

\[p(y\mid x, X_{tr},Y_{tr}) = \int p(y\mid x,\theta)p(\theta\mid X_{tr},Y_{tr})d\theta\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Full Bayesian inference&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;During training the evidence \(\int p(Y_{tr}\mid X_{tr},\theta)p(\theta) d\theta\) or in testing the predictive distribution \(\int p(y\mid x,\theta)p(\theta\mid X_{tr},Y_{tr})d\theta\) might be intractable, so it is impractical or impossible to perform full Bayesian inference. In other words, there is not closed form.&lt;/p&gt;

&lt;h1 id=&quot;conjugacy&quot;&gt;Conjugacy&lt;/h1&gt;
&lt;h2 id=&quot;conjugate-distributions&quot;&gt;Conjugate distributions&lt;/h2&gt;

&lt;p&gt;Distribution \(p(y)\) and \(p(x\mid y)\) are &lt;a href=&quot;(https://en.wikipedia.org/wiki/Conjugate_prior)&quot;&gt;conjugate&lt;/a&gt; \(\iff\) \(p(y\mid x)\) belongs to the same parametric family as \(p(y)\)&lt;/p&gt;

\[p(y)\in\mathcal{A}(\alpha), p(x\mid y)\in\mathcal{B}(y) \rightarrow p(y\mid x)\in\mathcal{A}(\alpha&apos;)\]

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;There’s not conjugacy&lt;/strong&gt; We can perform MAP to approximate the posterior with \(\theta_{MP}\) since we don’t need to calculate the normalization constant, but we cannot compute the true posterior.&lt;/li&gt;
&lt;/ul&gt;

\[\theta_{MP}=\operatorname*{arg\,max}p(\theta\mid X_{tr},Y_{tr})=\operatorname*{arg\,max}p(Y_{tr}\mid X_{tr},\theta)p(\theta)\]

&lt;p&gt;During testing:&lt;/p&gt;

\[p(y\mid x,X_{tr},Y_{tr})=\int p(y\mid x,\theta)p(\theta\mid X_{tr},Y_{tr})d\theta\approx p(y\mid x,\theta_{MP})\]

&lt;h2 id=&quot;conditional-conjugacy&quot;&gt;Conditional conjugacy&lt;/h2&gt;
&lt;p&gt;Given the model: \(p(x,\theta)=p(x\mid\theta)p(\theta)\) where \(\theta=[\theta_1,\dots,\theta_m]\)&lt;/p&gt;

&lt;p&gt;Conditional conjugacy of likelihood and prior on each \(\theta_j\) conditional on all other \(\{\theta_i\}_{i\neq j}\)&lt;/p&gt;

\[p(\theta_j\mid\theta_{i\neq j})\in\mathcal{A}(\alpha), p(x\mid\theta_j,\theta_{i\neq j})\in\mathcal{B}(\theta_j) \rightarrow p(\theta_j\mid x,\theta_{i\neq j})\in\mathcal{A}(\alpha&apos;)\]

&lt;p&gt;Check conditional conjugacy in practice:
For each \(\theta_j\)&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Fix all other \(\{\theta_i\}_{i\neq j}\) (look at them as constants)&lt;/li&gt;
  &lt;li&gt;Check whether \(p(x\mid\theta)\) and \(p(\theta)\) are conjugate w.r.t. \(\theta_j\)&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;variational-inference&quot;&gt;Variational Inference&lt;/h1&gt;
&lt;p&gt;Given the model \(p(x,\theta)=p(x\mid \theta)p(\theta)\), find a posterior approximation \(p(\theta\mid x) \approx q(\theta)\in\mathcal{Q}\), such that:&lt;/p&gt;

\[KL(q(\theta)\parallel p(\theta\mid x)) \rightarrow \min_{q(\theta)\in\mathcal{Q}}\]

&lt;p&gt;KL is a good mismatch measure between two distributions over the &lt;strong&gt;same domain&lt;/strong&gt; (see figure). And it has the following properties:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
\[KL(q\parallel p) \geq 0\]
  &lt;/li&gt;
  &lt;li&gt;
\[KL(q\parallel p)=0 \Leftrightarrow q=p\]
  &lt;/li&gt;
  &lt;li&gt;
\[KL(q\parallel p \neq KL(p\parallel q))\]
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/kl/kl_mismatch.png&quot; alt=&quot;KL&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;evidence-lower-bound-elbo-derivation&quot;&gt;Evidence Lower Bound (ELBO) derivation&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Posterior&lt;/strong&gt;: \(p(\theta\mid x)\)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Evidence&lt;/strong&gt;: \(p(x)\), shows the total probability of the observing data.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Lower bound&lt;/strong&gt;: \(\log p(x) \geq \mathcal{L}(q(\theta))\)&lt;/li&gt;
&lt;/ul&gt;

\[\begin{align*}
  \log p(x)  &amp;amp;= \int q(\theta) \log p(x)d\theta\\
            &amp;amp;= \int q(\theta) \log\frac{p(x,\theta)}{p(\theta\mid x)}d\theta\\
            &amp;amp;= \int q(\theta) \log\frac{p(x,\theta)q(\theta)}{p(\theta\mid x)q(\theta)}d\theta\\
            &amp;amp;= \int q(\theta) \log\frac{p(x,\theta)}{q(\theta)}d\theta + \int q(\theta) \log\frac{q(\theta)}{p(\theta\mid x)}d\theta\\
            &amp;amp;= \mathcal{L}(q(\theta)) + KL(q(\theta)\parallel p(\theta\mid x))
\end{align*}\]

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;\(\log p(x)\) does &lt;strong&gt;not depend&lt;/strong&gt; on \(q\)&lt;/li&gt;
  &lt;li&gt;\(\mathcal{L}\) and \(KL\) &lt;strong&gt;depend&lt;/strong&gt; on \(q\)&lt;/li&gt;
  &lt;li&gt;minimizing \(KL\) is the same as maximizing \(\mathcal{L}\).&lt;/li&gt;
&lt;/ul&gt;

\[KL(q(\theta)\parallel p(\theta\mid x)) \rightarrow \min_{q(\theta)\in\mathcal{Q}} \Leftrightarrow \mathcal{L}(q(\theta)) \rightarrow\max_{q(\theta)\in\mathcal{Q}}\]

&lt;h3 id=&quot;optimizing-elbo-mathcall&quot;&gt;Optimizing ELBO \(\mathcal{L}\)&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Goal:&lt;/strong&gt; \(\mathcal{L}(q(\theta)) \rightarrow\max_{q(\theta)\in\mathcal{Q}}\)&lt;/p&gt;

\[\begin{align*}
      \mathcal{L}(q(\theta)) &amp;amp;= \int q(\theta) \log\frac{p(x,\theta)}{q(\theta)}d\theta\\    
                             &amp;amp;= \int q(\theta) \log p(x\mid\theta)d\theta +
                                \int q(\theta) \log\frac{p(\theta)}{q(\theta)}d\theta\\
                             &amp;amp;= \mathbb{E}_{q(\theta)} \log p(x\mid\theta)
                                - KL(q(\theta)\parallel p(\theta))
  \end{align*}\]

&lt;ul&gt;
  &lt;li&gt;Data term: \(\mathbb{E}_{q(\theta)} \log p(x\mid\theta)\)&lt;/li&gt;
  &lt;li&gt;Regularizer: \(KL(q(\theta)\parallel p(\theta))\)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Necessary to perform optimization w.r.t. a distribution \(\max_{q(\theta)\in\mathcal{Q}} \mathcal{L}(q(\theta))\). Hard problem! In VI, we approximate with an approximate distribution \(q\). This approximate distribution can belong to a factorized or parametric family.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Mean field approximation&lt;/strong&gt;: Factorized family, \(q(\theta)=\prod_{j=1}^m q_j(\theta_j)\), \(\theta=[\theta_1,\dots,\theta_m]\)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Parametric approximation&lt;/strong&gt;: Parametric family, \(q(\theta)=q(\theta\mid \lambda)\)&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;mean-field-approximation&quot;&gt;Mean Field Approximation&lt;/h4&gt;
&lt;p&gt;Mean field assumes that \(\theta_1,\dots,\theta_m\) are independent.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Apply product rule to distribution \(q\): \(q(\theta)=\prod_{j=1}^m q_j(\theta_j\mid\theta_{&amp;lt;j})\)&lt;/li&gt;
  &lt;li&gt;Apply i.i.d. assumption: \(q(\theta)=\prod_{j=1}^m q_j(\theta_j)\)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The optimization problem becomes:&lt;/p&gt;

\[\max_{\prod_{j=1}^m q_j(\theta_j)\in\mathcal{Q}} \mathcal{L}(q(\theta))\]

&lt;p&gt;This can be solved with &lt;strong&gt;block coordinate assent&lt;/strong&gt; as follows: &lt;strong&gt;at each step fix all factors \(\{q_i(\theta_i)\}_{i\neq j}\) except one and optimize w.r.t. to it \(\max_{q_j(\theta_j)}\mathcal{L}(q(\theta))\)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Derivation&lt;/strong&gt;&lt;/p&gt;

\[\begin{align*}
      \mathcal{L}(q(\theta)) &amp;amp;= \mathbb{E}_{q(\theta)} \log p(x,\theta)
                              - \mathbb{E}_{q(\theta)} \log q(\theta) \\
                             &amp;amp;= \mathbb{E}_{q(\theta)} \log p(x,\theta)
                                -  \sum_{k=1}^m \mathbb{E}_{q_k(\theta_k)} \log q_k(\theta_k) \\
                             &amp;amp;= \mathbb{E}_{q(\theta)} \log p(x,\theta)
                                -  \mathbb{E}_{q_j(\theta_j)} \log q_j(\theta_j) + C \\
                                &amp;amp;= \{r_j(\theta_j)=\frac{1}{Z_j}\exp(\mathbb{E}_{q_{i\neq j}} \log p(x,\theta))\}\\
                                &amp;amp;= \mathbb{E}_{q_j(\theta_j)} \log r_j(\theta_j)
                                   -  \mathbb{E}_{q_j(\theta_j)} \log q_j(\theta_j) + C \\
                                &amp;amp;= - KL(q_j(\theta_j)\parallel r_j(\theta_j)) + C
  \end{align*}\]

&lt;p&gt;So, the optimization problem for step \(j\) is:&lt;/p&gt;

\[\max_{q_j(\theta_j)}\mathcal{L}(q(\theta)) = \max_{q_j(\theta_j)} - KL(q_j(\theta_j)\parallel r_j(\theta_j)) + C\]

&lt;p&gt;Where this happens when:&lt;/p&gt;

\[q_j(\theta_j) = r_j(\theta_j) = \frac{1}{Z_j}\exp(\mathbb{E}_{q_{i\neq j}} \log p(x,\theta))\]

\[\log q_j(\theta_j) = \mathbb{E}_{q_{i\neq j}} \log p(x,\theta) + C\]

&lt;p&gt;Block coordinate assent can be described in two steps 1) initialize; 2) iterate&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Initialize: \(q(\theta)=\prod_{j=1}^m q_j(\theta_j)\)&lt;/li&gt;
  &lt;li&gt;Iterate (repeat until ELBO converge):
    &lt;ul&gt;
      &lt;li&gt;Update each factor \(q_1,\dots,q_m\): \(q_j(\theta_j)=\frac{1}{Z_j}\exp(\mathbb{E}_{q_{i\neq j}} \log p(x,\theta))\)&lt;/li&gt;
      &lt;li&gt;Compute ELBO \(\mathcal{L}(q(\theta))\)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Mean-field can be applied when we can compute analytically \(\mathbb{E}_{q_{i\neq j}} \log p(x,\theta)\). In other words, applicable when we can compute the conditional conjugacy.&lt;/p&gt;

&lt;h4 id=&quot;parametric-approximation&quot;&gt;Parametric Approximation&lt;/h4&gt;
&lt;p&gt;Select a parametric family of variational distributions, \(q(\theta)=q(\theta\mid\lambda)\), where \(\lambda\) is a variational parameter.&lt;/p&gt;

&lt;p&gt;The restriction is that we need to select a family of some fixed form, and as a result:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;it might be too simple and insufficient to model the data&lt;/li&gt;
  &lt;li&gt;if it is complex enough then there is no guarantee we can train it well to fit the data&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The ELBO is:&lt;/p&gt;

\[\max_{\lambda}\mathcal{L}(q\theta\mid\lambda)=\int q(\theta\mid\lambda)\log\frac{p(x\mid\theta)}{q(\theta\mid\lambda)}d\theta\]

&lt;p&gt;If we’re able to calculate derivatives of ELBO w.r.t \(\theta\), then we can solve this problem using some numerical optimization solver.&lt;/p&gt;

&lt;h3 id=&quot;inference-methods&quot;&gt;Inference methods&lt;/h3&gt;
&lt;p&gt;So we have:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Full Bayesian inference: \(p(\theta\mid x)\)&lt;/li&gt;
  &lt;li&gt;MAP inference: \(p(\theta\mid x)\approx \delta (\theta-\theta_{MP})\)&lt;/li&gt;
  &lt;li&gt;Mean field variational inference: \(p(\theta\mid x)\approx q(\theta)=\prod_{j=1}^m q_j(\theta_j)\)&lt;/li&gt;
  &lt;li&gt;Parametric variational inference: \(p(\theta\mid x)\approx q(\theta)=q(\theta\mid\lambda)\)&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;latent-variable-model&quot;&gt;Latent variable model&lt;/h1&gt;
&lt;h2 id=&quot;mixture-of-gaussians&quot;&gt;Mixture of Gaussians&lt;/h2&gt;
&lt;p&gt;Establish a latent variable \(z_i\) for each data point \(x_i\) that denotes the \(ith\) gaussian where the model was generated.&lt;/p&gt;

&lt;p&gt;Model:&lt;/p&gt;

\[\begin{align*}
    p(X,Z\mid\theta) &amp;amp;= \prod_i^n p(x_i,z_i\mid\theta)\\
                     &amp;amp;= \prod_i^n p(x_i\mid z_i,\theta)p(z_i\mid\theta)\\
                     &amp;amp;= \prod_i^n \pi_{z_i}\mathcal{N}(x_i\mid\mu_{z_i},\sigma_{z_i}^2)
  \end{align*}\]

&lt;p&gt;where \(\pi_j=p(z_i=j)\) is the prior of the \(jth\) gaussian and \(\theta=\{\mu_j,\sigma^2_j,\pi_j\}_{j=1}^K\) are the parameters to estimate.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; If \(X\) and \(Z\) are known, we can use ML. For instance:&lt;/p&gt;

\[\begin{align*}
    \theta_{ML}&amp;amp;=\operatorname*{arg\,max}_{\theta} p(X,Z\mid\theta)\\
               &amp;amp;=\operatorname*{arg\,max}_{\theta} \log p(X,Z\mid\theta)
  \end{align*}\]

&lt;ul&gt;
  &lt;li&gt;Since \(Z\) is a latent variable, we need to maximize the log of incomplete likelihood w.r.t. \(\theta\).&lt;/li&gt;
  &lt;li&gt;Instead of optimizing \(\log p(X\mid \theta)\), we optimize the variational lower bound w.r.t. to both \(\theta\) and \(q(Z)\)&lt;/li&gt;
  &lt;li&gt;This can be solved by block-coordinate algorithm a.k.a. EM-algorithm.&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Variational Lower Bound:&lt;/strong&gt; \(g(\xi,\theta)\) is the variational lower bound function for \(f(x)\) iff:&lt;/p&gt;
  &lt;ol&gt;
    &lt;li&gt;For all \(\xi\) for all \(x\): \(f(x)\geq g(\xi,x)\)&lt;/li&gt;
    &lt;li&gt;For any \(x_0\) exists \(\xi(x_0)\) such that: \(f(x_0)=g(\xi(x0),x_0)\)&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;If we find such variational lower bound, instead of solving
\(f(x)\rightarrow\max_x\), we can interatively perform block coordinate updates of \(g(\xi, x)\).&lt;/p&gt;
  &lt;ol&gt;
    &lt;li&gt;
\[x_n=\operatorname*{arg\,max}_{x}g(\xi_{n-1},x)\]
    &lt;/li&gt;
    &lt;li&gt;
\[\xi_n=\xi(x_n)=\operatorname*{arg\,max}_{\xi} g(\xi,x_n)\]
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Expectation Maximization algorithm&lt;/strong&gt;
We want to solve:&lt;/p&gt;

\[\operatorname*{arg\,max}_{q,\theta}\mathcal{L}(q, \theta) = \operatorname*{arg\,max}_{q,\theta}\int q(Z)\frac{p(X,Z\mid\theta)}{q(Z)}dZ\]

&lt;p&gt;&lt;strong&gt;Algorithm&lt;/strong&gt;:
Set an initial point \(\theta_0\)&lt;/p&gt;

&lt;p&gt;Repeat iteratively 1 and 2 until convergence&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;E-step, find:
\(\begin{align*}
 q(Z)&amp;amp;=\operatorname*{arg\,max}_{q}\mathcal{L}(q,\theta_0)\\
    &amp;amp;=\operatorname*{arg\,max}_{q}{KL}(q\parallel p)\\
    &amp;amp;=p(Z\mid X,\theta_0)
  \end{align*}\)&lt;/li&gt;
  &lt;li&gt;M-step, solve:
\(\begin{align*}
 \theta_*&amp;amp;=\operatorname*{arg\,max}_{\theta}\mathcal{L}(q,\theta)\\
    &amp;amp;=\operatorname*{arg\,max}_{\theta}\mathbb{E}_Z \log p(X,Z\mid\theta)
  \end{align*}\)
    &lt;ul&gt;
      &lt;li&gt;Set \(\theta_0=\theta_*\) and go to 1&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;EM monotonically increases the lower bound and converges to a stationary point of \(\log p(X\mid\theta)\), see figure.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/em/em_optimize.gif&quot; alt=&quot;EM algorithm&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Benefits of EM&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;In some cases E-step and M-step can be solved in closed-information&lt;/li&gt;
  &lt;li&gt;Allow to build more complicated models&lt;/li&gt;
  &lt;li&gt;If true posterior \(p(Z\mid X,\theta)\) is intractable, we may search for the closest \(q(Z)\) &lt;em&gt;among tractable distributions&lt;/em&gt; by solving optimization problem&lt;/li&gt;
  &lt;li&gt;Allows to process missing data by treating them as latent variables
    &lt;ul&gt;
      &lt;li&gt;It can deal with both discrete and latent variables&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Categorical latent variables&lt;/strong&gt;
Since \(z_i\in\{1,\dots,K\}\) the marginal of a mixture of gaussians is a finite mixture of distributions:&lt;/p&gt;

\[p(x_i\mid\theta)=\sum_{k=1}^Kp(x_i\mid k,\theta)p(z_i=k\mid\theta)\]

&lt;ul&gt;
  &lt;li&gt;E-step is closed-form: \(q(z_i=k)=p(z_i=k\mid x_i,\theta)=\frac{p(x_i\mid k,\theta)p(z_i=k\mid\theta)}{\sum_{l=1}^Kp(x_i\mid l,\theta)p(z_i=l\mid\theta)}\)&lt;/li&gt;
  &lt;li&gt;M-step is a sum of finite terms: \(\mathbb{E}_Z\log p(X,Z\mid\theta)=\sum_{i=1}^n\mathbb{E}_{z_i}\log p(x_i,z_i\mid\theta)=\sum_{i=1}^n\sum_{k=1}^K q(z_i=k)\log p(x_i,k\mid\theta)\)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Continuous latent variables&lt;/strong&gt;
A mixture of continuous distributions&lt;/p&gt;

\[p(x_i\mid\theta)=\int p(x_i\mid z_i,\theta)p(z_i\mid\theta) dz_i\]

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;E-step: only done in closed form when &lt;strong&gt;conjugate distributions&lt;/strong&gt;, otherwise the true posterior is intractable&lt;/p&gt;

\[q(z_i)=p(z_i\mid x_i,\theta)=\frac{p(x_i\mid z_i,\theta)p(z_i\mid\theta)}{\int p(x_i\mid z_i,\theta)p(z_i\mid\theta) dz_i}\]
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Typically continuous latent variable are used for dimensionality reduction a.k.a. &lt;em&gt;representation learning&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;log-derivative-trick&quot;&gt;Log-derivative trick&lt;/h1&gt;

\[\frac{\partial}{\partial x}p(y\mid x)=p(y\mid x)\frac{\partial}{\partial x}\log p(y\mid x)\]

&lt;p&gt;For example, we commonly find expressions as follows:&lt;/p&gt;

\[\begin{align*}
    \frac{\partial}{\partial x}\int p(y\mid x)h(x,y)dy &amp;amp;= \int \frac{\partial}{\partial x} p(y\mid x)h(x,y)dy\\
                                  &amp;amp;= \int \left(h(x,y)\frac{\partial}{\partial x} p(y\mid x) + p(y\mid x)\frac{\partial}{\partial x} h(x,y) \right)dy \\
                                  &amp;amp;= \int p(y\mid x)\frac{\partial}{\partial x} h(x,y) dy + \int h(x,y)\frac{\partial}{\partial x} p(y\mid x)dy \\
                                  &amp;amp;= \int p(y\mid x)\frac{\partial}{\partial x} h(x,y) dy + \int p(y\mid x)h(x,y)\frac{\partial}{\partial x} \log p(y\mid x)dy
  \end{align*}\]

&lt;p&gt;Now, the first term can be replaced with Monte Carlo estimate of expectation. Using the log-derivative trick, the second expectation can also be estimated via Monte Carlo.&lt;/p&gt;

&lt;h1 id=&quot;score-function&quot;&gt;Score function&lt;/h1&gt;

&lt;p&gt;It is the gradient of the log-likelihood function with respect to the parameter vector. Since it has zero mean, the value \(z_i^*\) in \(\nabla_{\phi}\log q(z_i^*\mid\theta)\) oscillates around zero.&lt;/p&gt;

\[\begin{align*}
    \nabla_{\phi}\log q(z_i\mid\theta)
  \end{align*}\]

&lt;p&gt;Proof it has zero mean:&lt;/p&gt;

\[\begin{align*}
  \int q(z_i\mid\theta)\nabla_{\phi}\log q(z_i\mid\theta) d z_i&amp;amp;=\int\frac{q(z_i\mid\theta)}{q(z_i\mid\theta)}\nabla_{\phi}q(z_i\mid\theta)d z_i\\
                                                              &amp;amp;= \nabla_{\phi}\int q(z_i\mid\theta)dz_i\\
                                                              &amp;amp;= \nabla_{\phi} 1 =0
\end{align*}\]

&lt;h1 id=&quot;reinforce&quot;&gt;REINFORCE&lt;/h1&gt;</content><author><name></name></author><category term="Bayesian-inference" /><summary type="html">Some tools: Stochastic variational inference Variance reduction Normalizing flows Gaussian processes Scalable MCMC algorithms Semi-implicit variational inference</summary></entry><entry><title type="html">Expectation Propagation Notes</title><link href="http://localhost:4000/blog/2019/expectation-propagation/" rel="alternate" type="text/html" title="Expectation Propagation Notes" /><published>2019-07-25T21:41:19-04:00</published><updated>2019-07-25T21:41:19-04:00</updated><id>http://localhost:4000/blog/2019/expectation-propagation</id><content type="html" xml:base="http://localhost:4000/blog/2019/expectation-propagation/">Commonly in probabilistic models, we deal with expectations that are either too hard to compute or intractable. In Bayesian inference setting, we usually need to calculate the posterior $$p(z\mid x)$$ for parameter estimation or the evidence $$p(x)$$ for model selection, where $$z$$ is a latent variable and $$x$$ is known. We can use approximate inference to solve this complex integrals. There is many work that have been done in this area, but approximate methods can be classified in deterministic and sampling methods. The former evaluates the integral in several locations and constructs an approximate function. The latter relies in the law of large numbers and given enough samples, the integral will converge to the true value.

1. Sampling methods: Monte Carlo methods such as Importance sampling, Gibbs sampling, MCMC
2. Deterministic methods: Variational inference, Laplace approximation, Expectation Propagation

This notes include explanations for ADF and Expectation Propagation.

## Assumed-density Filtering (ADF)

ADF (aka. moment matching, online Bayesian learning and weak marginalization) can be used when given the joint $$p(x, z)$$, we want to calculate the posterior over the latent variable $$p(z \mid x)$$ and the evidence $$p(x)$$. This is a common task in statistical machine learning where we want to fit a parametric distribution.

**The common setting**: we are given a set of data points $$D=x_1,\dots,x_n$$, which we assume are i.i.d. Thus, we can model the joint distribution as the product of its observations and the prior $$p(D,z) = p(z)\prod_i p(x_i\mid z)$$.

1. **Factorization**: We can factorize the joint distribution as needed, but it is recommendable that **each factor is simple** enough for being propagated. Also the **fewer terms**, entails fewer approximations. In general, we can define the joint distribution as follows where $$t_0$$ is the prior.

    $$p(D,z) = \prod_{i=0}^n t_i(z)$$

2. **Choose a parametric approximating distribution**: our goal is to approximate the posterior with a simple distribution; $$p(z\mid D) \approx q(z)$$.

    - This distribution has to belong to the exponential family, so we can propagate its sufficient statistics.
    - Pick $$q$$ based in the nature and domain of $$z$$.

3. **Incorporate each $$t_i$$ term**: Sequence and incorporate the term $$t_i$$ into the approximate posterior $$q(z$$) going from an old posterior to a new posterior.

    - __incorporate__: $$\hat p(z) = \frac{q(z)t_i(z)}{\int_z q(z)t_i(z) dz}$$
    - __from old to new__: $$\min D_{KL}(\hat p \parallel q^{new})$$, which equivalent to match the moments.

### Intuition
  Here is a graphical example of ADF in practice.
  1. $$q(z)$$ can be initialized to $$1$$, and it&apos;s not necessary to approximate.

  ![ADF step 1](/assets/img/adf/adf_step1.png)

  2. Incorporate $$t_1$$, resulting in a one-step step posterior $$\hat p$$. We approximate $$\hat p$$ with $$q^{new}$$.

  ![ADF step 2](/assets/img/adf/adf_step2.png)

  3. Repeat step 2, including $$t_2$$.

  ![ADF step 3](/assets/img/adf/adf_step3.png)

  We can notice that there&apos;s a dependence in the order of the data points; **order matters**.

  **The error of ADF tend to increase when similar data points are processed**

## Expectation Propagation
Expectation Propagation ([Minka][1]., NIPS 2001) is a generalization of ADF. We can notice that ADF is an iterative method that performs a one-pass to all data points. EP solves this problem by performing iterative refinements.

ADF:

1. Treat $$t_i$$ as it is.
2. Approximate $$q(z)$$.

EP:

1. Approximate $$t_i$$ with $$\tilde t_i$$.
2. Use exact posterior

This refinements are always possible. It is the ratio of the new posterior to the old posterior times a constant:

  $$\tilde t_i(z)=Z\frac{q^{new}(z)}{q(z)}$$

Let&apos;s see how this looks in practice:

First, let&apos;s derive an intuition of ADF using $$\tilde t_i$$.

1. We approximate $$t_1$$ with $$\tilde t_1$$ using the prior $$t_0=1$$m and go from $$q^{old}$$ to $$q^{new}$$.

![ADF approx step 1](/assets/img/adf/adf_approx_step1.png)

2.

![ADF approx step 2](/assets/img/adf/adf_approx_step2.png)


[1]: https://arxiv.org/pdf/1301.2294.pdf</content><author><name></name></author><category term="Bayesian-inference" /><summary type="html">Commonly in probabilistic models, we deal with expectations that are either too hard to compute or intractable. In Bayesian inference setting, we usually need to calculate the posterior \(p(z\mid x)\) for parameter estimation or the evidence \(p(x)\) for model selection, where \(z\) is a latent variable and \(x\) is known. We can use approximate inference to solve this complex integrals. There is many work that have been done in this area, but approximate methods can be classified in deterministic and sampling methods. The former evaluates the integral in several locations and constructs an approximate function. The latter relies in the law of large numbers and given enough samples, the integral will converge to the true value.</summary></entry><entry><title type="html">Sampling methods notes</title><link href="http://localhost:4000/blog/2019/sampling-methods/" rel="alternate" type="text/html" title="Sampling methods notes" /><published>2019-05-17T21:41:19-04:00</published><updated>2019-05-17T21:41:19-04:00</updated><id>http://localhost:4000/blog/2019/sampling-methods</id><content type="html" xml:base="http://localhost:4000/blog/2019/sampling-methods/"># Monte Carlo Methods

Randomized algorithms have two categories: Las Vegas and Monte Carlo algorithms.

1. *Las Vegas:* always return an answer if available.
2. *Monte Carlo*: return answers with a random amount of error.

## Monte Carlo sampling
Probability distributions are commonly used in Machine Learning. Sampling provides a flexible way to approximate many sums and integrals. When a sum or integral cannot be computed, it is possible to approximate with **Monte Carlo sampling**. The main idea is to see the sum or integral as an expectation, and approximate it by an avearge.

$$s=\sum_x p(x)f(x)=\mathbb{E}_p[f(\mathbf{x})]$$

$$s=\int p(x)f(x) dx=\mathbb{E}_p[f(\mathbf{x})]$$

, where $$p$$ is a PMF or PDF over the random variable $$\mathbf{x}$$.  

$$s$$ can be approximated by drawing $$n$$ samples $$x^{(1)}, \dots, x^{(n)}$$ from $$p$$ and calculate the **empirical average** as follows:

$$\hat s_n=\frac{1}{n}\sum_{i=1}^n f(x^{(i)})$$

Some properties:
- Estimator $$\hat s$$ is unbiased.
- By the *[law of large numbers](https://en.wikipedia.org/wiki/Law_of_large_numbers)*, we say that if $$x^{(i)}$$ are i.i.d. the average converges to the expected values ($$\lim_{n\to\infty} \hat s_n=s$$).
- By calculating the variance $$Var[\hat s_n]$$ as $$n$$ increases, we can estimate the uncertainty of the Monte Carlo approximation.
- MC sampling relies in sampling from $$p$$. When *sampling* $$p$$ *is not possible*, importance sampling or MCMC can be used.


## Importance Sampling
In Monte Carlo, we can use any decomposition of $$p(\mathbf{x})$$ and $$f(\mathbf{x})$$ since $$f$$ can also be a probability (what factor plays which role of $$p$$ or $$f$$). We can even have a different decomposition as follows (i.e. we sample $$q$$ and average $$\frac{pf}{q})$$:

$$p(\mathbf{x})f(\mathbf{x})=q(\mathbf{x})\frac{p(\mathbf{x})f(\mathbf{x})}{q(\mathbf{x})}$$

However, we might not be able to sample from $$p$$ or we can pick another distribution to reach an optimal approximation. This optimal choice is $$q*$$, know as **optimal importance sampling**. So, we replace the empirical average with an **importance sampling estimator**:

$$\hat s_q=\frac{1}{n}\sum_{i=1, \mathbf{x}^{(i)}\sim q}^n \frac{p(\mathbf{x}^{(i)})f(\mathbf{x}^{(i)})}{q(\mathbf{x}^{(i)})}$$

Essentially, any distribution $$q$$ is valid. However, the choice of $$q$$ can be sensitive to the variance of the _importance sampling estimator_.

####  Biased Importance Sampling
BIS doesn&apos;t require to normalize $$p$$ or $$q$$, the estimator is given by:

$$
\hat s_{BIS}=\frac{\sum_{i=1}^n\frac{\tilde p(\mathbf{x^{(i)}})}{\tilde q(\mathbf{x^{(i)}})}f(\mathbf{x^{(i)}})}{\sum_{i=1}^n\frac{\tilde p(\mathbf{x^{(i)}})}{\tilde q(\mathbf{x^{(i)}})}}
$$

, where $$\tilde p$$ and $$\tilde q$$ are the unnormalized forms of $$p$$ and $$q$$. We take n samples $$\mathbf{x^{(i)}}$$ from $$q$$.

In general importance sampling is handy when it comes to:
- accelerate training in neural language models with a large vocabulary.
- estimate a partition function.
- estimate log-likelihood in deep directed models.
- improve estimates of the gradient of the cost function.

&lt;!-- 
## Markov Chain Monte Carlo

## Gibbs Sampling --&gt;</content><author><name></name></author><category term="Bayesian-inference" /><summary type="html">Monte Carlo Methods</summary></entry><entry><title type="html">Probabilistic Graphical Models</title><link href="http://localhost:4000/blog/2019/pgm-intro/" rel="alternate" type="text/html" title="Probabilistic Graphical Models" /><published>2019-05-17T21:41:19-04:00</published><updated>2019-05-17T21:41:19-04:00</updated><id>http://localhost:4000/blog/2019/pgm-intro</id><content type="html" xml:base="http://localhost:4000/blog/2019/pgm-intro/">A graphical model is a method for modeling probability distributions under certain uncertainty.

## Toolbox:

1. **Representation:** model uncertainty and encode domain knowledge
2. **Inference:** answer questions $$P(X\mid m)$$, where m is the model or data
3. **Learning:** what model fits my data $$m = \operatorname*{argmax}_{m\in M} F(D,m)$$.

## Benefits:

1. Efficient

    - **(Expensive)** The [chain rule][1] (aka product rule) allows to calculate joint probabilities.
    - **(Cheaper)** Using GM, we can model only those dependencies inferred by the graph, generating fewer parameters; encodes independence.

2. Encode domain knowledge through priors and incorporate them in inference via Bayes theorem.

## GMs vs PGMs:

- GMs use multivariate function.
- PGMs use multivariate distributions.

  ### Structure

  1. Edges represent relationship among the RVs.
  2. Directed nodes represent **causality** while undirected nodes represent **correlation**.

## Bayesian Network and Markov Random Field

### Bayesian Network

![Bayesian Network](/assets/img/bayesian-network.png)

- It is a directed acyclic graph (DAG) where each node has a [Markov blanked][2] (its parents, children and children&apos;s parents).
- A node is _conditionally independent_ of the nodes **outside** its Markov Blanket.
- _Joint probability distribution_ is determined by the local conditional probabilities as well as the graph structure.
- Model _can_ be used to generate new data.

### Markov Random Field

![Markov Random Field](/assets/img/markov-random-field.png)

- It is an undirected graph.
- A node is conditionally independent of the other graph nodes, except for its **immediate neighbors**.
- To determine the joint probability distribution, we need to know local contingency functions (_potentials_) as well as structural _cliques_.
- This model _cannot_ explicitly generate new data.

[1]: https://en.wikipedia.org/wiki/Chain_rule_(probability)
[2]: https://en.wikipedia.org/wiki/Markov_blanket</content><author><name></name></author><category term="Bayesian-inference" /><summary type="html">A graphical model is a method for modeling probability distributions under certain uncertainty.</summary></entry><entry><title type="html">CS224W notes - Machine learning with graphs</title><link href="http://localhost:4000/blog/2019/compcs224w-notes/" rel="alternate" type="text/html" title="CS224W notes - Machine learning with graphs" /><published>2019-05-17T21:41:19-04:00</published><updated>2019-05-17T21:41:19-04:00</updated><id>http://localhost:4000/blog/2019/compcs224w-notes</id><content type="html" xml:base="http://localhost:4000/blog/2019/compcs224w-notes/">## [Introduction and graph structure][1]

### Why graphs?
Networks are general languages for describing complex systems of interacting entities. In other words, a network is a universal language for describing complex data.

Network structure affects the robustness of a system

Most real networks are sparse

Tasks to perform in graphs:
- node classification
- link prediction
- community detection
- network similarity

**Embedding nodes:** map nodes to $$d$$-dimensional embeddings. Thus, nodes wit similar network neighbourhoods are embedded close together.

Terminology:
- Networks refer to a real system, e.g. Social Networks, Web.
- Graph is a mathematical representation of a network e.g. graph, vertex, edge

Graphs can be directed (followers on twitter) or undirected (friends on Facebook)

Node degrees:
  Undirected:
  - node degree: the number of edges adjacent to node i
  - avg. degree

  Directed:
  - in-degree
  - out-degree
  - the total degree of a node is the sum of in and out-degrees

Complete Graph
- maximum number of edges in an undirected graph on $N$ nodes is $$E_max=\frac{N(N-1)}{2}$$ and when the number of edges is $E=E_max$, the undirected graph is called compete graph.

Bipartite Graph
- It is a graph whose nodes can be divided into two disjoints sets $U$ and $V$, which are independent sets.

Representing Graphs
  Adjacency matrix
  - $$A_{ij}=1$$ if there is a link from node $$i$$ to $$j$$
  - $$A_{ij}=0$$ otherwise
  - they are sparse

  Edge List
  - graph as a set of edges

  Adjacency list
  - each row is a node and its line is the connected nodes
    1:
    2: 3,4
    3: 2,4
    4: 5
  - Easier to work with if network is large or sparse

Edge attributes
 - weight (frequency of communication)
 - raking (best friend, 2nd best friend)
 - type (friend, relative)
 - Sign (Friend vs Foe)

Other types of graphs:
  - self-edges: contain self-loops (proteins, hyperlinks)
  - multigraph: more than one edge between nodes (communication, collaboration)

Connectivity:
  Disconnected graph
  - a graph can become a disconnected graph, if the a subgraph becomes isolated. Thus it has more than one component...
  - bridge edge: if we delete an edge,
  - articulation node: if we erase a node
  - Each component can be represented in a block-diagonal form

  Strongly connected directed graph
  - has a path from each node to every other node and vice-versa.

  Weakly connected directed graph
  - is connected if we disregard the edge directions

## [Properties of networks and random graph models][2]

### How to Measure a Network

- Degree distribution $$p(k)$$: probability that a randomly node has degree $$k$$.

$$N_k\rightarrow$$: number of nodes with degree $$k$$

$$p(k)=\frac{N_k}{N}$$

**Note:** directed graphs have separate in- and out-degree distributions

- Path: sequence of nodes in which each node is linked to the next one.

$$P_n=\{i_0,\dots,i_n\}$$

$$P_n=\{(i_0,i_1),\dots,(i_{n-1},i_n)\}$$

- Distance between a pair of nodes $$h_{A,B}$$: number of edges along the shortest path connecting the nodes.

If node are not connected, distance is defined as *zero* or *infinity*

Directed graphs, paths need to follow the direction of the arrows, and as a consequence, distance is not symmetric $$h_{B,C}\neq h_{c,B}$$

- Diameter: shortest path distance between any pair of nodes in a graph

- Average path length $$\bar h$$:

$$\bar h= \frac{1}{2E_{max}}\sum_{i,j\neq i}h_{ij}$$

$$E_{max}$$: max number of edges (total number of node pairs) $$=\frac{n(n-1)}{2}$$

- Clustering Coefficient
 



[1]: (http://web.stanford.edu/class/cs224w/slides/01-intro.pdf)
[2]: (http://web.stanford.edu/class/cs224w/slides/02-gnp-smallworld.pdf)</content><author><name></name></author><category term="GRL" /><summary type="html">Introduction and graph structure</summary></entry><entry><title type="html">Linear Algebra</title><link href="http://localhost:4000/blog/2019/linear_algebra/" rel="alternate" type="text/html" title="Linear Algebra" /><published>2019-05-15T22:41:19-04:00</published><updated>2019-05-15T22:41:19-04:00</updated><id>http://localhost:4000/blog/2019/linear_algebra</id><content type="html" xml:base="http://localhost:4000/blog/2019/linear_algebra/">Some interesting links:
- [Matrix cookbook][2]
- [Matrix derivatives][3]

Notes of [chapter 2][1] of Deep Learning book.

#### Mathematical objects

1. **Scalar:** single number $$x=1$$
2. **Vector:** array of numbers $$\mathbf{x}=\begin{vmatrix}x_1\\\vdots\\x_n\end{vmatrix}$$

    $$\mathbf{x_{-1}}$$: ignore element $$x_1$$
3. **Matrix:** 2D array of numbers $$\mathbf{A}\in\mathbb{R}^{mxn}$$

    horizontal coordinate (all), ith row: $$\mathbf{A}_{i,:}$$

    vertical coordinate (all), ith column: $$\mathbf{A}_{:,i}$$

    value: $$f(\mathbf{A})_{i,j}$$
4. **Tensor:** array with more than two axes $$\mathbf{A}$$

    value: $$\mathbf{A}_{i,j,k}$$

#### Transpose
  - Mirror across _main diagonal_ $$\mathbf{A} \rightarrow \mathbf{A}^\intercal$$
  - Scalar matrix, one item $$\mathbf{a}=\mathbf{a}^\intercal$$
  - Vector $$\mathbf{x}^\intercal$$
  - Matrix product $$(\mathbf{A}\mathbf{B})^\intercal=\mathbf{B}^\intercal\mathbf{A}^\intercal$$

#### Addition
   If $$\mathbf{A}$$ and $$\mathbf{B}$$ same shape, $$\mathbf{C}=\mathbf{A}+\mathbf{B}$$, where $$\mathbf{C}_{i,j}=\mathbf{A}_{i,j}+\mathbf{B}_{i,j}$$.

#### Scalar addition and multiplication
  $$\mathbf{D}=a\cdot\mathbf{B}+c$$, where $$\mathbf{D}_{i,j}=a\cdot\mathbf{B}_{i,j}+c$$

#### Addition matrix and vector
  $$\mathbf{C}=\mathbf{A}+\mathbf{b}$$, where $$\mathbf{C}_{i,j}=\mathbf{A}_{i,j}+\mathbf{b}_j$$

  **broadcasting**: $$\mathbf{b}$$ is addded to each **row** of the matrix $$\mathbf{A}$$.

#### Operations
  1. Matrix product $$\mathbf{C}=\mathbf{A}\mathbf{B} \rightarrow \mathbf{C}_{m\times p}=\mathbf{A}_{m\times n}\mathbf{B}_{n\times p}$$ and $$\mathbf{C}_{i,j}=\sum_k\mathbf{A}_{i,k}\mathbf{B}_{k,j}$$

      Vectors of the same size: $$\mathbf{x}^\intercal\mathbf{y}$$
  2. Elementwise product / Hadamard product $$\mathbf{A}\odot\mathbf{B}$$

#### Properties
  - Distributive: $$\mathbf{A}(\mathbf{B}+\mathbf{C})=\mathbf{A}\mathbf{B}+\mathbf{A}\mathbf{C}$$  
  - Associative: $$\mathbf{A}(\mathbf{B}\mathbf{C})=(\mathbf{A}\mathbf{B})\mathbf{C}$$
  - Commutative: **not** $$\mathbf{A}\mathbf{B}\neq \mathbf{B}\mathbf{A}$$ (not always)

    **yes** for vectors $$\rightarrow$$ $$\mathbf{x}^\intercal \mathbf{y}=\mathbf{y}^\intercal\mathbf{x}$$

#### Linear equations

$$\mathbf{A}\mathbf{x}=\mathbf{b}$$ (notation compact)
 - $$\mathbf{A}\in\mathbb{R}^{m\times n}$$ known
 - $$\mathbf{x}\in\mathbb{R}^m$$ variable
 - $$\mathbf{b}\in\mathbb{R}^m$$ known

 Notation not compact, equations:

 $$A_{1,:}\mathbf{x}=b_1 \rightarrow A_{1,1}x_1+\dots+A_{1,n}x_n=b_1\\
 \vdots\\
 A_{m,:}\mathbf{x}=b_m \rightarrow A_{m,1}x_1+\dots+A_{m,n}x_n=b_m$$

#### Identity matrix
$$\mathbf{I}_n\in\mathbb{R}^{nxn} \rightarrow \mathbf{I}_n\mathbf{x}=\mathbf{x}\:\:\:\:\:\:\: \forall_{\mathbf{x}}\in\mathbb{R}^n$$

#### Inverse Matrix
$$A^{-1}A=\mathbf{I}_n$$, $$AA^{-1}=\mathbf{I}_n$$, for square matrix left and right are the same.

For example:
$$
\begin{align}
A\mathbf{x}&amp;=\mathbf{b} \\
A^{-1}A\mathbf{x}&amp;=A^{-1}\mathbf{b} \\
\mathbf{I}_n\mathbf{x}&amp;=A^{-1}\mathbf{b}\\
\mathbf{x}&amp;=A^{-1}\mathbf{b}
\end{align}
$$

- Should **not** be used in practical applications because **limited precision**
- $$A^{-1}$$ might not be possible to find (singular matrix).

**Note:** Matrix should be **square and singular**. If not, we cannot get $$A^{-1}$$.
#### Linear Combination
$${\mathbf{v}^{(1)},\dots,\mathbf{v}^{(n)}}$$ Given by multiplying each vector $$\mathbf{v}^{(i)}$$ by a scalar and adding.

$$\sum_i c_i\mathbf{v}^{(i)}$$

##### Span
Set of all points obtained by linear combination of the original vectors

$$A\mathbf{x}=\mathbf{b} \rightarrow$$ solution if $$\mathbf{b}$$ is in the **span** (known as __column space__ / __range__ of $$A$$) of columns $$A$$

$$A$$ square matrix $$m=n$$ and all columns linear independent.

#### Linear Dependence
Same column space / a combination
#### Linear Independence
No vector in the set is a **linear combination** of the other vectors.
#### Square matrix
$$A \in\mathbb{R}^{m\times n} m=n$$
#### Singular matrix
Matrix with linear independent columns
#### Norms
- Norms are functions mapping vectors to non-negative values (distance from origin to point $$\mathbf{x}$$)
- Measure the size of a vector
- Any function $$f$$ is a norma as long as:
  1. $$f(\mathbf{x})=0 \rightarrow \mathbf{x}=0$$.
  2. $$f(\mathbf{x} + \mathbf{y}) \leq f(\mathbf{x}) + f(\mathbf{y})$$ triangle inequality
  3. $$\forall \alpha\in\mathbb{R}, f(\alpha\mathbf{x}) = \mid\alpha\mid f(\mathbf{x})$$.


##### List:
- **$$L^p$$ norm**: $$\mid\mid x\mid\mid_p=(\sum_i\mid x_i\mid)^{\frac{1}{p}}$$ where $$p\in\mathbb{R}, p\geq 1$$
- **$$L^2$$ Euclidean norm (distance)**: $$\parallel\mathbf{x}\parallel_2=\parallel\mathbf{x}\parallel=\mathbf{x}^\intercal\mathbf{x}$$
  - increases really slowly near the original
  - squared $$L^2$$ norm is better to work: 1) mathematically and 2) computationally.
- **$$L^1$$ norm**: $$\parallel\mathbf{x}\parallel_1=\sum_i\mid x_i\mid$$
  - chooses a function that grows at the **same rate** in all locations.
  - Discriminate elements that are 1) exactly zero; 2) small, but not zero.
- **$$L^\infty$$ max norm**: $$\parallel\mathbf{x}\parallel_\infty=\max_i\mid x_i\mid$$
- **Frobenius norm**: *measure size of matrix*
  - Analogous to $$L^2$$ of a vector
  - Dot product in terms of norms $$\mathbf{x}^\intercal\mathbf{y}=\parallel\mathbf{x}\parallel_2\parallel\mathbf{y}\parallel_2\cos\theta$$

  $$\parallel A\parallel_F=\sqrt{\sum_{ij}A_{ij}^2}$$
### Special kind of matrices and Vectors
- Diagonal matrix ($$D$$)
  - $$diag(\mathbf{v})$$: square
  - $$diag(\mathbf{v})^{-1}=diag(\left[1/v_1,\dots,1/v_n\right]^\intercal)$$.
  - $$diag(\mathbf{v})\mathbf{x}=\mathbf{v}\odot\mathbf{x} \rightarrow x_i*v_i$$.
  - non-square: **no inverse**
- Symmetric $$A=A^\intercal$$
- Unit vector (unit norm): $$\parallel\mathbf{x}\parallel_2=1$$
- Orthogonal: $$\mathbf{x}^\intercal\mathbf{y}=0$$ Vectors are perpendicular (90 degrees)
  - mutually orthogonal: Set of vectors are orthogonal
  - **orthonormal**: vector is **orthogonal** and have **unit norm**
  - **orthogonal matrix**: square matrix, rows and columns are mutually orthonormal

    $$
      \begin{align}
        A^\intercal A&amp;=AA^\intercal=I\\
        A^{-1}&amp;=A^\intercal
      \end{align}
    $$

### Eigendecomposition
#### Matrix Decomposition
  1. Eigenvectors ($$\mathbf{v}$$ vector)
  2. Eigenvalues ($$\lambda$$ scalar)

  $$A\mathbf{v}=\lambda\mathbf{v}$$

#### Eigendecomposition
  1. $$\mathbf{v}^{-1}$$ ,linear independent eigenvectors
  2. $$\mathbf{\lambda}$$, corresponding eigenvalue

  $$A=\mathbf{v}diag(\mathbf{\lambda})\mathbf{v}^{-1}$$

  **Not every matrix can be decomposed**

#### Real Symmetric matrix
  1. $$Q$$, orthogonal matrix
  2. $$\Lambda$$, diagonal matrix

  $$A=Q\Lambda Q^\intercal$$

  **Not defined for non-squared matrices**

### Singular Value Decomposition (SVD)
  It&apos;s more generally applicable

  1. $$D$$, diagonal matrix (singular values=$$diag(D)$$, left-singular vector and right-singular vector are the columns of $$U$$ and $$V$$ respectively.)
  2. $$U$$ and $$V$$, orthogonal matrices

  $$A=UDV^\intercal$$

  $$m\times n = (m\times m) (m\times n) (n\times n)$$

  **Useful to partially generalize matrix inversion to non-square matrices.**

### The Moore-Penrose Pseudoinverse
Matrix inversion **not defined** for non-square matrices, these can be calculated with this method.

 1. real (pseudoinverse): $$A^+=\lim_{\alpha\searrow 0}(A^\intercal A+\alpha I)^{-1}A^\intercal$$
 2. practical $$A^+=VD^+U^\intercal$$

  - $$U, D, V$$: singular value decomposition of $$A$$
  - $$D^+$$: diagonal matrix. Take $$D$$ and get the **reciprocal** of its nonzero element and take the **transpose**.

### Trace operator

### Determinant

[1]: (http://www.deeplearningbook.org/contents/linear_algebra.html)
[2]: (https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf)
[3]: (https://people.maths.ox.ac.uk/gilesm/files/NA-08-01.pdf)</content><author><name></name></author><category term="Math" /><summary type="html">Some interesting links: Matrix cookbook Matrix derivatives</summary></entry></feed>